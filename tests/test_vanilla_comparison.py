"""
Test vanilla vs refactored implementation equivalence.

This test verifies that the refactored VirNucPro produces mathematically
equivalent results to the vanilla implementation by comparing:
1. DNABERT-S embeddings (.pt files)
2. ESM-2 embeddings (.pt files)
3. Merged features (.pt files)
4. Prediction output files (.txt and .csv)

Reference vanilla outputs must be generated once before running this test:
    python prediction.py tests/data/test_with_orfs.fa 500 500_model.pth

This will create outputs in tests/data/test_with_orfs_nucleotide/ which should
be copied to tests/data/reference_vanilla_output/ for comparison.
"""

import pytest
import torch
import pandas as pd
from pathlib import Path
import tempfile
import shutil
from virnucpro.pipeline.prediction import run_prediction
from virnucpro.core.config import Config
from tests.compare_vanilla_embeddings import compare_embeddings


@pytest.fixture
def test_input_fasta():
    """Path to test sequences with valid ORFs"""
    return Path(__file__).parent / "data" / "test_with_orfs.fa"


@pytest.fixture
def vanilla_reference_dir():
    """Path to vanilla implementation reference outputs"""
    ref_dir = Path(__file__).parent / "data" / "reference_vanilla_output"
    if not ref_dir.exists() or not any(ref_dir.glob("*.pt")):
        pytest.skip(
            "Vanilla reference outputs not found. Generate them first:\n"
            "  python prediction.py tests/data/test_with_orfs.fa 500 500_model.pth\n"
            "Then copy outputs:\n"
            "  cp -r tests/data/test_with_orfs_nucleotide/* tests/data/reference_vanilla_output/"
        )
    return ref_dir


@pytest.fixture
def refactored_output_dir(tmp_path):
    """Temporary directory for refactored outputs"""
    output_dir = tmp_path / "refactored_output"
    output_dir.mkdir()
    yield output_dir
    # Cleanup handled by tmp_path


@pytest.fixture
def model_checkpoint():
    """Path to model checkpoint"""
    checkpoint = Path("500_model.pth")
    if not checkpoint.exists():
        pytest.skip("Model checkpoint not found: 500_model.pth")
    return checkpoint


class TestVanillaEquivalence:
    """Test suite for vanilla vs refactored equivalence"""

    @pytest.mark.slow
    @pytest.mark.gpu
    def test_embeddings_equivalence(
        self, test_input_fasta, vanilla_reference_dir, refactored_output_dir, model_checkpoint
    ):
        """
        Verify that refactored implementation produces equivalent embeddings to vanilla.

        This is the primary test for mathematical equivalence. If embeddings match,
        predictions will match.
        """
        # Load config
        config = Config.load()

        # Set device
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        # Run refactored pipeline
        run_prediction(
            input_file=test_input_fasta,
            model_path=model_checkpoint,
            expected_length=500,
            output_dir=refactored_output_dir,
            device=device,
            dnabert_batch_size=4,
            parallel=False,
            batch_size=256,
            num_workers=1,
            cleanup_intermediate=False,
            resume=False,
            show_progress=False,
            config=config
        )

        # Find the output subdirectories (refactored creates nucleotide/, protein/, merged/)
        input_name = test_input_fasta.stem
        refactored_nucleotide_dir = refactored_output_dir / f"{input_name}_nucleotide"
        refactored_protein_dir = refactored_output_dir / f"{input_name}_protein"
        refactored_merged_dir = refactored_output_dir / f"{input_name}_merged"

        # Debug: Show what was actually created
        if not refactored_nucleotide_dir.exists():
            actual_dirs = list(refactored_output_dir.glob("*"))
            pytest.fail(
                f"Refactored nucleotide output dir not found.\n"
                f"Expected: {refactored_nucleotide_dir}\n"
                f"Output dir contents: {actual_dirs}"
            )

        # Collect all refactored .pt files from all subdirectories
        refactored_pt_files = {}
        for subdir in [refactored_nucleotide_dir, refactored_protein_dir, refactored_merged_dir]:
            if subdir.exists():
                for pt_file in subdir.glob("*.pt"):
                    refactored_pt_files[pt_file.name] = pt_file

        # Compare all .pt files from vanilla reference
        vanilla_pt_files = sorted(vanilla_reference_dir.glob("*.pt"))
        assert len(vanilla_pt_files) > 0, "No .pt files found in vanilla reference"

        embedding_results = {}
        # Tolerances based on empirical testing (see tests/VANILLA_COMPARISON_RESULTS.md):
        # - Observed embedding differences: ~1-2% (max 1.82%)
        # - Root cause: Batching + proper attention masking in refactored code
        # - Impact on predictions: Negligible (<0.001%)
        # - Conclusion: 2% tolerance appropriate for batching differences
        tolerance_rtol = 0.02  # 2% relative tolerance (accounts for batching effects)
        tolerance_atol = 1e-5  # Absolute tolerance

        for vanilla_file in vanilla_pt_files:
            # Look for corresponding file in refactored outputs
            if vanilla_file.name not in refactored_pt_files:
                pytest.fail(
                    f"Refactored output missing: {vanilla_file.name}\n"
                    f"Available refactored files: {list(refactored_pt_files.keys())}"
                )

            refactored_file = refactored_pt_files[vanilla_file.name]

            # Compare embeddings
            result = compare_embeddings(
                vanilla_file,
                refactored_file,
                tolerance_rtol=tolerance_rtol,
                tolerance_atol=tolerance_atol
            )

            embedding_results[vanilla_file.name] = result

            # Assert IDs match
            assert result['ids_match'], (
                f"Sequence IDs mismatch in {vanilla_file.name}"
            )

            # Assert embeddings match within tolerance
            if not result['all_match']:
                mismatch_details = "\n".join([
                    f"  - {m['id']}: max_abs_diff={m['max_abs_diff']:.2e}, "
                    f"max_rel_diff={m['max_rel_diff']:.2e}"
                    for m in result['mismatches'][:5]
                ])
                pytest.fail(
                    f"Embeddings mismatch in {vanilla_file.name}:\n"
                    f"  Mismatches: {len(result['mismatches'])}/{result['num_sequences']}\n"
                    f"  Match rate: {result['match_rate']*100:.2f}%\n"
                    f"  Tolerance: rtol={tolerance_rtol}, atol={tolerance_atol}\n"
                    f"  Details:\n{mismatch_details}"
                )

        # All embeddings match - test passes
        print(f"\nâœ… All {len(embedding_results)} embedding files match within tolerance")
        for name, result in embedding_results.items():
            print(f"   - {name}: {result['num_sequences']} sequences")

    @pytest.mark.slow
    @pytest.mark.gpu
    def test_prediction_output_equivalence(
        self, test_input_fasta, vanilla_reference_dir, refactored_output_dir, model_checkpoint
    ):
        """
        Verify that refactored implementation produces equivalent prediction outputs.

        Compares:
        - prediction_results.txt (raw predictions)
        - prediction_results_highestscore.csv (consensus results)
        """
        # Load config
        config = Config.load()

        # Set device
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        # Run refactored pipeline
        run_prediction(
            input_file=test_input_fasta,
            model_path=model_checkpoint,
            expected_length=500,
            output_dir=refactored_output_dir,
            device=device,
            dnabert_batch_size=4,
            parallel=False,
            batch_size=256,
            num_workers=1,
            cleanup_intermediate=False,
            resume=False,
            show_progress=False,
            config=config
        )

        # Find the merged output directory (where predictions are stored)
        input_name = test_input_fasta.stem
        refactored_merged_dir = refactored_output_dir / f"{input_name}_merged"

        # Debug: Show what was actually created
        if not refactored_merged_dir.exists():
            actual_dirs = list(refactored_output_dir.glob("*"))
            pytest.fail(
                f"Refactored merged output dir not found.\n"
                f"Expected: {refactored_merged_dir}\n"
                f"Output dir contents: {actual_dirs}"
            )

        # Compare raw predictions (prediction_results.txt)
        vanilla_raw = vanilla_reference_dir / "prediction_results.txt"
        refactored_raw = refactored_merged_dir / "prediction_results.txt"

        if vanilla_raw.exists():
            assert refactored_raw.exists(), "Refactored prediction_results.txt not found"

            vanilla_lines = vanilla_raw.read_text().strip().split('\n')
            refactored_lines = refactored_raw.read_text().strip().split('\n')

            # Parse and compare (format: Sequence_ID\tPrediction\tscore1\tscore2)
            # Skip header line
            vanilla_preds = {}
            for line in vanilla_lines[1:]:  # Skip header
                if line.strip():
                    parts = line.split('\t')
                    seq_id = parts[0]
                    vanilla_preds[seq_id] = parts[1:]  # [prediction, score1, score2]

            refactored_preds = {}
            for line in refactored_lines[1:]:  # Skip header
                if line.strip():
                    parts = line.split('\t')
                    seq_id = parts[0]
                    refactored_preds[seq_id] = parts[1:]

            # Compare predictions
            assert set(vanilla_preds.keys()) == set(refactored_preds.keys()), (
                f"Sequence ID mismatch in raw predictions.\n"
                f"Vanilla only: {set(vanilla_preds.keys()) - set(refactored_preds.keys())}\n"
                f"Refactored only: {set(refactored_preds.keys()) - set(vanilla_preds.keys())}"
            )

            mismatches = []
            for seq_id in vanilla_preds:
                v_pred = vanilla_preds[seq_id]
                r_pred = refactored_preds[seq_id]

                # Compare prediction label
                if v_pred[0] != r_pred[0]:
                    mismatches.append(f"{seq_id}: pred {v_pred[0]} vs {r_pred[0]}")

                # Compare scores (with floating-point tolerance)
                # Based on empirical testing, prediction scores differ by <0.001%
                # despite 1-2% embedding differences. Use strict tolerance.
                try:
                    v_scores = [float(x) for x in v_pred[1:]]
                    r_scores = [float(x) for x in r_pred[1:]]

                    for i, (vs, rs) in enumerate(zip(v_scores, r_scores)):
                        if not torch.allclose(
                            torch.tensor(vs),
                            torch.tensor(rs),
                            rtol=0,  # No relative tolerance needed
                            atol=1e-4  # Absolute tolerance: 0.01% (empirical max: 0.001%)
                        ):
                            mismatches.append(
                                f"{seq_id}: score{i} {vs:.6f} vs {rs:.6f} (diff: {abs(vs-rs):.2e})"
                            )
                except (ValueError, IndexError) as e:
                    mismatches.append(f"{seq_id}: score parsing error: {e}")

            assert len(mismatches) == 0, (
                f"Raw prediction mismatches:\n" + "\n".join(mismatches[:10])
            )

            print(f"\nâœ… Raw predictions match: {len(vanilla_preds)} sequences")

        # Compare consensus results (prediction_results_highestscore.csv)
        vanilla_consensus = vanilla_reference_dir / "prediction_results_highestscore.csv"
        refactored_consensus = refactored_merged_dir / "prediction_results_highestscore.csv"

        if vanilla_consensus.exists():
            assert refactored_consensus.exists(), (
                "Refactored prediction_results_highestscore.csv not found"
            )

            # Read as CSV
            vanilla_df = pd.read_csv(vanilla_consensus, sep='\t')
            refactored_df = pd.read_csv(refactored_consensus, sep='\t')

            # Sort by ID for comparison
            vanilla_df = vanilla_df.sort_values(by=vanilla_df.columns[0]).reset_index(drop=True)
            refactored_df = refactored_df.sort_values(by=refactored_df.columns[0]).reset_index(drop=True)

            # Compare IDs
            assert list(vanilla_df.iloc[:, 0]) == list(refactored_df.iloc[:, 0]), (
                "Sequence ID mismatch in consensus results"
            )

            # Compare Is_Virus column
            assert list(vanilla_df.iloc[:, 1]) == list(refactored_df.iloc[:, 1]), (
                "Is_Virus prediction mismatch in consensus results"
            )

            # Compare scores with tolerance
            # Consensus scores should match within 0.01% (empirical: <0.001%)
            for col_idx in range(2, len(vanilla_df.columns)):
                vanilla_scores = torch.tensor(vanilla_df.iloc[:, col_idx].values, dtype=torch.float32)
                refactored_scores = torch.tensor(refactored_df.iloc[:, col_idx].values, dtype=torch.float32)

                if not torch.allclose(vanilla_scores, refactored_scores, rtol=0, atol=1e-4):
                    # Find mismatches
                    diff = torch.abs(vanilla_scores - refactored_scores)
                    mismatches = torch.where(diff > 1e-4)[0]

                    mismatch_details = []
                    for idx in mismatches[:5]:
                        seq_id = vanilla_df.iloc[idx, 0]
                        v_score = vanilla_scores[idx].item()
                        r_score = refactored_scores[idx].item()
                        score_diff = diff[idx].item()
                        mismatch_details.append(
                            f"  {seq_id}: {v_score:.6f} vs {r_score:.6f} (diff: {score_diff:.2e})"
                        )

                    pytest.fail(
                        f"Score mismatch in column {vanilla_df.columns[col_idx]}:\n"
                        + "\n".join(mismatch_details)
                    )

            print(f"\nâœ… Consensus predictions match: {len(vanilla_df)} sequences")

    @pytest.mark.slow
    @pytest.mark.gpu
    def test_full_pipeline_equivalence(
        self, test_input_fasta, vanilla_reference_dir, refactored_output_dir, model_checkpoint
    ):
        """
        Comprehensive test that runs both embedding and prediction comparisons.

        This is a convenience test that combines both embedding and prediction
        verification in a single end-to-end test.
        """
        # This test delegates to the other two tests
        self.test_embeddings_equivalence(
            test_input_fasta, vanilla_reference_dir, refactored_output_dir, model_checkpoint
        )

        self.test_prediction_output_equivalence(
            test_input_fasta, vanilla_reference_dir, refactored_output_dir, model_checkpoint
        )

        print("\n" + "=" * 80)
        print("ðŸŽ‰ Full pipeline equivalence verified!")
        print("=" * 80)
        print("Refactored implementation produces mathematically equivalent results")
        print("to vanilla implementation for all embeddings and predictions.")
