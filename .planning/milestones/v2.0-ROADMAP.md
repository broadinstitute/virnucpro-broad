# Milestone v2.0: Async Architecture + Sequence Packing

**Status:** SHIPPED 2026-02-09
**Phases:** 5-10 (including 10.1, 10.2 insertions)
**Total Plans:** 42

## Overview

v2.0 replaces the v1.0 multi-worker-per-GPU architecture with a modern async DataLoader + sequence packing pipeline to achieve 4.5x speedup (45h to <10h target). The journey starts by establishing single-GPU async DataLoader foundation with CUDA safety guarantees (Phase 5), then adds sequence packing with FlashAttention varlen for 2-3x throughput gain (Phase 6), scales to multi-GPU coordination (Phase 7), validates FP16 precision for 2x memory reduction (Phase 8), integrates robust checkpointing for 6M sequence workloads (Phase 9), and completes with end-to-end performance validation and telemetry (Phase 10).

**Architectural Shift:** From multiprocessing.Pool with N workers per GPU to single process per GPU with async DataLoader pattern. Addresses v1.0 limitations: Nx11GB memory overhead, pickle serialization tax, GPU starvation from small batches.

## Phases

### Phase 5: Async DataLoader Foundation

**Goal**: Single-GPU DataLoader safely handles I/O without CUDA corruption
**Depends on**: Phase 4.1 (v1.0 baseline)
**Requirements**: ARCH-01, ARCH-02, ARCH-03, ARCH-04, ARCH-05, SAFE-01, SAFE-02, SAFE-03, SAFE-04, SAFE-05
**Plans**: 5 plans in 4 waves

Plans:
- [x] 05-01: CUDA-safe SequenceDataset and VarlenCollator
- [x] 05-02: GPU monitor DataLoader metrics extension
- [x] 05-03: Async DataLoader factory with CUDA safety
- [x] 05-04: AsyncInferenceRunner with stream overlap
- [x] 05-05: Integration tests and verification

**Completed:** 2026-02-03

### Phase 6: Sequence Packing Integration

**Goal**: Variable-length sequences pack efficiently with FlashAttention varlen
**Depends on**: Phase 5
**Requirements**: ARCH-09, ARCH-10, ARCH-11, PACK-01, PACK-02, PACK-03, PACK-04, PACK-04b, PACK-05, PACK-06, PACK-06a
**Plans**: 8 plans in 5 waves

Plans:
- [x] 06-01: GreedyPacker with FFD algorithm and dynamic token budget
- [x] 06-02: Position ID generator and FlashAttention varlen wrapper
- [x] 06-03: ESM2 forward_packed method
- [x] 06-04: Wire packed inference in AsyncInferenceRunner
- [x] 06-05: Packed vs unpacked equivalence validation
- [x] 06-06: Packing efficiency metrics and monitoring
- [x] 06-07: End-to-end integration tests and verification
- [x] 06-08: Integrate GreedyPacker into VarlenCollator (PACK-02)

**Completed:** 2026-02-04

### Phase 7: Multi-GPU Coordination

**Goal**: N GPUs process independent sequence shards without conflicts
**Depends on**: Phase 6
**Requirements**: ARCH-06, ARCH-07, ARCH-08, GPU-01, GPU-02, GPU-03, GPU-04
**Plans**: 8 plans in 5 waves

Plans:
- [x] 07-01: SequenceIndex with stride distribution and caching
- [x] 07-02: IndexBasedDataset for byte-offset sequence reading
- [x] 07-03: Per-worker logging infrastructure
- [x] 07-04: GPUProcessCoordinator for worker lifecycle
- [x] 07-05: HDF5 shard aggregation with validation
- [x] 07-06: GPU worker function integrating inference pipeline
- [x] 07-07: run_multi_gpu_inference orchestration entry point
- [x] 07-08: Integration tests and human verification

**Completed:** 2026-02-05

### Phase 8: FP16 Precision Validation

**Goal**: FP16 delivers throughput improvement while maintaining embedding accuracy
**Depends on**: Phase 7
**Requirements**: PREC-01, PREC-02, PREC-03
**Plans**: 5 plans in 3 waves (1 skipped — PREC-03 conditional, not needed)

Plans:
- [x] 08-01: FP16 model conversion with feature flag and FlashAttention alignment
- [x] 08-02: NaN/Inf detection, gpu_worker FP16 passthrough, unit tests
- [x] 08-03: FP16 vs FP32 equivalence integration tests (stratified validation)
- [x] 08-04: FP16 vs FP32 throughput benchmarking
- [ ] 08-05: Selective FP32 fallback for LayerNorm (SKIPPED — not needed)

**Completed:** 2026-02-05

### Phase 9: Checkpointing Integration

**Goal**: Pipeline resumes from partial completion after crashes
**Depends on**: Phase 8
**Requirements**: CKPT-01, CKPT-02, CKPT-03, CKPT-04, CKPT-05, CKPT-06
**Plans**: 7 plans in 5 waves

Plans:
- [x] 09-01: CheckpointTrigger, AsyncCheckpointWriter, HDF5 validation, resume logic
- [x] 09-02: CheckpointManifest for multi-GPU coordination
- [x] 09-03: Wire checkpointing into AsyncInferenceRunner
- [x] 09-04: Wire checkpointing into gpu_worker with resume
- [x] 09-05: Coordinator retry + manifest in multi_gpu_inference
- [x] 09-06: Unit tests for checkpoint components
- [x] 09-07: Integration tests for resume and crash recovery

**Completed:** 2026-02-06

### Phase 10: Performance Validation & Tuning

**Goal**: Full pipeline meets time targets on real 1M subset with correctness validation against v1.0
**Depends on**: Phase 9, Phase 10.1, Phase 10.2
**Requirements**: PERF-01, PERF-02, PERF-03, PERF-04, PERF-05
**Plans**: 3 plans in 3 waves

Plans:
- [x] 10-01: Per-stage timing instrumentation and pipeline summary block
- [x] 10-02: 1x RTX 4090 full pipeline benchmark + correctness validation
- [x] 10-03: 2x RTX 4090 benchmark + scaling/speedup analysis + final report

**Results (4/7 criteria met):**
- 1x RTX 4090: 53:20 (PASS, target <1h)
- 2x RTX 4090: 33:44 (FAIL, target <30min — DNABERT-S v1.0 drag)
- v1.0 speedup: 6.2x (PASS, target >=4.0x)
- Correctness: 99.87% consensus agreement (PASS, target >=99%)
- ESM-2 scaling: 1.87x / 93.7% efficiency (excellent)
- Overall scaling: 1.58x (FAIL — DNABERT-S 0.96x drag)
- Packing: ~358% (PASS, target >90%)

**Completed:** 2026-02-09

### Phase 10.1: CLI Integration for v2.0 Architecture (INSERTED)

**Goal**: Update CLI to use v2.0 architecture instead of v1.0 code
**Depends on**: Phase 10
**Plans**: 4 plans (2 core + 2 gap closure)

Plans:
- [x] 10.1-01: Update predict command to route --parallel to v2.0 API with CLI tests
- [x] 10.1-02: Benchmark CLI Phase 10 suites and migration guide
- [x] 10.1-03: Performance telemetry and regression tests (gap closure)
- [x] 10.1-04: Validation and cleanup improvements (gap closure)

**Completed:** 2026-02-06

### Phase 10.2: FlashAttention Scoring Divergence Resolution (INSERTED)

**Goal**: Resolve prediction divergence between v2.0 FlashAttention and v1.0 bmm attention
**Depends on**: Phase 10.1
**Plans**: 2 plans in 2 waves

Plans:
- [x] 10.2-01: Prediction-level divergence quantification test
- [x] 10.2-02: V1.0-compatible attention fallback and CLI integration

**Completed:** 2026-02-08

---

## Milestone Summary

**Decimal Phases:**
- Phase 10.1: CLI Integration (inserted — CLI never updated for v2.0 API)
- Phase 10.2: FlashAttention Scoring Divergence (inserted — v2.0 attention diverged from v1.0)

**Key Decisions:**
- v2.0 Async architecture: Single-process-per-GPU + async DataLoader replaces multi-worker pattern
- FP16 over BF16: ESM-2 trained in FP16, optimal precision for this model
- FlashAttention varlen: Required for sequence packing cross-sequence isolation
- Stride-based index distribution: [rank::world_size] on length-sorted index for balanced GPU load
- FFD packing algorithm: First-Fit Decreasing for 92-94% packing efficiency
- Hybrid architecture: ESM-2 uses v2.0, DNABERT-S stays v1.0 (staged validation)
- FlashAttention divergence is cosmetic: 100% label agreement despite minor embedding differences

**Issues Resolved:**
- Fixed EOS token inclusion in mean pooling (attention divergence root cause)
- Fixed missing token dropout 0.88x rescaling in FlashAttention path
- Fixed wrong GELU function (torch.nn.functional.gelu vs approximate)
- Fixed RoPE precision alignment between v1.0 and v2.0 paths
- Fixed stream sync race condition in embedding extraction
- Fixed DNABERT-S concurrent model loading cache race

**Issues Deferred:**
- DNABERT-S v2.0 architecture (stays v1.0 bin-packing — deferred to v2.1)
- GPU utilization optimization (60-80% vs 80% target — measurement methodology questionable)
- transformers security upgrade (4.30.0 has 12 CVEs — deferred to v3.0+)
- torch.compile integration for kernel fusion
- Parameter sweep for token budget / prefetch_factor tuning

**Technical Debt:**
- DNABERT-S v1.0 bin-packing causes 4% slowdown on 2 GPUs (0.96x scaling)
- repr_layers hardcoded to [36] in 6 locations (blocks ESM-2 model swaps)
- ESM-2 model name hardcoded in multiple files (not configurable via CLI)
- test_fp16_validation.py has overly tight absolute diff threshold (7.8e-3 vs 1e-3)

---
*Archived: 2026-02-09 as part of v2.0 milestone completion*
*For current project status, see .planning/ROADMAP.md*
