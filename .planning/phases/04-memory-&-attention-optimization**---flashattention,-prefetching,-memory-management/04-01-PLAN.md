---
phase: 04-memory-attention
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [virnucpro/cuda/attention_utils.py, virnucpro/models/esm2_flash.py, virnucpro/models/__init__.py, tests/test_flashattention.py]
autonomous: true
must_haves:
  truths:
    - "ESM-2 automatically uses FlashAttention-2 on Ampere+ GPUs"
    - "ESM-2 falls back to standard attention transparently on older GPUs"
    - "FlashAttention-2 status logged clearly at startup"
  artifacts:
    - path: "virnucpro/cuda/attention_utils.py"
      provides: "FlashAttention-2 detection and configuration"
      min_lines: 50
      exports: ["get_attention_implementation", "configure_flash_attention"]
    - path: "virnucpro/models/esm2_flash.py"
      provides: "ESM-2 wrapper with FlashAttention-2"
      min_lines: 100
      exports: ["ESM2WithFlashAttention", "load_esm2_model"]
  key_links:
    - from: "virnucpro/models/esm2_flash.py"
      to: "virnucpro/cuda/attention_utils.py"
      via: "import and use get_attention_implementation"
      pattern: "from.*attention_utils.*import.*get_attention_implementation"
    - from: "virnucpro/models/esm2_flash.py"
      to: "torch.backends.cuda.sdp_kernel"
      via: "FlashAttention-2 context manager"
      pattern: "torch\\.backends\\.cuda\\.sdp_kernel"
---

<objective>
Integrate FlashAttention-2 for ESM-2 model with automatic GPU detection and transparent fallback.

Purpose: Enable 2-4x attention speedup for transformer models on compatible GPUs while maintaining compatibility with older hardware.
Output: ESM-2 model wrapper that automatically uses FlashAttention-2 when available.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-CONTEXT.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-RESEARCH.md

# Prior work references
@.planning/phases/02-dnabert-s-optimization/02-01-SUMMARY.md
@virnucpro/pipeline/parallel_esm.py
@virnucpro/pipeline/base_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create FlashAttention-2 detection and configuration utilities</name>
  <files>virnucpro/cuda/attention_utils.py</files>
  <action>
    Create utilities for detecting and configuring FlashAttention-2:

    1. Create `virnucpro/cuda/` directory if it doesn't exist
    2. Implement `get_attention_implementation()` function that:
       - Checks if CUDA is available
       - Detects GPU compute capability (8.0+ for Ampere)
       - Tests FlashAttention-2 availability using torch.backends.cuda.sdp_kernel
       - Returns "flash_attention_2" or "standard_attention"
    3. Implement `configure_flash_attention(model, logger=None)` that:
       - Calls get_attention_implementation()
       - Logs implementation choice (FlashAttention-2: enabled or using standard attention)
       - Sets model.config._attn_implementation = "sdpa" for FlashAttention-2
       - Sets model.config.use_flash_attention_2 = True when available
       - Returns the configured model
    4. Add `is_flash_attention_available()` helper that returns boolean for testing
    5. Include proper error handling for import failures or unsupported configurations

    Use PyTorch 2.2+ scaled_dot_product_attention (sdpa) as the backend.
  </action>
  <verify>python -c "from virnucpro.cuda.attention_utils import get_attention_implementation; print(get_attention_implementation())"</verify>
  <done>attention_utils.py exists with GPU detection and FlashAttention-2 configuration functions</done>
</task>

<task type="auto">
  <name>Task 2: Create ESM-2 model wrapper with FlashAttention-2 support</name>
  <files>virnucpro/models/esm2_flash.py, virnucpro/models/__init__.py</files>
  <action>
    Create ESM-2 model wrapper that integrates FlashAttention-2:

    1. Create `virnucpro/models/` directory if it doesn't exist
    2. In esm2_flash.py, implement `ESM2WithFlashAttention` class that:
       - Wraps the existing ESM model from fair-esm
       - Uses attention_utils.configure_flash_attention() in __init__
       - Implements forward() method with proper context manager:
         * For flash_attention_2: Use torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False)
         * For standard: Use default attention
       - Tracks attention implementation in self.attention_impl
    3. Implement `load_esm2_model(model_name="esm2_t48_15B_UR50D", device="cuda", logger=None)`:
       - Loads base ESM model using esm.pretrained.load_model_and_alphabet()
       - Wraps with ESM2WithFlashAttention
       - Configures FlashAttention-2 if available
       - Logs model configuration details
       - Returns (model, batch_converter) tuple
    4. Add memory-efficient settings:
       - model.eval() and torch.no_grad() context
       - model.half() or model.bfloat16() based on GPU capability
    5. Update virnucpro/models/__init__.py to export the new functions

    Note: ESM models use different attention naming than HuggingFace transformers, adapt accordingly.
  </action>
  <verify>python -c "from virnucpro.models.esm2_flash import load_esm2_model; model, _ = load_esm2_model(); print(f'Model loaded with attention: {model.attention_impl}')"</verify>
  <done>ESM2WithFlashAttention wrapper created with automatic FlashAttention-2 configuration</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for FlashAttention-2 integration</name>
  <files>tests/test_flashattention.py</files>
  <action>
    Create comprehensive tests for FlashAttention-2 integration:

    1. Test `get_attention_implementation()`:
       - Mock different GPU scenarios (no GPU, old GPU, Ampere+)
       - Verify correct implementation selection
       - Test graceful handling when flash-attn not installed
    2. Test `configure_flash_attention()`:
       - Create mock model object
       - Verify config attributes are set correctly
       - Check logging output contains expected messages
    3. Test `ESM2WithFlashAttention`:
       - Mock ESM model for testing
       - Verify forward pass works with both attention implementations
       - Test that context manager is used correctly
       - Ensure backward compatibility with existing code
    4. Test performance characteristics:
       - Create small test sequences
       - Verify no errors with variable length batches
       - Check memory usage doesn't increase unexpectedly
    5. Add integration test:
       - Load actual ESM model (mark as @pytest.mark.slow)
       - Process sample sequences
       - Verify outputs are reasonable (not NaN, proper shape)

    Use pytest.mark.gpu for GPU-specific tests and mock appropriately for CI.
  </action>
  <verify>pytest tests/test_flashattention.py -v</verify>
  <done>Unit tests verify FlashAttention-2 detection, configuration, and ESM-2 integration</done>
</task>

</tasks>

<verification>
- ESM-2 model loads successfully with FlashAttention-2 on compatible GPUs
- Logs clearly show "FlashAttention-2: enabled" or "Using standard attention"
- Test suite passes for both FlashAttention-2 and fallback scenarios
- No errors when running on older GPUs without FlashAttention-2 support
</verification>

<success_criteria>
- attention_utils.py correctly detects GPU capabilities and FlashAttention-2 availability
- ESM2WithFlashAttention wrapper automatically configures optimal attention implementation
- Clear logging indicates which attention implementation is active
- Tests cover both FlashAttention-2 and standard attention paths
</success_criteria>

<output>
After completion, create `.planning/phases/04-memory-attention/04-01-SUMMARY.md`
</output>