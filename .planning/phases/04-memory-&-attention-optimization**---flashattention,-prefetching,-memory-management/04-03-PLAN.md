---
phase: 04-memory-attention
plan: 03
type: execute
wave: 2
depends_on: [04-01, 04-02]
files_modified: [virnucpro/cuda/stream_manager.py, virnucpro/pipeline/parallel_esm.py, virnucpro/pipeline/parallel_dnabert.py, tests/test_cuda_streams.py]
autonomous: true
must_haves:
  truths:
    - "CUDA streams overlap I/O and computation for latency hiding"
    - "Stream errors fail workers immediately with clear diagnostics"
    - "ESM-2 and DNABERT-S workers use stream-based processing"
  artifacts:
    - path: "virnucpro/cuda/stream_manager.py"
      provides: "CUDA stream orchestration"
      min_lines: 120
      exports: ["StreamManager", "StreamProcessor"]
    - path: "virnucpro/pipeline/parallel_esm.py"
      provides: "Updated ESM-2 worker with streams"
      contains: "StreamManager"
    - path: "virnucpro/pipeline/parallel_dnabert.py"
      provides: "Updated DNABERT-S worker with streams"
      contains: "StreamManager"
  key_links:
    - from: "virnucpro/cuda/stream_manager.py"
      to: "torch.cuda.Stream"
      via: "CUDA stream creation and management"
      pattern: "torch\\.cuda\\.Stream"
    - from: "virnucpro/pipeline/parallel_esm.py"
      to: "virnucpro/cuda/stream_manager.py"
      via: "Import and use StreamManager"
      pattern: "from.*stream_manager.*import.*StreamManager"
---

<objective>
Implement CUDA stream orchestration and integrate with existing embedding workers for I/O-compute overlap.

Purpose: Hide 20-40% I/O latency through asynchronous data transfer and computation overlap.
Output: Stream manager integrated with ESM-2 and DNABERT-S workers for improved throughput.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-CONTEXT.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-RESEARCH.md

# Dependencies from this wave
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-01-PLAN.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-02-PLAN.md

# Prior worker implementations
@virnucpro/pipeline/parallel_esm.py
@virnucpro/pipeline/parallel_dnabert.py
@virnucpro/pipeline/base_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CUDA stream manager for I/O-compute overlap</name>
  <files>virnucpro/cuda/stream_manager.py</files>
  <action>
    Create CUDA stream orchestration system:

    1. Implement `StreamManager` class:
       - __init__(self, num_streams_per_gpu=2, debug=False)
       - Initialize stream dictionary: {gpu_id: {'load': Stream, 'compute': Stream}}
       - Store debug flag for verbose logging
    2. Add stream creation and management:
       - `create_streams(gpu_ids)`: Create streams for specified GPUs
       - `get_stream(gpu_id, stream_type='compute')`: Return specific stream
       - `synchronize(gpu_id=None)`: Sync specific GPU or all GPUs
    3. Implement `StreamProcessor` class for batch processing:
       - __init__(self, stream_manager, gpu_id, logger=None)
       - Store references to manager and GPU
    4. Add async processing methods:
       - `async_load(data, device, stream_type='load')`:
         * Use specified stream for data transfer
         * Call tensor.to(device, non_blocking=True)
         * Return future/handle for synchronization
       - `process_with_overlap(batch_data, model, preprocess_fn=None)`:
         * Load data in 'load' stream
         * Process in 'compute' stream
         * Wait for load to complete before compute
         * Synchronize after batch completion
    5. Add error handling:
       - `handle_stream_error(error, gpu_id)`: Log error and cleanup
       - Wrap stream operations in try/except
       - Fail fast with clear diagnostics on CUDA errors
    6. Implement monitoring utilities:
       - `get_stream_status()`: Return active/idle status for all streams
       - `log_stream_operations(operation, gpu_id, stream_type)`: Debug logging
    7. Add context manager for stream scope:
       - `stream_scope(gpu_id, stream_type)`: Context manager for stream operations

    Reference PyTorch CUDA semantics documentation for proper synchronization.
  </action>
  <verify>python -c "from virnucpro.cuda.stream_manager import StreamManager; sm = StreamManager(); sm.create_streams([0]); print('Streams created')"</verify>
  <done>StreamManager and StreamProcessor classes created for CUDA stream orchestration</done>
</task>

<task type="auto">
  <name>Task 2: Integrate stream manager with ESM-2 and DNABERT-S workers</name>
  <files>virnucpro/pipeline/parallel_esm.py, virnucpro/pipeline/parallel_dnabert.py</files>
  <action>
    Update embedding workers to use CUDA streams:

    For virnucpro/pipeline/parallel_esm.py:
    1. Import StreamManager and StreamProcessor from cuda.stream_manager
    2. In ESM2Worker.__init__:
       - Create self.stream_manager = StreamManager(num_streams_per_gpu=2)
       - Initialize self.stream_processor after GPU assignment
    3. Update process_file() method:
       - Create StreamProcessor for assigned GPU
       - Use stream_processor.async_load() for batch data transfer
       - Use stream_processor.process_with_overlap() for model inference
       - Add debug logging if self.verbose
    4. Update run_parallel_esm2():
       - Pass debug flag from CLI to workers
       - Add stream synchronization in exception handling

    For virnucpro/pipeline/parallel_dnabert.py:
    1. Apply same pattern as ESM-2:
       - Import StreamManager and StreamProcessor
       - Add to DNABERTWorker.__init__
       - Update process_sequences() to use streams
       - Use async_load for tokenized inputs
       - Process with overlap for model forward pass
    2. Ensure consistency with ESM-2 implementation

    Both updates should:
    - Maintain backward compatibility (streams optional)
    - Add --cuda-streams CLI flag (default: True)
    - Log stream usage when verbose mode enabled
    - Synchronize streams on errors for clean shutdown
  </action>
  <verify>python -c "from virnucpro.pipeline.parallel_esm import ESM2Worker; print('ESM2Worker with streams imported successfully')"</verify>
  <done>ESM-2 and DNABERT-S workers updated with CUDA stream integration</done>
</task>

<task type="auto">
  <name>Task 3: Add CUDA stream integration tests</name>
  <files>tests/test_cuda_streams.py</files>
  <action>
    Create comprehensive tests for CUDA stream integration:

    1. Test StreamManager:
       - Test stream creation for multiple GPUs
       - Verify stream count matches configuration
       - Test synchronization methods
       - Mock torch.cuda.Stream for CI
    2. Test StreamProcessor:
       - Test async_load with mock tensors
       - Verify non_blocking=True is used
       - Test process_with_overlap flow
       - Check stream synchronization order
    3. Test error handling:
       - Simulate CUDA errors in stream operations
       - Verify fail-fast behavior
       - Check error diagnostics clarity
    4. Test worker integration:
       - Mock ESM2Worker with streams
       - Verify stream usage in process_file()
       - Test stream cleanup on worker exit
       - Check backward compatibility without streams
    5. Add performance tests (mark @pytest.mark.gpu):
       - Compare with/without streams (if GPU available)
       - Measure overlap effectiveness
       - Verify no correctness issues
    6. Test stream monitoring:
       - Verify status reporting
       - Check debug logging output
       - Test stream scope context manager

    Mock CUDA operations for CI, use real GPU tests when available.
  </action>
  <verify>pytest tests/test_cuda_streams.py -v</verify>
  <done>Comprehensive tests for CUDA stream manager and worker integration</done>
</task>

</tasks>

<verification>
- StreamManager creates correct number of streams per GPU
- ESM-2 and DNABERT-S workers use streams for I/O overlap
- Stream errors cause immediate worker failure with diagnostics
- Tests verify stream synchronization and overlap behavior
</verification>

<success_criteria>
- StreamManager successfully creates and manages CUDA streams
- Workers process batches with I/O-compute overlap via streams
- Stream synchronization happens correctly after each batch
- Error handling provides clear diagnostics for stream failures
- Tests pass for both stream-enabled and disabled configurations
</success_criteria>

<output>
After completion, create `.planning/phases/04-memory-attention/04-03-SUMMARY.md`
</output>