---
phase: 04-memory-attention
plan: 02
type: execute
wave: 1
depends_on: []
files_modified: [virnucpro/data/dataloader_utils.py, virnucpro/cuda/memory_manager.py, tests/test_memory_optimization.py]
autonomous: true
must_haves:
  truths:
    - "DataLoader uses CPU-aware worker count for optimal I/O"
    - "Memory fragmentation prevented through sequence sorting and cache management"
    - "Expandable segments configurable via CLI flag"
  artifacts:
    - path: "virnucpro/data/dataloader_utils.py"
      provides: "Optimized DataLoader configuration"
      min_lines: 80
      exports: ["create_optimized_dataloader", "get_optimal_workers"]
    - path: "virnucpro/cuda/memory_manager.py"
      provides: "Memory fragmentation prevention"
      min_lines: 100
      exports: ["MemoryManager", "configure_memory_optimization"]
  key_links:
    - from: "virnucpro/data/dataloader_utils.py"
      to: "torch.utils.data.DataLoader"
      via: "DataLoader creation with optimized settings"
      pattern: "DataLoader.*num_workers.*prefetch_factor"
    - from: "virnucpro/cuda/memory_manager.py"
      to: "torch.cuda.empty_cache"
      via: "Periodic cache clearing"
      pattern: "torch\\.cuda\\.empty_cache"
---

<objective>
Create optimized DataLoader configuration and memory fragmentation prevention utilities.

Purpose: Enable efficient data loading with CPU-aware worker counts and prevent OOM errors from memory fragmentation.
Output: DataLoader utilities and memory management system for stable high-throughput processing.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-CONTEXT.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-RESEARCH.md

# Prior work references
@virnucpro/pipeline/features.py
@virnucpro/utils/sequence_utils.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create optimized DataLoader configuration utilities</name>
  <files>virnucpro/data/dataloader_utils.py</files>
  <action>
    Create DataLoader optimization utilities:

    1. Create `virnucpro/data/` directory if it doesn't exist
    2. Implement `get_optimal_workers(num_gpus, dataloader_workers=None)`:
       - If dataloader_workers provided, return it
       - Otherwise: return min(cpu_count() // max(num_gpus, 1), 8)
       - Add logging for auto-detected worker count
    3. Implement `create_optimized_dataloader(dataset, batch_size, num_gpus=1, dataloader_workers=None, pin_memory=None, shuffle=True)`:
       - Call get_optimal_workers() for worker count
       - Default pin_memory to torch.cuda.is_available() if None
       - Create DataLoader with:
         * num_workers from get_optimal_workers()
         * pin_memory flag
         * prefetch_factor=2 (fixed good default)
         * persistent_workers=True if num_workers > 0
         * multiprocessing_context='spawn' if num_workers > 0
       - Log configuration details
       - Return configured DataLoader
    4. Add `create_sequence_dataloader(sequences, batch_size, sort_by_length=True, **kwargs)`:
       - Optionally sort sequences by length for memory efficiency
       - Create dataset wrapper for sequences
       - Call create_optimized_dataloader() with dataset
    5. Include memory usage estimation helper:
       - `estimate_memory_usage(batch_size, max_seq_length, model_size_gb=3.0)`
       - Returns estimated GPU memory needed in GB

    Reference PyTorch DataLoader best practices for production settings.
  </action>
  <verify>python -c "from virnucpro.data.dataloader_utils import get_optimal_workers; print(f'Optimal workers for 4 GPUs: {get_optimal_workers(4)}')"</verify>
  <done>dataloader_utils.py created with CPU-aware worker configuration and optimized settings</done>
</task>

<task type="auto">
  <name>Task 2: Create memory fragmentation prevention manager</name>
  <files>virnucpro/cuda/memory_manager.py</files>
  <action>
    Create memory management system for fragmentation prevention:

    1. Implement `MemoryManager` class:
       - __init__(self, enable_expandable_segments=False, cache_clear_interval=100, verbose=False)
       - Store configuration and batch counter
    2. Add memory configuration methods:
       - `configure_expandable_segments()`: Sets PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" in os.environ
       - `get_memory_stats(device=0)`: Returns dict with allocated, reserved, free memory
       - `log_memory_status(prefix="")`: Logs current memory usage
    3. Implement fragmentation prevention:
       - `clear_cache()`: Calls torch.cuda.empty_cache() with optional memory stats logging
       - `should_clear_cache(batch_num)`: Returns True every cache_clear_interval batches
       - `sort_sequences_by_length(sequences)`: Sorts to minimize padding/fragmentation
    4. Add OOM prevention utilities:
       - `check_memory_available(required_gb, device=0)`: Returns True if enough memory
       - `get_safe_batch_size(model_size_gb, max_seq_length, available_memory_gb)`: Calculates safe batch size
    5. Create context manager for memory tracking:
       - `memory_tracking()`: Context manager that logs memory before/after
    6. Implement `configure_memory_optimization(enable_expandable=False, cache_interval=100)`:
       - Global configuration function
       - Sets environment variables
       - Returns configured MemoryManager instance
    7. Add diagnostic methods:
       - `get_fragmentation_ratio()`: Estimates fragmentation from reserved vs allocated
       - `suggest_batch_size_adjustment()`: Recommends batch size based on OOM patterns

    Follow PyTorch memory management best practices.
  </action>
  <verify>python -c "from virnucpro.cuda.memory_manager import MemoryManager; mm = MemoryManager(); print(mm.get_memory_stats())"</verify>
  <done>MemoryManager class created with fragmentation prevention and cache management</done>
</task>

<task type="auto">
  <name>Task 3: Add comprehensive memory optimization tests</name>
  <files>tests/test_memory_optimization.py</files>
  <action>
    Create tests for DataLoader and memory optimization:

    1. Test DataLoader utilities:
       - Test get_optimal_workers() with various GPU counts
       - Verify worker count capping at 8
       - Test create_optimized_dataloader() configuration
       - Mock cpu_count() for consistent testing
       - Verify pin_memory and prefetch_factor settings
    2. Test MemoryManager:
       - Test expandable segments configuration (check os.environ)
       - Test cache clearing interval logic
       - Mock torch.cuda functions for CI compatibility
       - Test memory stats collection
       - Verify OOM prevention calculations
    3. Test sequence sorting:
       - Create sequences of varying lengths
       - Verify sorting reduces padding overhead
       - Test with empty sequence list
    4. Test integration scenarios:
       - DataLoader with MemoryManager
       - Batch processing with periodic cache clearing
       - Memory tracking context manager
    5. Add performance benchmarks:
       - Mark with @pytest.mark.slow
       - Compare sorted vs unsorted sequence processing
       - Measure memory fragmentation over time
    6. Test error handling:
       - OOM simulation and recovery suggestions
       - Invalid configuration parameters
       - Missing CUDA availability

    Use mocking for CUDA operations to ensure tests run in CI.
  </action>
  <verify>pytest tests/test_memory_optimization.py -v</verify>
  <done>Comprehensive tests for DataLoader optimization and memory management</done>
</task>

</tasks>

<verification>
- DataLoader correctly auto-detects optimal worker count based on CPUs and GPUs
- Memory manager configures expandable segments when requested
- Cache clearing happens at specified intervals
- Tests pass for both CUDA and CPU environments
</verification>

<success_criteria>
- get_optimal_workers() returns min(cpu_count//num_gpus, 8) correctly
- DataLoader created with prefetch_factor=2 and persistent_workers=True
- MemoryManager tracks memory usage and clears cache periodically
- Expandable segments configurable via environment variable
- All tests pass including memory tracking and fragmentation prevention
</success_criteria>

<output>
After completion, create `.planning/phases/04-memory-attention/04-02-SUMMARY.md`
</output>