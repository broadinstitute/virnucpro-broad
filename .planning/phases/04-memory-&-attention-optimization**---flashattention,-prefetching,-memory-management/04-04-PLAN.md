---
phase: 04-memory-attention
plan: 04
type: execute
wave: 3
depends_on: [04-01, 04-02, 04-03]
files_modified: [virnucpro/cli/predict.py, virnucpro/pipeline/prediction.py, virnucpro/models/dnabert_flash.py, tests/integration/test_memory_attention_integration.py]
autonomous: true
user_setup:
  - service: flash-attention
    why: "FlashAttention-2 for attention optimization"
    notes:
      - instruction: "Install flash-attn package (requires CUDA 12.0+ and Ampere+ GPU)"
      - command: "MAX_JOBS=4 pip install flash-attn --no-build-isolation"
must_haves:
  truths:
    - "CLI exposes memory optimization flags to users"
    - "Pipeline integrates all memory optimizations seamlessly"
    - "DNABERT-S also benefits from FlashAttention-2"
  artifacts:
    - path: "virnucpro/cli/predict.py"
      provides: "CLI flags for memory optimization"
      contains: "--dataloader-workers, --pin-memory, --expandable-segments, --cuda-streams"
    - path: "virnucpro/pipeline/prediction.py"
      provides: "Integrated memory optimizations in pipeline"
      contains: "MemoryManager, create_optimized_dataloader"
    - path: "virnucpro/models/dnabert_flash.py"
      provides: "DNABERT-S with FlashAttention-2"
      exports: ["DNABERTWithFlashAttention"]
  key_links:
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/prediction.py"
      via: "Pass memory optimization flags"
      pattern: "dataloader_workers.*expandable_segments"
    - from: "virnucpro/pipeline/prediction.py"
      to: "virnucpro/cuda/memory_manager.py"
      via: "Use MemoryManager"
      pattern: "from.*memory_manager.*import.*MemoryManager"
---

<objective>
Integrate all memory optimizations into the pipeline with CLI control and add DNABERT-S FlashAttention-2 support.

Purpose: Complete the memory optimization phase by wiring everything together and exposing user controls.
Output: Fully integrated memory-optimized pipeline with CLI flags and DNABERT-S FlashAttention-2.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-CONTEXT.md

# Dependencies from previous waves
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-01-PLAN.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-02-PLAN.md
@.planning/phases/04-memory-&-attention-optimization**---flashattention,-prefetching,-memory-management/04-03-PLAN.md

# Existing files to modify
@virnucpro/cli/predict.py
@virnucpro/pipeline/prediction.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add memory optimization CLI flags</name>
  <files>virnucpro/cli/predict.py</files>
  <action>
    Add CLI flags for memory optimization control:

    1. Add DataLoader optimization flags:
       - `--dataloader-workers`: type=int, default=None, help="Number of DataLoader workers (default: auto-detect based on CPU/GPU ratio)"
       - `--pin-memory`: is_flag=True, default=None, help="Pin memory for faster GPU transfer (default: auto based on available RAM)"
    2. Add memory management flags:
       - `--expandable-segments`: is_flag=True, default=False, help="Enable expandable CUDA memory segments for fragmentation prevention"
       - `--cache-clear-interval`: type=int, default=100, help="Clear CUDA cache every N batches (0 to disable)"
    3. Add stream control flag:
       - `--cuda-streams/--no-cuda-streams`: default=True, help="Use CUDA streams for I/O overlap (default: enabled)"
    4. Update the predict() function signature to accept these new parameters
    5. Pass parameters to run_prediction() with proper naming:
       - dataloader_workers -> dataloader_workers
       - pin_memory -> pin_memory
       - expandable_segments -> expandable_segments
       - cache_clear_interval -> cache_clear_interval
       - cuda_streams -> cuda_streams
    6. Add validation:
       - Warn if --cuda-streams used without CUDA available
       - Log auto-detected values when flags not specified

    Follow existing CLI patterns in the file.
  </action>
  <verify>python -m virnucpro predict --help | grep -E "(dataloader|memory|segments|cuda-streams)"</verify>
  <done>CLI flags added for dataloader-workers, pin-memory, expandable-segments, cache-clear-interval, cuda-streams</done>
</task>

<task type="auto">
  <name>Task 2: Integrate memory optimizations into pipeline</name>
  <files>virnucpro/pipeline/prediction.py</files>
  <action>
    Wire memory optimizations into the main pipeline:

    1. Import new modules at top:
       - from virnucpro.cuda.memory_manager import MemoryManager, configure_memory_optimization
       - from virnucpro.data.dataloader_utils import create_optimized_dataloader, get_optimal_workers
       - from virnucpro.models.esm2_flash import load_esm2_model
    2. Update run_prediction() signature to accept memory optimization parameters
    3. At pipeline start (before any GPU work):
       - Initialize MemoryManager with expandable_segments and cache_clear_interval
       - Call configure_memory_optimization() if expandable_segments is True
       - Log memory optimization configuration
    4. Update parallel worker calls:
       - Pass cuda_streams flag to run_parallel_esm2() and run_parallel_dnabert()
       - Pass dataloader config to workers if they create DataLoaders
    5. Add memory management hooks:
       - After each major stage, check if should_clear_cache()
       - Log memory status at key checkpoints
       - Add memory tracking around ESM-2 and DNABERT-S stages
    6. Update model loading:
       - Use load_esm2_model() instead of direct ESM loading (if applicable)
       - Log which attention implementation is active
    7. Add graceful OOM handling:
       - Wrap GPU operations in try/except for RuntimeError
       - On OOM, log memory diagnostics and suggest lower batch size
       - Exit with code 4 (new code for OOM)

    Maintain backward compatibility - all new features should be optional.
  </action>
  <verify>python -c "from virnucpro.pipeline.prediction import run_prediction; print('Pipeline with memory optimizations imports successfully')"</verify>
  <done>Memory optimizations integrated into pipeline with MemoryManager and optimized DataLoader</done>
</task>

<task type="auto">
  <name>Task 3: Add DNABERT-S FlashAttention-2 support and integration tests</name>
  <files>virnucpro/models/dnabert_flash.py, tests/integration/test_memory_attention_integration.py</files>
  <action>
    Complete integration with DNABERT-S FlashAttention and comprehensive tests:

    For virnucpro/models/dnabert_flash.py:
    1. Create DNABERTWithFlashAttention class similar to ESM2WithFlashAttention:
       - Import from transformers AutoModel, AutoTokenizer
       - Use attention_utils.configure_flash_attention()
       - Wrap DNABERT-S model with FlashAttention-2 support
    2. Implement load_dnabert_model(model_name="zhihan1996/DNABERT-S", device="cuda"):
       - Load base model with AutoModel.from_pretrained()
       - Configure FlashAttention-2 if available
       - Return (model, tokenizer) tuple
    3. Ensure compatibility with existing DNABERT code

    For tests/integration/test_memory_attention_integration.py:
    1. Test full pipeline with memory optimizations:
       - Create small test FASTA file
       - Run pipeline with all optimization flags
       - Verify successful completion
    2. Test FlashAttention-2 integration:
       - Load both ESM-2 and DNABERT-S models
       - Check attention implementation detection
       - Process sample sequences
    3. Test DataLoader optimization:
       - Verify worker count auto-detection
       - Check pin_memory configuration
       - Test with various num_gpus values
    4. Test memory management:
       - Mock memory operations
       - Verify cache clearing at intervals
       - Test expandable segments configuration
    5. Test CUDA streams:
       - Verify stream creation and usage
       - Test with --no-cuda-streams flag
    6. Test CLI integration:
       - Run predict command with various flag combinations
       - Verify flags propagate to pipeline
       - Check backward compatibility (no flags = still works)

    Mark GPU-required tests with @pytest.mark.gpu.
  </action>
  <verify>pytest tests/integration/test_memory_attention_integration.py -v -k "not gpu"</verify>
  <done>DNABERT-S FlashAttention-2 support added and comprehensive integration tests created</done>
</task>

</tasks>

<verification>
- CLI help shows all new memory optimization flags
- Pipeline successfully initializes MemoryManager with CLI parameters
- Both ESM-2 and DNABERT-S use FlashAttention-2 when available
- Integration tests pass with various optimization configurations
- Pipeline maintains backward compatibility without optimization flags
</verification>

<success_criteria>
- All memory optimization flags accessible via CLI
- MemoryManager integrated and clearing cache at specified intervals
- ESM-2 and DNABERT-S both benefit from FlashAttention-2
- DataLoader uses optimal worker configuration
- Integration tests verify end-to-end optimization workflow
- OOM errors handled gracefully with diagnostics
</success_criteria>

<output>
After completion, create `.planning/phases/04-memory-attention/04-04-SUMMARY.md`
</output>