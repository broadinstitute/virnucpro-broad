---
phase: 05-advanced-load-balancing
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - virnucpro/pipeline/load_dashboard.py
  - virnucpro/pipeline/load_balancer.py
  - virnucpro/pipeline/gpu_utilization.py
autonomous: true

must_haves:
  truths:
    - "Dashboard shows real-time per-GPU metrics"
    - "Work stealing events are visible in dashboard"
    - "Dashboard updates every 2-3 seconds without flicker"
    - "GPU utilization tracked via nvitop"
    - "Throughput metrics logged persistently"
    - "Stalled workers are detected and reported"
  artifacts:
    - path: "virnucpro/pipeline/load_dashboard.py"
      provides: "LoadBalancingDashboard class"
      min_lines: 150
      exports: ["LoadBalancingDashboard"]
    - path: "virnucpro/pipeline/gpu_utilization.py"
      provides: "GPU utilization monitoring via nvitop"
      min_lines: 100
      exports: ["GPUMonitor", "get_gpu_utilization"]
  key_links:
    - from: "load_dashboard.py"
      to: "rich.live"
      via: "Live context for updates"
      pattern: "from rich.live import Live"
    - from: "gpu_utilization.py"
      to: "nvitop"
      via: "nvitop library for GPU monitoring"
      pattern: "from nvitop import"
    - from: "load_dashboard.py"
      to: "load_balancer.py"
      via: "gets metrics from coordinator"
      pattern: "WorkStealingCoordinator"
---

<objective>
Create real-time monitoring dashboard showing per-GPU utilization, throughput, queue depths, and work stealing events with persistent logging and stalled worker detection.

Purpose: Provide visibility into GPU load distribution and work stealing effectiveness.
Output: LoadBalancingDashboard class with Rich-based live terminal UI and GPU utilization monitoring via nvitop.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-advanced-load-balancing/05-CONTEXT.md
@.planning/phases/05-advanced-load-balancing/05-RESEARCH.md

@virnucpro/pipeline/dashboard.py
@virnucpro/pipeline/gpu_monitor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPU utilization monitoring with nvitop</name>
  <files>virnucpro/pipeline/gpu_utilization.py</files>
  <action>
    Create gpu_utilization.py with GPUMonitor class using nvitop library for real GPU compute and memory utilization tracking.

    Implementation using nvitop (per MON-01 requirement):
    - Install nvitop if not present (pip install nvitop)
    - Initialize nvitop Device objects in __init__, handle gracefully if not available (CPU-only systems)
    - get_gpu_utilization(device_id) method returns:
      * compute_percent: GPU compute utilization % (0-100) via device.gpu_utilization()
      * memory_percent: GPU memory usage % (0-100) via device.memory_percent()
      * temperature: GPU temperature in Celsius via device.temperature()
      * power_watts: Current power draw via device.power_usage()
    - Cache results for 1-2 seconds to avoid excessive queries
    - Thread-safe for concurrent dashboard updates
    - Fallback to nvidia-smi subprocess if nvitop unavailable

    Key implementation:
    ```python
    from nvitop import Device

    class GPUMonitor:
        def __init__(self):
            try:
                self.devices = [Device(i) for i in range(Device.count())]
                self.use_nvitop = True
            except:
                self.use_nvitop = False

        def get_gpu_utilization(self, device_id):
            if self.use_nvitop:
                device = self.devices[device_id]
                return {
                    'compute_percent': device.gpu_utilization(),
                    'memory_percent': device.memory_percent(),
                    'temperature': device.temperature(),
                    'power_watts': device.power_usage() / 1000.0  # mW to W
                }
            else:
                # fallback to nvidia-smi
    ```

    MON-01 requirement: Use nvitop to log compute % and memory usage for actual GPU utilization tracking.
  </action>
  <verify>grep -n "class GPUMonitor" virnucpro/pipeline/gpu_utilization.py && grep -n "from nvitop import" virnucpro/pipeline/gpu_utilization.py</verify>
  <done>GPUMonitor class provides real GPU utilization metrics via nvitop library</done>
</task>

<task type="auto">
  <name>Task 2: Create LoadBalancingDashboard with persistent logging</name>
  <files>virnucpro/pipeline/load_dashboard.py</files>
  <action>
    Create load_dashboard.py with LoadBalancingDashboard class for real-time GPU metrics display.

    Implementation using Rich library (Pattern 3 from RESEARCH.md):
    - Use Rich Live context manager with Table regeneration
    - Update frequency: 2-3 seconds (configurable)
    - Regenerate entire table on update (simpler than mutating rows)

    Dashboard columns:
    - GPU ID
    - GPU Name (model)
    - Compute % (from GPUMonitor via nvitop - actual utilization, not estimated)
    - Memory % (from GPUMonitor via nvitop)
    - Throughput (sequences/sec, rolling average over 60s or last 10 files)
    - Queue Depth (files remaining)
    - Files Done (completed count)
    - Work Stolen (count of files stolen by/from this GPU)

    Key features:
    - Non-blocking dashboard update in background thread
    - TTY detection with fallback to logging if not interactive
    - Handle terminal resize gracefully (Rich does this automatically)
    - Show work stealing events in separate section below table
    - Recent events list (last 5-10 steal events with timestamp)
    - Persistent logging of throughput metrics to file (MON-02 requirement)

    Methods:
    - __init__(coordinator, gpu_monitor, update_interval=2.0, log_file="gpu_metrics.log")
    - start() - begins background update thread
    - stop() - cleanly shuts down dashboard
    - _generate_table(metrics) - creates Rich Table from current metrics
    - _collect_metrics() - gathers data from coordinator and GPUMonitor
    - _update_loop() - background thread that refreshes display
    - _log_metrics(metrics) - writes throughput/utilization to persistent log file

    MON-02 requirement: Log sequences/sec per GPU to file for analysis.
    Format: timestamp, gpu_id, sequences_per_sec, compute_percent, memory_percent
  </action>
  <verify>grep -n "class LoadBalancingDashboard" virnucpro/pipeline/load_dashboard.py && grep -n "from rich.live import Live" virnucpro/pipeline/load_dashboard.py</verify>
  <done>LoadBalancingDashboard class exists with Rich Live display for real-time metrics and persistent logging</done>
</task>

<task type="auto">
  <name>Task 3: Add metrics collection with stalled worker detection</name>
  <files>virnucpro/pipeline/load_balancer.py</files>
  <action>
    Update WorkStealingCoordinator to provide metrics for dashboard display, persistent logging, and stalled worker detection.

    Add methods:
    - get_gpu_metrics() - returns dict of per-GPU statistics:
      * device_id, device_name
      * queue_depth (current files in queue)
      * files_completed (processed count)
      * work_stolen_from (files stolen from this GPU)
      * work_stolen_to (files stolen by this GPU)
      * throughput (sequences/sec based on recent completions)
      * last_activity_time (for stall detection)
      * is_stalled (True if no activity for stall_threshold seconds)

    - get_recent_steal_events() - returns list of recent work stealing events:
      * timestamp, source_gpu, dest_gpu, file_name

    - record_file_completion(gpu_id, file_path, sequences, duration) - track throughput:
      * Store with timestamp for persistent logging
      * Calculate rolling average for dashboard display
      * Update last_activity_time for stall detection

    - detect_stalled_workers(stall_threshold=300) - identify stalled workers (MON-03):
      * Check each GPU's last_activity_time
      * Mark as stalled if no activity for stall_threshold seconds (default 5 minutes)
      * Return list of stalled GPU IDs
      * Log warning when worker detected as stalled

    Add internal tracking:
    - Per-GPU completion history with timestamps (for throughput calculation and logging)
    - Work stealing event log (circular buffer, keep last 50 events)
    - Queue depth snapshots for monitoring
    - Per-GPU last_activity_time tracking (updated on file start/completion)
    - Stalled worker detection with configurable threshold (MON-03 requirement)

    Stalled worker detection logic (MON-03 requirement):
    ```python
    def detect_stalled_workers(self, stall_threshold=300):
        current_time = time.time()
        stalled = []
        for gpu_id, last_activity in self.last_activity.items():
            if current_time - last_activity > stall_threshold and self.queue_depths[gpu_id] > 0:
                stalled.append(gpu_id)
                logger.warning(f"GPU {gpu_id} stalled - no activity for {current_time - last_activity:.0f}s")
        return stalled
    ```

    Calculate throughput as rolling average over 60 seconds or last 10 files.
    Include timestamp in all metrics for persistent logging (MON-02).
    MON-03 requirement: Detect stalled workers (no progress for N minutes) and imbalanced load (via queue depth differences).
  </action>
  <verify>grep -n "def get_gpu_metrics" virnucpro/pipeline/load_balancer.py && grep -n "def detect_stalled_workers" virnucpro/pipeline/load_balancer.py</verify>
  <done>WorkStealingCoordinator provides metrics collection with stalled worker detection (MON-03)</done>
</task>

</tasks>

<verification>
- GPUMonitor tracks real GPU compute % and memory % via nvitop (MON-01)
- LoadBalancingDashboard displays real-time GPU metrics with actual utilization
- Dashboard persistently logs throughput metrics to file (MON-02)
- Stalled worker detection identifies GPUs with no progress (MON-03)
- Rich Live updates table every 2-3 seconds
- Work stealing events logged and displayed
- WorkStealingCoordinator tracks metrics with timestamps and stall detection
</verification>

<success_criteria>
- Dashboard shows actual GPU utilization % from nvitop (not estimated)
- Throughput (sequences/sec per GPU) logged to persistent file
- Stalled workers detected and logged when no activity for threshold period
- Dashboard updates live without flicker (Rich handles this)
- Per-GPU metrics visible in tabular format
- Work stealing events shown with timestamps
- Color-coded utilization for quick status assessment
- TTY detection with logging fallback for non-interactive
</success_criteria>

<output>
After completion, create `.planning/phases/05-advanced-load-balancing/05-03-SUMMARY.md`
</output>