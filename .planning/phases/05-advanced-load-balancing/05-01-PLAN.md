---
phase: 05-advanced-load-balancing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/load_balancer.py
  - virnucpro/pipeline/work_queue.py
autonomous: true

must_haves:
  truths:
    - "Idle GPUs can steal work from busy GPU queues"
    - "Work stealing happens automatically when imbalance detected"
    - "Multiple GPUs can share work queues safely"
  artifacts:
    - path: "virnucpro/pipeline/load_balancer.py"
      provides: "WorkStealingCoordinator class"
      min_lines: 200
      exports: ["WorkStealingCoordinator", "create_shared_work_queues"]
  key_links:
    - from: "load_balancer.py"
      to: "multiprocessing.Manager"
      via: "Manager().Queue() for shared queues"
      pattern: "Manager.*Queue"
    - from: "work_queue.py"
      to: "load_balancer.py"
      via: "imports WorkStealingCoordinator"
      pattern: "from.*load_balancer import"
---

<objective>
Create work stealing infrastructure with shared queues and coordinator for dynamic load rebalancing across GPUs.

Purpose: Enable idle GPUs to automatically steal work from busy GPUs to maximize overall utilization.
Output: WorkStealingCoordinator class and shared queue management functions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-advanced-load-balancing/05-CONTEXT.md
@.planning/phases/05-advanced-load-balancing/05-RESEARCH.md

@virnucpro/pipeline/work_queue.py
@virnucpro/pipeline/base_worker.py
@virnucpro/pipeline/persistent_pool.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create WorkStealingCoordinator class</name>
  <files>virnucpro/pipeline/load_balancer.py</files>
  <action>
    Create load_balancer.py with WorkStealingCoordinator class that manages shared work queues for multi-GPU processing.

    Key implementation details:
    - Use multiprocessing.Manager().Queue() for process-safe shared queues (avoids deadlocks vs regular Queue)
    - Create per-GPU work queues that workers can access across process boundaries
    - Implement try_steal_work() method using get_nowait() with try-except Empty pattern (never use qsize() or empty() - race conditions)
    - Polling interval: 2-5 seconds (configurable) - files take minutes to process, don't over-poll
    - Victim selection: Round-robin or random (simple is sufficient for large files)
    - Imbalance threshold: max_queue >= 3 AND (max > 2Ã—min OR max - min >= 3) - conservative to avoid thrashing
    - Keep Manager object reference alive for queue lifetime (return with queues, shutdown when done)
    - Thread-safe monitoring of queue depths and work stealing events

    The coordinator should track:
    - Queue depths per GPU
    - Work stealing events (source, destination, timestamp)
    - Files processed per GPU
    - Throughput metrics (sequences/sec calculated over 60s or last 10 files)

    Follow patterns from RESEARCH.md Pattern 1 (shared queues) and Pattern 2 (non-blocking operations).
  </action>
  <verify>grep -n "class WorkStealingCoordinator" virnucpro/pipeline/load_balancer.py && grep -n "Manager().Queue" virnucpro/pipeline/load_balancer.py</verify>
  <done>load_balancer.py exists with WorkStealingCoordinator class using Manager.Queue for shared work queues</done>
</task>

<task type="auto">
  <name>Task 2: Integrate work stealing with BatchQueueManager</name>
  <files>virnucpro/pipeline/work_queue.py</files>
  <action>
    Update BatchQueueManager to support work stealing coordination.

    Add to BatchQueueManager:
    - Optional work_stealing_enabled parameter (default: False for backward compatibility)
    - Integration with WorkStealingCoordinator when enabled
    - Pass shared queues to workers via initializer pattern (not as arguments - spawn context issue)
    - Worker function modifications to check own queue then try stealing if empty
    - Periodic work stealing check (every 2-5 seconds via configurable interval)
    - Logging of work stealing events for debugging

    Key patterns to follow:
    - Pass queues via Pool initializer, not worker arguments (avoids pickling issues in spawn context)
    - Use global _worker_queue in worker functions (same pattern as existing _esm_model, _batch_converter)
    - Check own queue first, only steal when idle
    - Skip locked queues when stealing (don't block)
    - Report stolen work to coordinator for metrics

    Ensure backward compatibility - work stealing is opt-in via parameter.
  </action>
  <verify>grep -n "work_stealing_enabled" virnucpro/pipeline/work_queue.py && grep -n "WorkStealingCoordinator" virnucpro/pipeline/work_queue.py</verify>
  <done>BatchQueueManager integrates with WorkStealingCoordinator for optional work stealing</done>
</task>

</tasks>

<verification>
- WorkStealingCoordinator class exists with shared queue management
- Manager.Queue used for process-safe queue sharing
- try_steal_work() method with proper exception handling
- BatchQueueManager integrates work stealing as opt-in feature
- Worker functions can steal from other GPU queues when idle
</verification>

<success_criteria>
- Shared work queues accessible across GPU worker processes
- Idle workers can steal work from busy workers
- Work stealing triggers based on queue depth imbalance
- No deadlocks or race conditions in queue operations
- Backward compatible (work stealing is opt-in)
</success_criteria>

<output>
After completion, create `.planning/phases/05-advanced-load-balancing/05-01-SUMMARY.md`
</output>