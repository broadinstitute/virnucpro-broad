---
phase: 05-async-dataloader-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/data/sequence_dataset.py
  - virnucpro/data/collators.py
  - virnucpro/data/__init__.py
autonomous: true

must_haves:
  truths:
    - "DataLoader workers parse FASTA files and yield raw sequence strings"
    - "Workers have no CUDA access (CUDA_VISIBLE_DEVICES='' validated)"
    - "Collator tokenizes sequences in main process only"
    - "Collator produces packed format with cu_seqlens for FlashAttention varlen"
  artifacts:
    - path: "virnucpro/data/sequence_dataset.py"
      provides: "CUDA-safe IterableDataset for FASTA streaming"
      exports: ["SequenceDataset"]
      min_lines: 80
    - path: "virnucpro/data/collators.py"
      provides: "Varlen collator for FlashAttention packed format"
      exports: ["VarlenCollator"]
      min_lines: 60
  key_links:
    - from: "virnucpro/data/sequence_dataset.py"
      to: "Bio.SeqIO"
      via: "FASTA parsing in worker"
      pattern: "SeqIO\\.parse"
    - from: "virnucpro/data/collators.py"
      to: "ESM batch_converter"
      via: "tokenization in collate_fn"
      pattern: "batch_converter"
---

<objective>
Create CUDA-safe data loading components for async DataLoader architecture.

Purpose: Establish the foundation for async DataLoader where CPU workers handle pure I/O (FASTA parsing) while the main process performs tokenization. Workers must NEVER initialize CUDA to prevent context corruption.

Output:
- `virnucpro/data/sequence_dataset.py`: IterableDataset with CUDA safety validation
- `virnucpro/data/collators.py`: VarlenCollator producing packed batch format
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-async-dataloader-foundation/05-CONTEXT.md
@.planning/phases/05-async-dataloader-foundation/05-RESEARCH.md

# Existing patterns to follow
@virnucpro/data/dataloader_utils.py
@virnucpro/models/esm2_flash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CUDA-safe SequenceDataset</name>
  <files>virnucpro/data/sequence_dataset.py</files>
  <action>
Create `SequenceDataset(IterableDataset)` that:

1. **Constructor (`__init__`):**
   - Accept `fasta_files: List[Path]` and `max_length: int = 1024`
   - Store files and max_length
   - Do NOT validate CUDA in __init__ (runs in main process during Dataset creation)

2. **CUDA validation method (`_validate_cuda_isolation`):**
   - Call in `__iter__` at start (when running in worker)
   - Check `get_worker_info()` - if None, skip validation (main process)
   - If in worker: assert `os.environ.get('CUDA_VISIBLE_DEVICES') == ''`
   - If in worker: assert `torch.cuda.is_available() == False`
   - Raise RuntimeError with clear message if CUDA detected: "Worker {id}: CUDA access detected. Workers must be CPU-only."
   - Set `self._validated = True` to skip on subsequent calls

3. **Iterator (`__iter__`):**
   - Call `_validate_cuda_isolation()` once at start
   - Get worker_info via `get_worker_info()`
   - If worker_info is None (single-process): process all files
   - If worker_info exists: shard files by `i % num_workers == worker_id`
   - For each file, use `Bio.SeqIO.parse(file_path, 'fasta')`
   - Yield dict with: `{'id': record.id, 'sequence': str(record.seq)[:max_length], 'file': file_path.name}`

4. **Add module docstring** explaining:
   - Purpose: CPU-only FASTA streaming for async DataLoader
   - Safety: Workers NEVER touch CUDA
   - Pattern: Yields raw strings, tokenization in collate_fn

Import: torch, Bio.SeqIO, Path, List, get_worker_info, os, IterableDataset
  </action>
  <verify>
```bash
python -c "
from virnucpro.data.sequence_dataset import SequenceDataset
from pathlib import Path
import tempfile
import os

# Create test FASTA
with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
    f.write('>seq1\nMKTAYIAK\n>seq2\nVLSPADKTNV\n')
    test_file = f.name

# Test basic iteration
ds = SequenceDataset([Path(test_file)])
items = list(ds)
assert len(items) == 2, f'Expected 2 items, got {len(items)}'
assert items[0]['id'] == 'seq1', f'Expected seq1, got {items[0][\"id\"]}'
assert 'sequence' in items[0] and 'file' in items[0]
print('SequenceDataset basic test passed')

os.unlink(test_file)
"
```
  </verify>
  <done>SequenceDataset parses FASTA files and yields sequence dicts with id, sequence, file keys</done>
</task>

<task type="auto">
  <name>Task 2: Create VarlenCollator for FlashAttention format</name>
  <files>virnucpro/data/collators.py</files>
  <action>
Create `VarlenCollator` class that tokenizes sequences in main process and produces packed format:

1. **Constructor (`__init__`):**
   - Accept `batch_converter` (ESM alphabet.get_batch_converter())
   - Accept `max_tokens: int = 4096` (token budget per batch)
   - Store both as instance attributes

2. **Call method (`__call__`):**
   - Input: `batch: List[Dict[str, str]]` (from SequenceDataset)
   - Extract sequences: `[(item['id'], item['sequence']) for item in batch]`
   - Tokenize using ESM batch_converter: `labels, strs, tokens = self.batch_converter(sequences)`
   - Build packed format:
     - Initialize `all_tokens = []`, `cu_seqlens = [0]`
     - For each sequence in tokens (skip padding):
       - Get actual tokens (non-padding)
       - If `cu_seqlens[-1] + len(seq_tokens) > max_tokens`: stop packing
       - Append seq_tokens to all_tokens
       - Append cumulative length to cu_seqlens
   - Return dict:
     - `'input_ids'`: `torch.tensor(all_tokens, dtype=torch.long)`
     - `'cu_seqlens'`: `torch.tensor(cu_seqlens, dtype=torch.int32)`
     - `'max_seqlen'`: max individual sequence length in batch
     - `'sequence_ids'`: list of sequence IDs that were packed
     - `'num_sequences'`: number of sequences packed

3. **Handle padding detection:**
   - ESM uses padding token (typically 1) - detect by looking at actual sequence lengths
   - Use `strs` from batch_converter to get actual lengths, not padded tensor

4. **Add module docstring** explaining:
   - Purpose: Tokenize in main process for CUDA safety
   - Output format: Packed 1D tensor with cu_seqlens for FlashAttention varlen
   - Integration: Used as collate_fn with SequenceDataset

Import: torch, List, Dict, Any, Optional
  </action>
  <verify>
```bash
python -c "
import torch
import esm
from virnucpro.data.collators import VarlenCollator

# Load ESM alphabet for batch_converter
_, alphabet = esm.pretrained.esm2_t33_650M_UR50D()
batch_converter = alphabet.get_batch_converter()

# Create collator
collator = VarlenCollator(batch_converter, max_tokens=100)

# Test batch
batch = [
    {'id': 'seq1', 'sequence': 'MKTAYIAK', 'file': 'test.fasta'},
    {'id': 'seq2', 'sequence': 'VLSPADKTNV', 'file': 'test.fasta'},
]

result = collator(batch)

assert 'input_ids' in result, 'Missing input_ids'
assert 'cu_seqlens' in result, 'Missing cu_seqlens'
assert 'max_seqlen' in result, 'Missing max_seqlen'
assert 'sequence_ids' in result, 'Missing sequence_ids'
assert 'num_sequences' in result, 'Missing num_sequences'

assert result['input_ids'].dtype == torch.long, f'Wrong dtype: {result[\"input_ids\"].dtype}'
assert result['cu_seqlens'].dtype == torch.int32, f'Wrong cu_seqlens dtype: {result[\"cu_seqlens\"].dtype}'
assert len(result['cu_seqlens']) == result['num_sequences'] + 1, 'cu_seqlens length mismatch'

print(f'VarlenCollator test passed: {result[\"num_sequences\"]} sequences packed')
"
```
  </verify>
  <done>VarlenCollator tokenizes sequences and produces packed format with input_ids (1D tensor) and cu_seqlens for FlashAttention varlen</done>
</task>

<task type="auto">
  <name>Task 3: Update data module exports</name>
  <files>virnucpro/data/__init__.py</files>
  <action>
Update `virnucpro/data/__init__.py` to export new classes:

1. Add imports for new modules:
   - `from virnucpro.data.sequence_dataset import SequenceDataset`
   - `from virnucpro.data.collators import VarlenCollator`

2. Add to `__all__` list:
   - `'SequenceDataset'`
   - `'VarlenCollator'`

Keep existing exports from dataloader_utils.py intact.
  </action>
  <verify>
```bash
python -c "
from virnucpro.data import SequenceDataset, VarlenCollator
print('Exports work: SequenceDataset and VarlenCollator importable from virnucpro.data')
"
```
  </verify>
  <done>SequenceDataset and VarlenCollator importable from virnucpro.data</done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `python -c "from virnucpro.data import SequenceDataset, VarlenCollator"` succeeds
2. SequenceDataset yields sequence dicts from FASTA files
3. VarlenCollator produces packed format with cu_seqlens
4. No CUDA imports or initialization in worker-accessible code paths
</verification>

<success_criteria>
- SequenceDataset parses FASTA files in CPU workers
- CUDA validation raises RuntimeError if worker has CUDA access
- VarlenCollator tokenizes using ESM batch_converter
- Packed output includes input_ids (1D long tensor), cu_seqlens (int32), sequence_ids
- Both classes importable from virnucpro.data
</success_criteria>

<output>
After completion, create `.planning/phases/05-async-dataloader-foundation/05-01-SUMMARY.md`
</output>
