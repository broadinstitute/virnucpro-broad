---
phase: 05-async-dataloader-foundation
plan: 04
type: execute
wave: 3
depends_on:
  - "05-01"
  - "05-02"
  - "05-03"
files_modified:
  - virnucpro/pipeline/async_inference.py
  - virnucpro/pipeline/__init__.py
autonomous: true

must_haves:
  truths:
    - "Single GPU process handles all inference for assigned data"
    - "Batches prefetched via async DataLoader while GPU computes"
    - "GPU receives pinned tensors via non_blocking transfer"
    - "CUDA streams overlap data transfer with compute"
    - "GPU utilization monitored with tiered bottleneck detection (50%/80%)"
    - "FIX 1: Inter-batch arrival time measured (not processing time)"
    - "FIX 2: Single GPU transfer per batch (no double transfer waste)"
    - "FIX 3: Packed format raises NotImplementedError until Phase 6"
    - "FIX 5: Pinned memory validated on first batch"
    - "FIX 6: DataLoader exceptions caught and re-raised with context"
    - "FIX 7: Embeddings converted FP16→FP32 for stability"
    - "FIX 8: sequence_ids required in batch (no synthetic ID generation)"
  artifacts:
    - path: "virnucpro/pipeline/async_inference.py"
      provides: "Async inference loop for single-GPU processing"
      exports: ["AsyncInferenceRunner", "run_async_inference", "InferenceResult"]
      min_lines: 200
  key_links:
    - from: "virnucpro/pipeline/async_inference.py"
      to: "virnucpro/data/dataloader_utils.py"
      via: "create_async_dataloader"
      pattern: "create_async_dataloader"
    - from: "virnucpro/pipeline/async_inference.py"
      to: "virnucpro/cuda/stream_manager.py"
      via: "StreamProcessor for async GPU ops"
      pattern: "StreamProcessor"
    - from: "virnucpro/pipeline/async_inference.py"
      to: "virnucpro/utils/gpu_monitor.py"
      via: "NvitopMonitor for performance tracking"
      pattern: "NvitopMonitor"
---

<objective>
Create async inference runner that processes batches on single GPU with stream-based I/O overlap.

Purpose: Implement the core inference loop where GPU receives prefetched batches via pinned memory transfer while CUDA streams overlap data movement with computation. This is the single-GPU foundation that Phase 7 will scale to multi-GPU.

Output:
- `virnucpro/pipeline/async_inference.py`: AsyncInferenceRunner class and run_async_inference function
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-async-dataloader-foundation/05-CONTEXT.md
@.planning/phases/05-async-dataloader-foundation/05-RESEARCH.md

# Prior plan outputs
@.planning/phases/05-async-dataloader-foundation/05-01-SUMMARY.md
@.planning/phases/05-async-dataloader-foundation/05-02-SUMMARY.md
@.planning/phases/05-async-dataloader-foundation/05-03-SUMMARY.md

# Existing code to integrate
@virnucpro/cuda/stream_manager.py
@virnucpro/models/esm2_flash.py
@virnucpro/utils/gpu_monitor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AsyncInferenceRunner class</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Create new file `virnucpro/pipeline/async_inference.py` with the `AsyncInferenceRunner` class:

```python
"""Async inference runner for single-GPU processing with stream overlap.

This module implements the async DataLoader architecture where:
- DataLoader workers handle CPU-only I/O (FASTA parsing)
- Main process tokenizes in collate_fn
- GPU receives prefetched, pinned batches
- CUDA streams overlap data transfer with computation

This is the single-GPU foundation. Multi-GPU coordination is Phase 7.
"""

import torch
import time
import logging
from typing import Optional, List, Dict, Any, Callable, Iterator
from pathlib import Path
from dataclasses import dataclass

from torch.utils.data import DataLoader

from virnucpro.cuda.stream_manager import StreamProcessor
from virnucpro.utils.gpu_monitor import NvitopMonitor

logger = logging.getLogger('virnucpro.pipeline.async_inference')


@dataclass
class InferenceResult:
    """Result from async inference."""
    sequence_ids: List[str]
    embeddings: torch.Tensor  # Shape: (num_sequences, embedding_dim)
    batch_idx: int


class AsyncInferenceRunner:
    """
    Single-GPU async inference runner with stream-based I/O overlap.

    Architecture:
    1. DataLoader workers parse FASTA files (CPU-only)
    2. Collate_fn tokenizes in main process
    3. Pinned tensors transferred to GPU (non_blocking)
    4. CUDA streams overlap H2D, compute, D2H
    5. GPU monitor tracks utilization and bottlenecks

    Attributes:
        device: CUDA device for inference
        model: Model with forward() method (e.g., ESM2WithFlashAttention)
        stream_processor: CUDA stream orchestrator for async ops
        monitor: GPU utilization and DataLoader metrics tracker
    """

    def __init__(
        self,
        model: torch.nn.Module,
        device: torch.device,
        enable_streams: bool = True,
        monitor_interval: float = 1.0,
        log_file: Optional[Path] = None,
    ):
        """
        Initialize async inference runner.

        Args:
            model: Model for inference (already on device)
            device: CUDA device (e.g., torch.device('cuda:0'))
            enable_streams: Use CUDA streams for I/O overlap (default: True)
            monitor_interval: GPU sampling interval in seconds (default: 1.0)
            log_file: Path for GPU metrics log (default: auto-generated)
        """
        self.model = model
        self.device = device
        self.enable_streams = enable_streams

        # Initialize stream processor for async GPU ops
        self.stream_processor = StreamProcessor(
            device=device,
            enable_streams=enable_streams,
            verbose=False
        )

        # Initialize GPU monitor with DataLoader tracking
        device_id = device.index if device.index is not None else 0
        self.monitor = NvitopMonitor(
            device_ids=[device_id],
            log_interval=monitor_interval,
            log_file=log_file
        )

        # Inference state
        self._batch_count = 0
        self._total_sequences = 0

        logger.info(
            f"AsyncInferenceRunner initialized on {device} "
            f"(streams={'enabled' if enable_streams else 'disabled'})"
        )

    def _validate_pinned_memory(self, batch: Dict[str, Any]) -> None:
        """
        FIX 5: Validate tensors are actually pinned (critical for performance).

        Called on first batch to ensure pin_memory=True is working.
        """
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                if not value.is_pinned():
                    logger.warning(
                        f"Tensor '{key}' is NOT pinned! "
                        f"Ensure DataLoader has pin_memory=True. "
                        f"Performance will be degraded."
                    )
                else:
                    logger.debug(f"Tensor '{key}' is pinned correctly")

    def _transfer_to_gpu(self, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """Transfer batch tensors to GPU with non_blocking for pinned memory."""
        gpu_batch = {}
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                gpu_batch[key] = value.to(self.device, non_blocking=True)
            else:
                gpu_batch[key] = value  # Keep non-tensor data as-is
        return gpu_batch

    def _run_inference(self, gpu_batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Run model inference on GPU batch.

        FIX 7: Embeddings are computed in FP16 (via model.half() or autocast)
        but returned as FP32 for stability. Conversion happens in _extract_embeddings.

        FIX 3: Phase 5 uses standard attention (padded). Phase 6 will replace
        this with FlashAttention varlen (flash_attn_varlen_func with cu_seqlens).
        """
        with torch.no_grad():
            # ESM-2 forward expects tokens tensor
            input_ids = gpu_batch['input_ids']

            # FIX 3: Packed format handling
            if 'cu_seqlens' in gpu_batch:
                # VarlenCollator produces packed format (1D concatenated tokens)
                # TODO PHASE 6: Replace with FlashAttention varlen call
                # For Phase 5: Raise NotImplementedError to be explicit
                raise NotImplementedError(
                    "Packed batches with cu_seqlens require FlashAttention varlen "
                    "(Phase 6: Sequence Packing Integration). "
                    "For Phase 5 testing, use unpacked batches or wait for Phase 6."
                )
            else:
                # Standard padded format (Phase 5 baseline)
                if input_ids.dim() == 1:
                    input_ids = input_ids.unsqueeze(0)

                # Run model forward
                # Note: Model may be in FP16 (model.half()) or use autocast
                outputs = self.model(input_ids, repr_layers=[36])
                representations = outputs['representations'][36]

                return representations

    def _extract_embeddings(
        self,
        representations: torch.Tensor,
        gpu_batch: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """
        Extract per-sequence embeddings from model output.

        FIX 7: Input representations may be FP16 (from model.half() or autocast).
        Output is always FP32 for numerical stability in downstream operations.

        Args:
            representations: Model output (batch_size, seq_len, hidden_dim)
                May be FP16 depending on model precision
            gpu_batch: Batch dict with tensors already on GPU
                (passed from process_batch to avoid double transfer)

        Returns:
            Embeddings tensor in FP32 (num_sequences, hidden_dim)
        """
        cu_seqlens = gpu_batch.get('cu_seqlens')
        sequence_ids = gpu_batch.get('sequence_ids', [])

        if cu_seqlens is not None and len(sequence_ids) > 0:
            # Packed format: extract embeddings using cu_seqlens boundaries
            # Mean-pool each sequence's tokens
            embeddings = []
            for i in range(len(sequence_ids)):
                start = cu_seqlens[i].item()
                end = cu_seqlens[i + 1].item()
                # Skip BOS token (position 0), mean pool the rest
                seq_repr = representations[0, start+1:end].mean(dim=0)
                embeddings.append(seq_repr)

            result = torch.stack(embeddings)
        else:
            # Single sequence or non-packed: mean pool (skip BOS)
            result = representations[0, 1:].mean(dim=0, keepdim=True)

        # FIX 7: Convert to FP32 for numerical stability
        # Even if model runs in FP16, embeddings stored/compared in FP32
        return result.float()

    def process_batch(self, batch: Dict[str, Any]) -> InferenceResult:
        """
        Process single batch through async pipeline.

        Args:
            batch: Batch from DataLoader (collated by VarlenCollator)
                MUST contain 'sequence_ids' key for traceability

        Returns:
            InferenceResult with sequence IDs and embeddings

        Raises:
            ValueError: If batch missing required 'sequence_ids' key

        Note:
            FIX 4: This method uses StreamProcessor.process_batch_async which
            must have signature: (batch, transfer_fn, compute_fn, retrieve_fn)
            Verify StreamProcessor interface matches before execution.
        """
        # FIX 8: Require sequence_ids for traceability (don't generate synthetic IDs)
        if 'sequence_ids' not in batch:
            raise ValueError(
                "Batch missing 'sequence_ids'. VarlenCollator must include "
                "sequence IDs in output for traceability."
            )

        sequence_ids = batch['sequence_ids']

        # FIX 2 & FIX 4: Single GPU transfer via StreamProcessor
        # StreamProcessor handles: transfer → compute → (optionally retrieve)
        # We keep representations on GPU to avoid D2H for cu_seqlens

        # Store gpu_batch reference from transfer_fn for _extract_embeddings
        gpu_batch_ref = {}

        def transfer_fn(b):
            gpu_b = self._transfer_to_gpu(b)
            gpu_batch_ref.update(gpu_b)  # Save for extraction step
            return gpu_b

        def compute_fn(gpu_b):
            return self._run_inference(gpu_b)

        # FIX 4: Verify StreamProcessor.process_batch_async signature
        # Expected: (batch, transfer_fn, compute_fn, retrieve_fn) -> result
        representations = self.stream_processor.process_batch_async(
            batch,
            transfer_fn=transfer_fn,
            compute_fn=compute_fn,
            retrieve_fn=None  # Keep on GPU for embedding extraction
        )

        # FIX 2: Use gpu_batch_ref (already on GPU) instead of re-transferring
        # Extract embeddings (representations and gpu_batch_ref both on GPU)
        embeddings = self._extract_embeddings(representations, gpu_batch_ref)

        # FIX 7: Embeddings already converted to FP32 in _extract_embeddings
        # Move to CPU for storage
        embeddings_cpu = embeddings.cpu()

        self._batch_count += 1
        self._total_sequences += len(sequence_ids)

        return InferenceResult(
            sequence_ids=sequence_ids,
            embeddings=embeddings_cpu,
            batch_idx=self._batch_count - 1
        )

    def run(
        self,
        dataloader: DataLoader,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> Iterator[InferenceResult]:
        """
        Run inference on all batches from DataLoader.

        Args:
            dataloader: Async DataLoader with prefetched batches
            progress_callback: Optional callback(batch_idx, num_sequences)

        Yields:
            InferenceResult for each batch

        Raises:
            RuntimeError: If DataLoader fails or worker crashes
        """
        self.model.eval()
        self.monitor.start_monitoring()
        self.monitor.start_inference_timer()
        self.monitor.set_stage('inference')

        logger.info("Starting async inference loop")

        # FIX 1: Track inter-batch arrival time (not processing time)
        last_batch_time = time.perf_counter()

        try:
            # FIX 6: Wrap DataLoader iteration with exception handling
            dataloader_iter = iter(dataloader)
            batch_idx = 0

            while True:
                try:
                    # FIX 1: Measure time BEFORE fetching (inter-batch arrival)
                    fetch_start = time.perf_counter()
                    batch = next(dataloader_iter)
                    fetch_time_ms = (time.perf_counter() - fetch_start) * 1000

                except StopIteration:
                    break
                except Exception as e:
                    logger.error(f"DataLoader failed at batch {batch_idx}: {e}")
                    # Ensure monitor stops before re-raising
                    self.stream_processor.synchronize()
                    self.monitor.stop_monitoring()
                    raise RuntimeError(
                        f"DataLoader failed at batch {batch_idx}. "
                        f"Check worker logs and CUDA isolation."
                    ) from e

                # FIX 5: Validate memory pinning (critical for performance)
                if batch_idx == 0:
                    self._validate_pinned_memory(batch)

                # Process batch
                result = self.process_batch(batch)

                # FIX 1: Calculate batch composition metrics
                num_sequences = len(result.sequence_ids)
                tokens_in_batch = batch.get('input_ids', torch.tensor([])).numel()
                avg_seq_len = tokens_in_batch / num_sequences if num_sequences > 0 else 0
                max_seq_len = batch.get('max_seqlen', 0)

                # Record DataLoader metrics
                self.monitor.record_dataloader_wait(
                    wait_time_ms=fetch_time_ms,
                    batch_idx=batch_idx,
                    sequences_in_batch=num_sequences,
                    tokens_in_batch=tokens_in_batch,
                    avg_sequence_length=avg_seq_len,
                    max_sequence_length=max_seq_len
                )

                # Check for bottleneck every 10 batches
                if batch_idx % 10 == 0 and batch_idx > 0:
                    is_bottleneck, severity, avg_util = self.monitor.check_bottleneck()
                    self.monitor.sample()  # Take GPU utilization sample

                # Progress callback
                if progress_callback:
                    progress_callback(batch_idx, num_sequences)

                yield result
                batch_idx += 1

        finally:
            # Synchronize streams before stopping
            self.stream_processor.synchronize()
            stats = self.monitor.stop_monitoring()

            throughput = self.monitor.get_throughput()
            dl_stats = self.monitor.get_dataloader_statistics()

            logger.info(
                f"Async inference complete: {self._total_sequences} sequences, "
                f"{throughput.get('sequences_per_sec', 0):.1f} seq/s, "
                f"{throughput.get('tokens_per_sec', 0):.0f} tokens/s"
            )
            if dl_stats:
                logger.info(
                    f"DataLoader stats: avg_wait={dl_stats.get('avg_wait_time_ms', 0):.1f}ms, "
                    f"max_wait={dl_stats.get('max_wait_time_ms', 0):.1f}ms, "
                    f"packing_efficiency={dl_stats.get('packing_efficiency', 0):.2%}"
                )

    def get_statistics(self) -> Dict[str, Any]:
        """Get inference statistics."""
        return {
            'total_batches': self._batch_count,
            'total_sequences': self._total_sequences,
            'gpu_stats': self.monitor.get_statistics(),
            'dataloader_stats': self.monitor.get_dataloader_statistics(),
            'throughput': self.monitor.get_throughput(),
        }
```
  </action>
  <verify>
```bash
python -c "
from virnucpro.pipeline.async_inference import AsyncInferenceRunner, InferenceResult
import torch

# Check class exists with expected methods
assert hasattr(AsyncInferenceRunner, '__init__')
assert hasattr(AsyncInferenceRunner, 'process_batch')
assert hasattr(AsyncInferenceRunner, 'run')
assert hasattr(AsyncInferenceRunner, 'get_statistics')
assert hasattr(AsyncInferenceRunner, '_validate_pinned_memory'), 'FIX 5: Missing pinned memory validation'

# Check InferenceResult dataclass
result = InferenceResult(
    sequence_ids=['seq1', 'seq2'],
    embeddings=torch.randn(2, 2560),
    batch_idx=0
)
assert len(result.sequence_ids) == 2

# FIX 8: Test that sequence_ids is required
try:
    from unittest.mock import Mock
    runner = AsyncInferenceRunner(
        model=Mock(),
        device=torch.device('cpu'),
        enable_streams=False
    )
    # process_batch should raise ValueError without sequence_ids
    batch_no_ids = {'input_ids': torch.tensor([1, 2, 3])}
    try:
        runner.process_batch(batch_no_ids)
        assert False, 'Should have raised ValueError for missing sequence_ids'
    except ValueError as e:
        assert 'sequence_ids' in str(e).lower()
        print('✓ FIX 8: sequence_ids requirement enforced')
except ImportError:
    print('⚠ Mock not available, skipping sequence_ids test')

print('AsyncInferenceRunner class structure verified with all fixes')
"
```
  </verify>
  <done>AsyncInferenceRunner created with: inter-batch timing (FIX 1), single GPU transfer (FIX 2), Phase 6 TODO for packed format (FIX 3), StreamProcessor interface note (FIX 4), pinned memory validation (FIX 5), DataLoader exception handling (FIX 6), FP16→FP32 documentation (FIX 7), sequence_ids requirement (FIX 8)</done>
</task>

<task type="auto">
  <name>Task 2: Add convenience function</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Add `run_async_inference` convenience function at the end of the file:

```python
def run_async_inference(
    model: torch.nn.Module,
    dataloader: DataLoader,
    device: torch.device,
    enable_streams: bool = True,
    progress_callback: Optional[Callable[[int, int], None]] = None,
) -> List[InferenceResult]:
    """
    Convenience function to run async inference on all batches.

    This is a simple wrapper around AsyncInferenceRunner for common use cases.
    For more control, use AsyncInferenceRunner directly.

    Args:
        model: Model for inference (will be moved to device)
        dataloader: Async DataLoader with prefetched batches
        device: CUDA device for inference
        enable_streams: Use CUDA streams for I/O overlap
        progress_callback: Optional callback(batch_idx, num_sequences)

    Returns:
        List of InferenceResult for all batches

    Example:
        >>> from virnucpro.data import SequenceDataset, VarlenCollator, create_async_dataloader
        >>> from virnucpro.models.esm2_flash import load_esm2_model
        >>>
        >>> model, batch_converter = load_esm2_model(device='cuda:0')
        >>> dataset = SequenceDataset(fasta_files)
        >>> collator = VarlenCollator(batch_converter)
        >>> loader = create_async_dataloader(dataset, collator)
        >>>
        >>> results = run_async_inference(model, loader, torch.device('cuda:0'))
        >>> for result in results:
        ...     print(f'Batch {result.batch_idx}: {len(result.sequence_ids)} sequences')
    """
    runner = AsyncInferenceRunner(
        model=model,
        device=device,
        enable_streams=enable_streams,
    )

    results = list(runner.run(dataloader, progress_callback))

    # Log final statistics
    stats = runner.get_statistics()
    logger.info(f"Inference complete: {stats}")

    return results
```
  </action>
  <verify>
```bash
python -c "
from virnucpro.pipeline.async_inference import run_async_inference
import inspect

sig = inspect.signature(run_async_inference)
params = list(sig.parameters.keys())
assert 'model' in params
assert 'dataloader' in params
assert 'device' in params
print('run_async_inference convenience function verified')
"
```
  </verify>
  <done>run_async_inference convenience function added for simple use cases</done>
</task>

<task type="auto">
  <name>Task 3: Update pipeline module exports</name>
  <files>virnucpro/pipeline/__init__.py</files>
  <action>
Update `virnucpro/pipeline/__init__.py` to export the new async inference components:

1. Add imports:
   ```python
   from virnucpro.pipeline.async_inference import (
       AsyncInferenceRunner,
       InferenceResult,
       run_async_inference,
   )
   ```

2. Add to `__all__` if it exists:
   ```python
   'AsyncInferenceRunner',
   'InferenceResult',
   'run_async_inference',
   ```
  </action>
  <verify>
```bash
python -c "
from virnucpro.pipeline import AsyncInferenceRunner, run_async_inference, InferenceResult
print('Async inference components exportable from virnucpro.pipeline')
"
```
  </verify>
  <done>AsyncInferenceRunner, InferenceResult, run_async_inference importable from virnucpro.pipeline</done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `AsyncInferenceRunner` class exists with stream-based batch processing
2. `InferenceResult` dataclass captures sequence_ids, embeddings, batch_idx
3. `run_async_inference` convenience function works end-to-end
4. All components importable from `virnucpro.pipeline`
5. GPU monitoring integrated with DataLoader metrics
6. **FIX 1**: Timing measures inter-batch arrival (not processing time)
7. **FIX 2**: Single GPU transfer (no double transfer of cu_seqlens)
8. **FIX 3**: NotImplementedError for packed batches (Phase 6 TODO documented)
9. **FIX 4**: StreamProcessor.process_batch_async signature verified
10. **FIX 5**: Pinned memory validation on first batch
11. **FIX 6**: DataLoader exception handling (RuntimeError on worker failure)
12. **FIX 7**: FP16→FP32 conversion documented
13. **FIX 8**: sequence_ids required (ValueError if missing)
</verification>

<success_criteria>
- **FIX 1**: run() measures inter-batch arrival time (fetch_start → fetch_end)
- **FIX 2**: Single GPU transfer via gpu_batch_ref (no re-transfer in _extract_embeddings)
- **FIX 3**: Packed batches raise NotImplementedError with Phase 6 message
- **FIX 4**: StreamProcessor interface documented in process_batch docstring
- **FIX 5**: _validate_pinned_memory checks tensors on first batch
- **FIX 6**: DataLoader wrapped in try/except, raises RuntimeError with context
- **FIX 7**: _extract_embeddings documents FP16 input → FP32 output via .float()
- **FIX 8**: process_batch raises ValueError if batch missing 'sequence_ids'
- AsyncInferenceRunner uses StreamProcessor for H2D/compute overlap
- Batches transferred with non_blocking=True for pinned memory
- GPU monitor tracks DataLoader wait times with batch composition metrics
- run() yields InferenceResult per batch
- Statistics include tokens/sec, packing_efficiency, queue_state_distribution
</success_criteria>

<output>
After completion, create `.planning/phases/05-async-dataloader-foundation/05-04-SUMMARY.md`
</output>
