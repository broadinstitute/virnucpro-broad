---
phase: 05-async-dataloader-foundation
plan: 04
type: execute
wave: 3
depends_on:
  - "05-01"
  - "05-02"
  - "05-03"
files_modified:
  - virnucpro/pipeline/async_inference.py
  - virnucpro/pipeline/__init__.py
autonomous: true

must_haves:
  truths:
    - "Single GPU process handles all inference for assigned data"
    - "Batches prefetched via async DataLoader while GPU computes"
    - "GPU receives pinned tensors via non_blocking transfer"
    - "CUDA streams overlap data transfer with compute"
    - "GPU utilization monitored with bottleneck detection"
  artifacts:
    - path: "virnucpro/pipeline/async_inference.py"
      provides: "Async inference loop for single-GPU processing"
      exports: ["AsyncInferenceRunner", "run_async_inference"]
      min_lines: 150
  key_links:
    - from: "virnucpro/pipeline/async_inference.py"
      to: "virnucpro/data/dataloader_utils.py"
      via: "create_async_dataloader"
      pattern: "create_async_dataloader"
    - from: "virnucpro/pipeline/async_inference.py"
      to: "virnucpro/cuda/stream_manager.py"
      via: "StreamProcessor for async GPU ops"
      pattern: "StreamProcessor"
    - from: "virnucpro/pipeline/async_inference.py"
      to: "virnucpro/utils/gpu_monitor.py"
      via: "NvitopMonitor for performance tracking"
      pattern: "NvitopMonitor"
---

<objective>
Create async inference runner that processes batches on single GPU with stream-based I/O overlap.

Purpose: Implement the core inference loop where GPU receives prefetched batches via pinned memory transfer while CUDA streams overlap data movement with computation. This is the single-GPU foundation that Phase 7 will scale to multi-GPU.

Output:
- `virnucpro/pipeline/async_inference.py`: AsyncInferenceRunner class and run_async_inference function
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-async-dataloader-foundation/05-CONTEXT.md
@.planning/phases/05-async-dataloader-foundation/05-RESEARCH.md

# Prior plan outputs
@.planning/phases/05-async-dataloader-foundation/05-01-SUMMARY.md
@.planning/phases/05-async-dataloader-foundation/05-02-SUMMARY.md
@.planning/phases/05-async-dataloader-foundation/05-03-SUMMARY.md

# Existing code to integrate
@virnucpro/cuda/stream_manager.py
@virnucpro/models/esm2_flash.py
@virnucpro/utils/gpu_monitor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AsyncInferenceRunner class</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Create new file `virnucpro/pipeline/async_inference.py` with the `AsyncInferenceRunner` class:

```python
"""Async inference runner for single-GPU processing with stream overlap.

This module implements the async DataLoader architecture where:
- DataLoader workers handle CPU-only I/O (FASTA parsing)
- Main process tokenizes in collate_fn
- GPU receives prefetched, pinned batches
- CUDA streams overlap data transfer with computation

This is the single-GPU foundation. Multi-GPU coordination is Phase 7.
"""

import torch
import time
import logging
from typing import Optional, List, Dict, Any, Callable, Iterator
from pathlib import Path
from dataclasses import dataclass

from torch.utils.data import DataLoader

from virnucpro.cuda.stream_manager import StreamProcessor
from virnucpro.utils.gpu_monitor import NvitopMonitor

logger = logging.getLogger('virnucpro.pipeline.async_inference')


@dataclass
class InferenceResult:
    """Result from async inference."""
    sequence_ids: List[str]
    embeddings: torch.Tensor  # Shape: (num_sequences, embedding_dim)
    batch_idx: int


class AsyncInferenceRunner:
    """
    Single-GPU async inference runner with stream-based I/O overlap.

    Architecture:
    1. DataLoader workers parse FASTA files (CPU-only)
    2. Collate_fn tokenizes in main process
    3. Pinned tensors transferred to GPU (non_blocking)
    4. CUDA streams overlap H2D, compute, D2H
    5. GPU monitor tracks utilization and bottlenecks

    Attributes:
        device: CUDA device for inference
        model: Model with forward() method (e.g., ESM2WithFlashAttention)
        stream_processor: CUDA stream orchestrator for async ops
        monitor: GPU utilization and DataLoader metrics tracker
    """

    def __init__(
        self,
        model: torch.nn.Module,
        device: torch.device,
        enable_streams: bool = True,
        monitor_interval: float = 1.0,
        log_file: Optional[Path] = None,
    ):
        """
        Initialize async inference runner.

        Args:
            model: Model for inference (already on device)
            device: CUDA device (e.g., torch.device('cuda:0'))
            enable_streams: Use CUDA streams for I/O overlap (default: True)
            monitor_interval: GPU sampling interval in seconds (default: 1.0)
            log_file: Path for GPU metrics log (default: auto-generated)
        """
        self.model = model
        self.device = device
        self.enable_streams = enable_streams

        # Initialize stream processor for async GPU ops
        self.stream_processor = StreamProcessor(
            device=device,
            enable_streams=enable_streams,
            verbose=False
        )

        # Initialize GPU monitor with DataLoader tracking
        device_id = device.index if device.index is not None else 0
        self.monitor = NvitopMonitor(
            device_ids=[device_id],
            log_interval=monitor_interval,
            log_file=log_file
        )

        # Inference state
        self._batch_count = 0
        self._total_sequences = 0

        logger.info(
            f"AsyncInferenceRunner initialized on {device} "
            f"(streams={'enabled' if enable_streams else 'disabled'})"
        )

    def _transfer_to_gpu(self, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """Transfer batch tensors to GPU with non_blocking for pinned memory."""
        gpu_batch = {}
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                gpu_batch[key] = value.to(self.device, non_blocking=True)
            else:
                gpu_batch[key] = value  # Keep non-tensor data as-is
        return gpu_batch

    def _run_inference(self, gpu_batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Run model inference on GPU batch."""
        with torch.no_grad():
            # ESM-2 forward expects tokens tensor
            input_ids = gpu_batch['input_ids']

            # For packed format, we have cu_seqlens
            # Note: FlashAttention varlen integration is Phase 6
            # For now, reshape to (1, seq_len) for standard attention
            if input_ids.dim() == 1:
                input_ids = input_ids.unsqueeze(0)

            # Run model forward
            outputs = self.model(input_ids, repr_layers=[36])
            representations = outputs['representations'][36]

            return representations

    def _extract_embeddings(
        self,
        representations: torch.Tensor,
        batch: Dict[str, Any]
    ) -> torch.Tensor:
        """Extract per-sequence embeddings from model output."""
        cu_seqlens = batch.get('cu_seqlens')
        sequence_ids = batch.get('sequence_ids', [])

        if cu_seqlens is not None and len(sequence_ids) > 0:
            # Packed format: extract embeddings using cu_seqlens boundaries
            # Mean-pool each sequence's tokens
            embeddings = []
            for i in range(len(sequence_ids)):
                start = cu_seqlens[i].item()
                end = cu_seqlens[i + 1].item()
                # Skip BOS token (position 0), mean pool the rest
                seq_repr = representations[0, start+1:end].mean(dim=0)
                embeddings.append(seq_repr)

            return torch.stack(embeddings)
        else:
            # Single sequence or non-packed: mean pool (skip BOS)
            return representations[0, 1:].mean(dim=0, keepdim=True)

    def process_batch(self, batch: Dict[str, Any]) -> InferenceResult:
        """
        Process single batch through async pipeline.

        Args:
            batch: Batch from DataLoader (collated by VarlenCollator)

        Returns:
            InferenceResult with sequence IDs and embeddings
        """
        sequence_ids = batch.get('sequence_ids', [f'seq_{self._batch_count}_{i}'
                                                   for i in range(batch.get('num_sequences', 1))])

        # Use stream processor for async operations
        def transfer_fn(b):
            return self._transfer_to_gpu(b)

        def compute_fn(gpu_b):
            return self._run_inference(gpu_b)

        representations = self.stream_processor.process_batch_async(
            batch,
            transfer_fn=transfer_fn,
            compute_fn=compute_fn,
            retrieve_fn=None  # Keep on GPU for embedding extraction
        )

        # Extract embeddings
        gpu_batch = self._transfer_to_gpu(batch)  # Need cu_seqlens on GPU
        embeddings = self._extract_embeddings(representations, gpu_batch)

        # Move to CPU for storage
        embeddings_cpu = embeddings.float().cpu()

        self._batch_count += 1
        self._total_sequences += len(sequence_ids)

        return InferenceResult(
            sequence_ids=sequence_ids,
            embeddings=embeddings_cpu,
            batch_idx=self._batch_count - 1
        )

    def run(
        self,
        dataloader: DataLoader,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> Iterator[InferenceResult]:
        """
        Run inference on all batches from DataLoader.

        Args:
            dataloader: Async DataLoader with prefetched batches
            progress_callback: Optional callback(batch_idx, num_sequences)

        Yields:
            InferenceResult for each batch
        """
        self.model.eval()
        self.monitor.start_monitoring()
        self.monitor.start_inference_timer()
        self.monitor.set_stage('inference')

        logger.info("Starting async inference loop")

        try:
            for batch_idx, batch in enumerate(dataloader):
                # Time DataLoader fetch
                t0 = time.perf_counter()
                # Note: batch already fetched by enumerate, so this measures
                # time since last iteration (approximates wait time)

                # Process batch
                result = self.process_batch(batch)

                # Record metrics
                fetch_time_ms = (time.perf_counter() - t0) * 1000
                self.monitor.record_dataloader_wait(
                    wait_time_ms=fetch_time_ms,
                    batch_idx=batch_idx,
                    sequences_in_batch=result.embeddings.shape[0],
                    tokens_in_batch=batch.get('input_ids', torch.tensor([])).numel(),
                    queue_depth=0  # Not directly accessible
                )

                # Check for bottleneck every 10 batches
                if batch_idx % 10 == 0 and batch_idx > 0:
                    is_bottleneck, avg_util = self.monitor.check_bottleneck()
                    self.monitor.sample()  # Take GPU utilization sample

                # Progress callback
                if progress_callback:
                    progress_callback(batch_idx, result.embeddings.shape[0])

                yield result

        finally:
            # Synchronize streams before stopping
            self.stream_processor.synchronize()
            stats = self.monitor.stop_monitoring()

            throughput = self.monitor.get_throughput()
            dl_stats = self.monitor.get_dataloader_statistics()

            logger.info(
                f"Async inference complete: {self._total_sequences} sequences, "
                f"{throughput.get('sequences_per_sec', 0):.1f} seq/s"
            )
            if dl_stats:
                logger.info(
                    f"DataLoader stats: avg_wait={dl_stats.get('avg_wait_time_ms', 0):.1f}ms, "
                    f"max_wait={dl_stats.get('max_wait_time_ms', 0):.1f}ms"
                )

    def get_statistics(self) -> Dict[str, Any]:
        """Get inference statistics."""
        return {
            'total_batches': self._batch_count,
            'total_sequences': self._total_sequences,
            'gpu_stats': self.monitor.get_statistics(),
            'dataloader_stats': self.monitor.get_dataloader_statistics(),
            'throughput': self.monitor.get_throughput(),
        }
```
  </action>
  <verify>
```bash
python -c "
from virnucpro.pipeline.async_inference import AsyncInferenceRunner, InferenceResult
import torch

# Check class exists with expected methods
assert hasattr(AsyncInferenceRunner, '__init__')
assert hasattr(AsyncInferenceRunner, 'process_batch')
assert hasattr(AsyncInferenceRunner, 'run')
assert hasattr(AsyncInferenceRunner, 'get_statistics')

# Check InferenceResult dataclass
result = InferenceResult(
    sequence_ids=['seq1', 'seq2'],
    embeddings=torch.randn(2, 2560),
    batch_idx=0
)
assert len(result.sequence_ids) == 2
print('AsyncInferenceRunner class structure verified')
"
```
  </verify>
  <done>AsyncInferenceRunner class created with process_batch, run, and get_statistics methods</done>
</task>

<task type="auto">
  <name>Task 2: Add convenience function</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Add `run_async_inference` convenience function at the end of the file:

```python
def run_async_inference(
    model: torch.nn.Module,
    dataloader: DataLoader,
    device: torch.device,
    enable_streams: bool = True,
    progress_callback: Optional[Callable[[int, int], None]] = None,
) -> List[InferenceResult]:
    """
    Convenience function to run async inference on all batches.

    This is a simple wrapper around AsyncInferenceRunner for common use cases.
    For more control, use AsyncInferenceRunner directly.

    Args:
        model: Model for inference (will be moved to device)
        dataloader: Async DataLoader with prefetched batches
        device: CUDA device for inference
        enable_streams: Use CUDA streams for I/O overlap
        progress_callback: Optional callback(batch_idx, num_sequences)

    Returns:
        List of InferenceResult for all batches

    Example:
        >>> from virnucpro.data import SequenceDataset, VarlenCollator, create_async_dataloader
        >>> from virnucpro.models.esm2_flash import load_esm2_model
        >>>
        >>> model, batch_converter = load_esm2_model(device='cuda:0')
        >>> dataset = SequenceDataset(fasta_files)
        >>> collator = VarlenCollator(batch_converter)
        >>> loader = create_async_dataloader(dataset, collator)
        >>>
        >>> results = run_async_inference(model, loader, torch.device('cuda:0'))
        >>> for result in results:
        ...     print(f'Batch {result.batch_idx}: {len(result.sequence_ids)} sequences')
    """
    runner = AsyncInferenceRunner(
        model=model,
        device=device,
        enable_streams=enable_streams,
    )

    results = list(runner.run(dataloader, progress_callback))

    # Log final statistics
    stats = runner.get_statistics()
    logger.info(f"Inference complete: {stats}")

    return results
```
  </action>
  <verify>
```bash
python -c "
from virnucpro.pipeline.async_inference import run_async_inference
import inspect

sig = inspect.signature(run_async_inference)
params = list(sig.parameters.keys())
assert 'model' in params
assert 'dataloader' in params
assert 'device' in params
print('run_async_inference convenience function verified')
"
```
  </verify>
  <done>run_async_inference convenience function added for simple use cases</done>
</task>

<task type="auto">
  <name>Task 3: Update pipeline module exports</name>
  <files>virnucpro/pipeline/__init__.py</files>
  <action>
Update `virnucpro/pipeline/__init__.py` to export the new async inference components:

1. Add imports:
   ```python
   from virnucpro.pipeline.async_inference import (
       AsyncInferenceRunner,
       InferenceResult,
       run_async_inference,
   )
   ```

2. Add to `__all__` if it exists:
   ```python
   'AsyncInferenceRunner',
   'InferenceResult',
   'run_async_inference',
   ```
  </action>
  <verify>
```bash
python -c "
from virnucpro.pipeline import AsyncInferenceRunner, run_async_inference, InferenceResult
print('Async inference components exportable from virnucpro.pipeline')
"
```
  </verify>
  <done>AsyncInferenceRunner, InferenceResult, run_async_inference importable from virnucpro.pipeline</done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `AsyncInferenceRunner` class exists with stream-based batch processing
2. `InferenceResult` dataclass captures sequence_ids, embeddings, batch_idx
3. `run_async_inference` convenience function works end-to-end
4. All components importable from `virnucpro.pipeline`
5. GPU monitoring integrated with DataLoader metrics
</verification>

<success_criteria>
- AsyncInferenceRunner uses StreamProcessor for H2D/compute overlap
- Batches transferred with non_blocking=True for pinned memory
- GPU monitor tracks DataLoader wait times and bottlenecks
- run() yields InferenceResult per batch
- Statistics include GPU utilization, DataLoader metrics, throughput
</success_criteria>

<output>
After completion, create `.planning/phases/05-async-dataloader-foundation/05-04-SUMMARY.md`
</output>
