---
phase: 05-async-dataloader-foundation
plan: 03
type: execute
wave: 2
depends_on:
  - "05-01"
files_modified:
  - virnucpro/data/dataloader_utils.py
autonomous: true

must_haves:
  truths:
    - "DataLoader uses spawn multiprocessing context (not fork)"
    - "Workers initialized with CUDA_VISIBLE_DEVICES='' via worker_init_fn"
    - "pin_memory=True for fast CPU-to-GPU transfer"
    - "prefetch_factor=4 for aggressive batch prefetching"
    - "persistent_workers=True keeps workers alive across batches"
  artifacts:
    - path: "virnucpro/data/dataloader_utils.py"
      provides: "Async DataLoader factory with CUDA safety"
      exports: ["create_async_dataloader", "cuda_safe_worker_init"]
      contains: "multiprocessing_context='spawn'"
  key_links:
    - from: "virnucpro/data/dataloader_utils.py"
      to: "virnucpro/data/sequence_dataset.py"
      via: "SequenceDataset usage"
      pattern: "SequenceDataset"
    - from: "virnucpro/data/dataloader_utils.py"
      to: "virnucpro/data/collators.py"
      via: "VarlenCollator as collate_fn"
      pattern: "collate_fn"
---

<objective>
Create async DataLoader factory with CUDA-safe worker configuration.

Purpose: Provide a factory function that creates DataLoaders with all the safety and performance settings required for async GPU inference: spawn context, CUDA isolation in workers, aggressive prefetching, and memory pinning.

Output:
- Extended `virnucpro/data/dataloader_utils.py` with `create_async_dataloader()` function
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-async-dataloader-foundation/05-CONTEXT.md
@.planning/phases/05-async-dataloader-foundation/05-RESEARCH.md

# Prior plan output
@.planning/phases/05-async-dataloader-foundation/05-01-SUMMARY.md

# Existing code to extend
@virnucpro/data/dataloader_utils.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add CUDA-safe worker_init_fn</name>
  <files>virnucpro/data/dataloader_utils.py</files>
  <action>
Add a `cuda_safe_worker_init` function to ensure workers have no CUDA access:

```python
def cuda_safe_worker_init(worker_id: int) -> None:
    """
    Initialize DataLoader worker with CUDA isolation.

    This function is called in each worker process to ensure:
    1. CUDA_VISIBLE_DEVICES is empty (no GPU access)
    2. HuggingFace tokenizer parallelism disabled (prevents deadlocks)
    3. Worker is seeded for reproducibility

    Args:
        worker_id: Worker process ID (0 to num_workers-1)

    Raises:
        RuntimeError: If CUDA is accessible in worker (should never happen with spawn)
    """
    import os
    import numpy as np

    # Hide CUDA devices from worker
    os.environ['CUDA_VISIBLE_DEVICES'] = ''

    # Disable HuggingFace tokenizer parallelism to prevent deadlocks with DataLoader
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    # Seed for reproducibility
    np.random.seed(worker_id)

    # Validate CUDA is not accessible
    # Note: We import torch here to check, but the env var should prevent CUDA init
    import torch
    if torch.cuda.is_available():
        raise RuntimeError(
            f"Worker {worker_id}: CUDA is accessible despite CUDA_VISIBLE_DEVICES=''. "
            "This indicates a multiprocessing context issue. Use 'spawn' context."
        )

    logger.debug(f"Worker {worker_id} initialized with CUDA isolation")
```

Add import for numpy at top of file if not present.
  </action>
  <verify>
```bash
python -c "
from virnucpro.data.dataloader_utils import cuda_safe_worker_init
print('cuda_safe_worker_init function exists')
# Note: Can't fully test in main process as it modifies env vars
"
```
  </verify>
  <done>cuda_safe_worker_init function defined with CUDA_VISIBLE_DEVICES='', TOKENIZERS_PARALLELISM='false', and CUDA validation</done>
</task>

<task type="auto">
  <name>Task 2: Create async DataLoader factory</name>
  <files>virnucpro/data/dataloader_utils.py</files>
  <action>
Add `create_async_dataloader` function for CUDA-safe async loading:

```python
def create_async_dataloader(
    dataset: 'IterableDataset',
    collate_fn: Callable,
    batch_size: int = 32,
    num_workers: int = 4,
    prefetch_factor: int = 4,
    pin_memory: bool = True,
    timeout: float = 300.0,
) -> DataLoader:
    """
    Create DataLoader with CUDA-safe configuration for async GPU inference.

    This factory configures DataLoader for the async architecture where:
    - Workers perform CPU-only I/O (FASTA parsing)
    - Main process handles tokenization (via collate_fn)
    - GPU inference receives prefetched, pinned batches

    Configuration follows PyTorch best practices for GPU workloads:
    - spawn context: Prevents CUDA context inheritance (safe multiprocessing)
    - persistent_workers: Keeps workers alive (faster, no restart overhead)
    - pin_memory: Enables fast CPU-to-GPU transfer
    - prefetch_factor: Aggressive prefetching (4 batches per worker = 16 total)

    Args:
        dataset: IterableDataset (e.g., SequenceDataset) for streaming data
        collate_fn: Callable to process batches (e.g., VarlenCollator)
        batch_size: Number of items per batch (sequences, not tokens)
        num_workers: Number of CPU workers for I/O (default: 4)
        prefetch_factor: Batches to prefetch per worker (default: 4)
        pin_memory: Enable pinned memory for fast GPU transfer (default: True)
        timeout: Timeout in seconds for worker operations (default: 300)

    Returns:
        Configured DataLoader ready for async GPU inference

    Example:
        >>> from virnucpro.data import SequenceDataset, VarlenCollator
        >>> dataset = SequenceDataset(fasta_files)
        >>> collator = VarlenCollator(batch_converter)
        >>> loader = create_async_dataloader(dataset, collator)
        >>> for batch in loader:
        ...     # batch has pinned tensors ready for GPU transfer
        ...     gpu_batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}

    Raises:
        ValueError: If num_workers < 1 (async requires workers)

    Note:
        - Workers MUST NOT touch CUDA (enforced via cuda_safe_worker_init)
        - Tokenization happens in main process via collate_fn
        - For single-threaded loading, use create_optimized_dataloader instead
    """
    if num_workers < 1:
        raise ValueError(
            f"Async DataLoader requires num_workers >= 1, got {num_workers}. "
            "Use create_optimized_dataloader for single-threaded loading."
        )

    logger.info(
        f"Creating async DataLoader: batch_size={batch_size}, num_workers={num_workers}, "
        f"prefetch_factor={prefetch_factor}, pin_memory={pin_memory}"
    )

    return DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=pin_memory,
        persistent_workers=True,
        multiprocessing_context='spawn',
        collate_fn=collate_fn,
        worker_init_fn=cuda_safe_worker_init,
        timeout=timeout,
    )
```

Add import for `Callable` from typing if not present.
Add import for `IterableDataset` from torch.utils.data if not present.
  </action>
  <verify>
```bash
python -c "
from virnucpro.data.dataloader_utils import create_async_dataloader
from torch.utils.data import IterableDataset
import inspect

# Check signature
sig = inspect.signature(create_async_dataloader)
params = list(sig.parameters.keys())
assert 'dataset' in params, 'Missing dataset param'
assert 'collate_fn' in params, 'Missing collate_fn param'
assert 'num_workers' in params, 'Missing num_workers param'
assert 'prefetch_factor' in params, 'Missing prefetch_factor param'
assert 'pin_memory' in params, 'Missing pin_memory param'

print('create_async_dataloader function has correct signature')

# Check ValueError for num_workers=0
try:
    class DummyDataset(IterableDataset):
        def __iter__(self):
            yield 1
    create_async_dataloader(DummyDataset(), lambda x: x, num_workers=0)
    assert False, 'Should have raised ValueError'
except ValueError as e:
    assert 'num_workers >= 1' in str(e)
    print('ValueError raised correctly for num_workers=0')
"
```
  </verify>
  <done>create_async_dataloader factory creates DataLoader with spawn context, CUDA-safe worker_init_fn, prefetch_factor=4, pin_memory=True, persistent_workers=True</done>
</task>

<task type="auto">
  <name>Task 3: Update module exports</name>
  <files>virnucpro/data/dataloader_utils.py</files>
  <action>
Ensure the new functions are exported:

1. Check if `__all__` exists in the file
2. If yes, add `'create_async_dataloader'` and `'cuda_safe_worker_init'` to the list
3. If no `__all__`, the functions are already accessible via normal import

Also update the module docstring to mention async DataLoader support:
- Add a bullet point: "- Async DataLoader for GPU inference with CUDA-safe workers"
  </action>
  <verify>
```bash
python -c "
from virnucpro.data.dataloader_utils import (
    create_async_dataloader,
    cuda_safe_worker_init,
    create_optimized_dataloader,  # Existing
    get_optimal_workers,  # Existing
)
print('All DataLoader utilities importable')
"
```
  </verify>
  <done>create_async_dataloader and cuda_safe_worker_init importable from dataloader_utils</done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `cuda_safe_worker_init` sets CUDA_VISIBLE_DEVICES='' and validates no CUDA
2. `create_async_dataloader` returns DataLoader with spawn context
3. DataLoader config includes: pin_memory=True, prefetch_factor=4, persistent_workers=True
4. ValueError raised if num_workers < 1
5. All functions importable from module
</verification>

<success_criteria>
- create_async_dataloader creates DataLoader with all CUDA safety settings
- Spawn multiprocessing context prevents CUDA inheritance
- Workers initialized with CUDA_VISIBLE_DEVICES='' via worker_init_fn
- Aggressive prefetching (prefetch_factor=4) for deep queue
- Memory pinning enabled for fast GPU transfer
</success_criteria>

<output>
After completion, create `.planning/phases/05-async-dataloader-foundation/05-03-SUMMARY.md`
</output>
