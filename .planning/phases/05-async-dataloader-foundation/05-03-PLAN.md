---
phase: 05-async-dataloader-foundation
plan: 03
type: execute
wave: 2
depends_on:
  - "05-01"
files_modified:
  - virnucpro/data/dataloader_utils.py
autonomous: true

must_haves:
  truths:
    - "DataLoader uses spawn multiprocessing context (not fork)"
    - "Workers initialized with CUDA_VISIBLE_DEVICES='' via worker_init_fn"
    - "CUDA isolation verified in actual spawned worker (not just function test)"
    - "batch_size=None by default (allows VarlenCollator dynamic token-budget packing)"
    - "timeout=600s (10 minutes) for large FASTA parsing"
    - "pin_memory=True for fast CPU-to-GPU transfer"
    - "prefetch_factor=4 for aggressive batch prefetching"
    - "persistent_workers=True keeps workers alive across batches"
  artifacts:
    - path: "virnucpro/data/dataloader_utils.py"
      provides: "Async DataLoader factory with CUDA safety"
      exports: ["create_async_dataloader", "cuda_safe_worker_init"]
      contains: "multiprocessing_context='spawn'"
  key_links:
    - from: "virnucpro/data/dataloader_utils.py"
      to: "virnucpro/data/sequence_dataset.py"
      via: "SequenceDataset usage"
      pattern: "SequenceDataset"
    - from: "virnucpro/data/dataloader_utils.py"
      to: "virnucpro/data/collators.py"
      via: "VarlenCollator as collate_fn"
      pattern: "collate_fn"
---

<objective>
Create async DataLoader factory with CUDA-safe worker configuration.

Purpose: Provide a factory function that creates DataLoaders with all the safety and performance settings required for async GPU inference: spawn context, CUDA isolation in workers, aggressive prefetching, and memory pinning.

Output:
- Extended `virnucpro/data/dataloader_utils.py` with `create_async_dataloader()` function
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-async-dataloader-foundation/05-CONTEXT.md
@.planning/phases/05-async-dataloader-foundation/05-RESEARCH.md

# Prior plan output
@.planning/phases/05-async-dataloader-foundation/05-01-SUMMARY.md

# Existing code to extend
@virnucpro/data/dataloader_utils.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add CUDA-safe worker_init_fn</name>
  <files>virnucpro/data/dataloader_utils.py</files>
  <action>
Add a `cuda_safe_worker_init` function to ensure workers have no CUDA access:

```python
def cuda_safe_worker_init(worker_id: int) -> None:
    """
    Initialize DataLoader worker with CUDA isolation.

    This function is called in each worker process to ensure:
    1. CUDA_VISIBLE_DEVICES is empty (no GPU access)
    2. HuggingFace tokenizer parallelism disabled (prevents deadlocks)
    3. Worker is seeded for reproducibility

    Args:
        worker_id: Worker process ID (0 to num_workers-1)

    Raises:
        RuntimeError: If CUDA is accessible in worker (should never happen with spawn)
    """
    import os
    import numpy as np

    # Hide CUDA devices from worker
    os.environ['CUDA_VISIBLE_DEVICES'] = ''

    # Disable HuggingFace tokenizer parallelism to prevent deadlocks with DataLoader
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'

    # Seed for reproducibility
    np.random.seed(worker_id)

    # Validate CUDA is not accessible
    # Note: We import torch here to check, but the env var should prevent CUDA init
    import torch
    if torch.cuda.is_available():
        raise RuntimeError(
            f"Worker {worker_id}: CUDA is accessible despite CUDA_VISIBLE_DEVICES=''. "
            "This indicates a multiprocessing context issue. Use 'spawn' context."
        )

    logger.debug(f"Worker {worker_id} initialized with CUDA isolation")
```

Add import for numpy at top of file if not present.
  </action>
  <verify>
```bash
python -c "
from virnucpro.data.dataloader_utils import cuda_safe_worker_init
print('cuda_safe_worker_init function exists')
# Note: Can't fully test in main process as it modifies env vars
"
```
  </verify>
  <done>cuda_safe_worker_init function defined with CUDA_VISIBLE_DEVICES='', TOKENIZERS_PARALLELISM='false', and CUDA validation</done>
</task>

<task type="auto">
  <name>Task 2: Create async DataLoader factory</name>
  <files>virnucpro/data/dataloader_utils.py</files>
  <action>
Add `create_async_dataloader` function for CUDA-safe async loading:

**CRITICAL: Add torch import at module level** (before function definitions):
```python
import torch
from torch.utils.data import DataLoader, IterableDataset
from typing import Callable, Optional
```

Then add the factory function:

```python
def create_async_dataloader(
    dataset: IterableDataset,
    collate_fn: Callable,
    batch_size: Optional[int] = None,
    num_workers: int = 4,
    prefetch_factor: int = 4,
    pin_memory: bool = True,
    timeout: float = 600.0,
) -> DataLoader:
    """
    Create DataLoader with CUDA-safe configuration for async GPU inference.

    This factory configures DataLoader for the async architecture where:
    - Workers perform CPU-only I/O (FASTA parsing)
    - Main process handles tokenization (via collate_fn)
    - GPU inference receives prefetched, pinned batches

    Configuration follows PyTorch best practices for GPU workloads:
    - spawn context: Prevents CUDA context inheritance (safe multiprocessing)
    - persistent_workers: Keeps workers alive (faster, no restart overhead)
    - pin_memory: Enables fast CPU-to-GPU transfer
    - prefetch_factor: Aggressive prefetching (4 batches per worker = 16 total)

    Args:
        dataset: IterableDataset (e.g., SequenceDataset) for streaming data
        collate_fn: Callable to process batches (e.g., VarlenCollator)
        batch_size: Number of items per batch. Use None for dynamic batching where
            collate_fn determines batch size (e.g., VarlenCollator with token budget).
            For VarlenCollator (token-budget packing), MUST be None - otherwise fixed
            batch_size would override the packing logic and hurt efficiency.
        num_workers: Number of CPU workers for I/O (default: 4)
        prefetch_factor: Batches to prefetch per worker (default: 4)
        pin_memory: Enable pinned memory for fast GPU transfer (default: True)
        timeout: Timeout in seconds for worker operations (default: 600 = 10 minutes,
            increased from 5 minutes to handle large FASTA parsing)

    Returns:
        Configured DataLoader ready for async GPU inference

    Example:
        >>> from virnucpro.data import SequenceDataset, VarlenCollator
        >>> dataset = SequenceDataset(fasta_files)
        >>> collator = VarlenCollator(batch_converter, max_tokens_per_batch=4096)
        >>> # batch_size=None lets VarlenCollator control batching via token budget
        >>> loader = create_async_dataloader(dataset, collator, batch_size=None)
        >>> for batch in loader:
        ...     # batch has pinned tensors ready for GPU transfer
        ...     gpu_batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}

    Raises:
        ValueError: If num_workers < 1 (async requires workers)

    Note:
        - Workers MUST NOT touch CUDA (enforced via cuda_safe_worker_init)
        - Tokenization happens in main process via collate_fn
        - batch_size=None is recommended for VarlenCollator (token-budget packing)
        - For single-threaded loading, use create_optimized_dataloader instead
    """
    if num_workers < 1:
        raise ValueError(
            f"Async DataLoader requires num_workers >= 1, got {num_workers}. "
            "Use create_optimized_dataloader for single-threaded loading."
        )

    logger.info(
        f"Creating async DataLoader: batch_size={batch_size}, num_workers={num_workers}, "
        f"prefetch_factor={prefetch_factor}, pin_memory={pin_memory}, timeout={timeout}s"
    )

    return DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=pin_memory,
        persistent_workers=True,
        multiprocessing_context='spawn',
        collate_fn=collate_fn,
        worker_init_fn=cuda_safe_worker_init,
        timeout=timeout,
    )
```

**Type hints requirement:** torch, DataLoader, IterableDataset, Callable, Optional MUST be imported at module level (not inside functions) for type hints to work properly.
  </action>
  <verify>
```bash
python -c "
from virnucpro.data.dataloader_utils import create_async_dataloader
from torch.utils.data import IterableDataset
import torch
import inspect

# Check signature
sig = inspect.signature(create_async_dataloader)
params = list(sig.parameters.keys())
assert 'dataset' in params, 'Missing dataset param'
assert 'collate_fn' in params, 'Missing collate_fn param'
assert 'num_workers' in params, 'Missing num_workers param'
assert 'prefetch_factor' in params, 'Missing prefetch_factor param'
assert 'pin_memory' in params, 'Missing pin_memory param'
assert 'batch_size' in params, 'Missing batch_size param'
assert 'timeout' in params, 'Missing timeout param'

# Check batch_size default is None
assert sig.parameters['batch_size'].default is None, 'batch_size default should be None for VarlenCollator'

# Check timeout default is 600
assert sig.parameters['timeout'].default == 600.0, 'timeout default should be 600.0 (10 minutes)'

print('create_async_dataloader function has correct signature with batch_size=None, timeout=600')

# Check ValueError for num_workers=0
try:
    class DummyDataset(IterableDataset):
        def __iter__(self):
            yield {'seq': 'A'}
    create_async_dataloader(DummyDataset(), lambda x: x, num_workers=0)
    assert False, 'Should have raised ValueError'
except ValueError as e:
    assert 'num_workers >= 1' in str(e)
    print('ValueError raised correctly for num_workers=0')

# CRITICAL TEST: Verify CUDA isolation in actual spawned worker
print('Testing CUDA isolation in spawned worker...')

class TestDataset(IterableDataset):
    '''Dataset that yields a single item for testing.'''
    def __iter__(self):
        yield {'id': 'test', 'sequence': 'MKTAYIAK', 'file': 'test.fasta'}

def test_collate(batch):
    '''Collate that checks CUDA is NOT available in worker context.'''
    # This runs in main process, CUDA should be available here
    return {'data': [item['sequence'] for item in batch]}

# Create DataLoader with 1 worker to test spawn + worker_init_fn
dataset = TestDataset()
loader = create_async_dataloader(
    dataset,
    collate_fn=test_collate,
    batch_size=1,
    num_workers=1,
    prefetch_factor=2,
    timeout=10.0  # Short timeout for test
)

# Fetch one batch - this will spawn worker and run cuda_safe_worker_init
try:
    batch = next(iter(loader))
    print('✓ Worker spawned successfully with CUDA isolation')
    print('  (No RuntimeError from cuda_safe_worker_init means CUDA was hidden)')
except RuntimeError as e:
    if 'CUDA is accessible' in str(e):
        print('✗ FAILED: Worker has CUDA access despite worker_init_fn')
        raise
    else:
        raise

# Clean shutdown
del loader

print('CUDA isolation test passed: worker cannot access CUDA')
"
```
  </verify>
  <done>create_async_dataloader factory creates DataLoader with batch_size=None (for VarlenCollator), timeout=600s, spawn context, CUDA-safe worker_init_fn, prefetch_factor=4, pin_memory=True, persistent_workers=True. CUDA isolation verified in spawned worker.</done>
</task>

<task type="auto">
  <name>Task 3: Update module exports</name>
  <files>virnucpro/data/dataloader_utils.py</files>
  <action>
Ensure the new functions are exported:

1. Check if `__all__` exists in the file
2. If yes, add `'create_async_dataloader'` and `'cuda_safe_worker_init'` to the list
3. If no `__all__`, the functions are already accessible via normal import

Also update the module docstring to mention async DataLoader support:
- Add a bullet point: "- Async DataLoader for GPU inference with CUDA-safe workers"
  </action>
  <verify>
```bash
python -c "
from virnucpro.data.dataloader_utils import (
    create_async_dataloader,
    cuda_safe_worker_init,
    create_optimized_dataloader,  # Existing
    get_optimal_workers,  # Existing
)
print('All DataLoader utilities importable')
"
```
  </verify>
  <done>create_async_dataloader and cuda_safe_worker_init importable from dataloader_utils</done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `cuda_safe_worker_init` sets CUDA_VISIBLE_DEVICES='' and validates no CUDA
2. `create_async_dataloader` returns DataLoader with spawn context
3. DataLoader config includes: batch_size=None (default), timeout=600.0, pin_memory=True, prefetch_factor=4, persistent_workers=True
4. ValueError raised if num_workers < 1
5. All functions importable from module
6. **CRITICAL**: CUDA isolation verified in actual spawned worker (test spawns worker and confirms no CUDA access)
7. torch imported at module level (for type hints to work)
</verification>

<success_criteria>
- create_async_dataloader creates DataLoader with all CUDA safety settings
- batch_size=None by default (allows VarlenCollator dynamic token-budget packing)
- timeout=600s (10 minutes) for large FASTA parsing
- Spawn multiprocessing context prevents CUDA inheritance
- Workers initialized with CUDA_VISIBLE_DEVICES='' via worker_init_fn
- **CRITICAL**: Actual worker spawn test confirms CUDA isolation (not just unit test of function)
- Aggressive prefetching (prefetch_factor=4) for deep queue
- Memory pinning enabled for fast GPU transfer
- Type hints work correctly (torch imported at module level)
</success_criteria>

<output>
After completion, create `.planning/phases/05-async-dataloader-foundation/05-03-SUMMARY.md`
</output>
