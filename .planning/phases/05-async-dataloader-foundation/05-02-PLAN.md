---
phase: 05-async-dataloader-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/utils/gpu_monitor.py
autonomous: true

must_haves:
  truths:
    - "GPU monitor tracks DataLoader wait time between batches"
    - "Monitor uses tiered thresholds (<50% critical, <80% mild) for bottleneck detection"
    - "Queue state inferred from wait time heuristic (<1ms full, >50ms starved)"
    - "Throughput metrics include both sequences/sec and tokens/sec"
    - "Packing efficiency metric tracks padding waste for packed batches"
  artifacts:
    - path: "virnucpro/utils/gpu_monitor.py"
      provides: "Extended GPU monitor with DataLoader metrics"
      exports: ["NvitopMonitor", "GPUMonitor", "DataLoaderMetrics", "infer_queue_state"]
      contains: "record_dataloader_wait"
  key_links:
    - from: "virnucpro/utils/gpu_monitor.py"
      to: "nvitop.Device"
      via: "GPU utilization sampling"
      pattern: "gpu_utilization"
---

<objective>
Extend GPU monitoring to track DataLoader-specific metrics for async pipeline performance analysis.

Purpose: Add DataLoader wait time tracking, bottleneck detection, and throughput metrics to the existing NvitopMonitor. These metrics are essential for validating that the async DataLoader achieves <5% GPU idle time (per roadmap success criteria).

Output:
- Extended `virnucpro/utils/gpu_monitor.py` with DataLoader metrics tracking
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-async-dataloader-foundation/05-CONTEXT.md
@.planning/phases/05-async-dataloader-foundation/05-RESEARCH.md

# Existing code to extend
@virnucpro/utils/gpu_monitor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add DataLoaderMetrics dataclass</name>
  <files>virnucpro/utils/gpu_monitor.py</files>
  <action>
Add a new `DataLoaderMetrics` dataclass after the existing `GPUMetrics` dataclass:

```python
@dataclass
class DataLoaderMetrics:
    """
    DataLoader performance metrics snapshot.

    NOTE: queue_depth is a heuristic estimate, not actual DataLoader queue state.
    PyTorch DataLoader doesn't expose internal queue depth. We infer from timing:
    - wait_time_ms < 1ms → queue likely full (batch ready)
    - wait_time_ms > 50ms → queue likely empty (starved)
    """
    timestamp: float
    wait_time_ms: float        # Time spent waiting for next batch
    batch_idx: int             # Current batch index
    sequences_in_batch: int    # Sequences in this batch
    tokens_in_batch: int       # Tokens in this batch (for packed batches)

    # Batch composition (for packed batches)
    avg_sequence_length: float = 0.0  # Average tokens per sequence
    max_sequence_length: int = 0      # Longest sequence in batch

    # Heuristic queue state (not actual DataLoader queue)
    queue_state: str = 'unknown'  # 'full' | 'starved' | 'normal' | 'unknown'
```

Add helper method to infer queue state from wait time:

```python
def infer_queue_state(wait_time_ms: float) -> str:
    """Infer DataLoader queue state from wait time heuristic."""
    if wait_time_ms < 1.0:
        return 'full'      # Batch was ready, queue had data
    elif wait_time_ms > 50.0:
        return 'starved'   # Long wait, queue was likely empty
    else:
        return 'normal'    # Typical prefetch timing
```

This dataclass captures per-batch DataLoader performance for bottleneck analysis.
  </action>
  <verify>
```bash
python -c "
from virnucpro.utils.gpu_monitor import DataLoaderMetrics, infer_queue_state
import time

m = DataLoaderMetrics(
    timestamp=time.time(),
    wait_time_ms=5.2,
    batch_idx=0,
    sequences_in_batch=32,
    tokens_in_batch=1024,
    avg_sequence_length=32.0,
    max_sequence_length=64,
    queue_state=infer_queue_state(5.2)
)
print(f'DataLoaderMetrics created: wait={m.wait_time_ms}ms, queue_state={m.queue_state}')
print(f'  Batch composition: {m.sequences_in_batch} seqs, avg_len={m.avg_sequence_length:.1f}')

# Test heuristic
assert infer_queue_state(0.5) == 'full', 'Fast fetch should be full queue'
assert infer_queue_state(100.0) == 'starved', 'Slow fetch should be starved queue'
assert infer_queue_state(10.0) == 'normal', 'Moderate fetch should be normal queue'
print('Queue state heuristic test passed')
"
```
  </verify>
  <done>DataLoaderMetrics dataclass defined with wait_time_ms, batch composition, and heuristic queue_state inference</done>
</task>

<task type="auto">
  <name>Task 2: Extend NvitopMonitor with DataLoader tracking</name>
  <files>virnucpro/utils/gpu_monitor.py</files>
  <action>
Add DataLoader metrics tracking to the existing `NvitopMonitor` class:

1. **Add instance variables in `__init__`:**
   ```python
   self._dataloader_metrics: List[DataLoaderMetrics] = []
   self._batch_log_interval: int = 10  # Log every N batches
   self._total_sequences: int = 0
   self._total_tokens: int = 0
   self._inference_start_time: Optional[float] = None
   ```

2. **Add `record_dataloader_wait` method:**
   ```python
   def record_dataloader_wait(
       self,
       wait_time_ms: float,
       batch_idx: int,
       sequences_in_batch: int,
       tokens_in_batch: int = 0,
       avg_sequence_length: float = 0.0,
       max_sequence_length: int = 0
   ) -> None:
       """
       Record DataLoader fetch timing for bottleneck detection.

       Args:
           wait_time_ms: Time spent waiting for DataLoader to return next batch
           batch_idx: Current batch index
           sequences_in_batch: Number of sequences in batch
           tokens_in_batch: Total tokens in batch (for packed batches)
           avg_sequence_length: Average tokens per sequence
           max_sequence_length: Longest sequence in batch
       """
       metrics = DataLoaderMetrics(
           timestamp=time.time(),
           wait_time_ms=wait_time_ms,
           batch_idx=batch_idx,
           sequences_in_batch=sequences_in_batch,
           tokens_in_batch=tokens_in_batch,
           avg_sequence_length=avg_sequence_length,
           max_sequence_length=max_sequence_length,
           queue_state=infer_queue_state(wait_time_ms)
       )
       with self._lock:
           self._dataloader_metrics.append(metrics)
           self._total_sequences += sequences_in_batch
           self._total_tokens += tokens_in_batch

       # Log every N batches per CONTEXT.md decision
       if batch_idx % self._batch_log_interval == 0:
           logger.info(
               f"Batch {batch_idx}: wait={wait_time_ms:.1f}ms, "
               f"seqs={sequences_in_batch}, tokens={tokens_in_batch}, "
               f"queue_state={metrics.queue_state}"
           )
   ```

3. **Add `check_bottleneck` method with tiered thresholds:**
   ```python
   def check_bottleneck(self, recent_samples: int = 10) -> Tuple[bool, str, float]:
       """
       Check if GPU is idle too often indicating I/O bottleneck.

       Tiered thresholds to avoid false positives with short sequences:
       - <50%: Critical bottleneck (definitely I/O bound)
       - <80%: Mild bottleneck (may be batch size or I/O issue)
       - ≥80%: No bottleneck

       Returns:
           Tuple of (is_bottleneck, severity, avg_utilization)
           severity: 'critical' | 'mild' | 'none'
       """
       with self._lock:
           if len(self._metrics) < recent_samples:
               return False, 'none', 0.0

           recent = self._metrics[-recent_samples:]
           avg_util = sum(m.gpu_util for m in recent) / len(recent)

       # Tiered bottleneck detection
       if avg_util < 50:
           logger.warning(
               f"CRITICAL I/O bottleneck: GPU utilization {avg_util:.1f}% "
               f"(threshold: 50%). DataLoader is starving the GPU."
           )
           return True, 'critical', avg_util
       elif avg_util < 80:
           logger.info(
               f"Mild I/O bottleneck: GPU utilization {avg_util:.1f}% "
               f"(threshold: 80%). Consider increasing batch size or prefetch_factor."
           )
           return True, 'mild', avg_util
       else:
           return False, 'none', avg_util
   ```

4. **Add `get_dataloader_statistics` method:**
   ```python
   def get_dataloader_statistics(self) -> Dict[str, Any]:
       """Get aggregated DataLoader performance statistics."""
       with self._lock:
           metrics = self._dataloader_metrics.copy()
           total_sequences = self._total_sequences
           total_tokens = self._total_tokens

       if not metrics:
           return {}

       wait_times = [m.wait_time_ms for m in metrics]
       queue_states = [m.queue_state for m in metrics]

       # Calculate packing efficiency from batch composition
       seq_lengths = [m.avg_sequence_length for m in metrics if m.avg_sequence_length > 0]
       max_lengths = [m.max_sequence_length for m in metrics if m.max_sequence_length > 0]

       # Packing efficiency: actual_tokens / (num_sequences * max_length_in_batch)
       # For packed batches: high efficiency means less padding waste
       total_actual_tokens = sum(m.tokens_in_batch for m in metrics)
       total_theoretical_tokens = sum(
           m.sequences_in_batch * m.max_sequence_length
           for m in metrics if m.max_sequence_length > 0
       )
       packing_efficiency = (
           total_actual_tokens / total_theoretical_tokens
           if total_theoretical_tokens > 0 else 0.0
       )

       # Queue state distribution
       queue_state_counts = {
           'full': sum(1 for s in queue_states if s == 'full'),
           'normal': sum(1 for s in queue_states if s == 'normal'),
           'starved': sum(1 for s in queue_states if s == 'starved'),
       }

       return {
           # Wait time metrics
           'avg_wait_time_ms': sum(wait_times) / len(wait_times),
           'max_wait_time_ms': max(wait_times),
           'min_wait_time_ms': min(wait_times),
           'p95_wait_time_ms': sorted(wait_times)[int(len(wait_times) * 0.95)] if wait_times else 0,

           # Queue state heuristic (not actual queue depth)
           'queue_state_distribution': queue_state_counts,
           'pct_starved': queue_state_counts['starved'] / len(metrics) * 100 if metrics else 0,

           # Batch composition
           'avg_sequence_length': sum(seq_lengths) / len(seq_lengths) if seq_lengths else 0,
           'avg_max_length': sum(max_lengths) / len(max_lengths) if max_lengths else 0,

           # Packing efficiency
           'packing_efficiency': packing_efficiency,

           # Volume
           'total_batches': len(metrics),
           'total_sequences': total_sequences,
           'total_tokens': total_tokens,
       }
   ```

5. **Add `start_inference_timer` and `get_throughput` methods:**
   ```python
   def start_inference_timer(self) -> None:
       """Start timer for throughput calculation."""
       self._inference_start_time = time.time()

   def get_throughput(self) -> Dict[str, float]:
       """
       Get throughput metrics.

       Returns both sequences/sec and tokens/sec.
       tokens/sec is more stable for packed batches with variable sequence lengths.
       """
       if self._inference_start_time is None:
           return {
               'sequences_per_sec': 0.0,
               'tokens_per_sec': 0.0,
               'elapsed_seconds': 0.0
           }

       elapsed = time.time() - self._inference_start_time
       seqs_per_sec = self._total_sequences / elapsed if elapsed > 0 else 0.0
       tokens_per_sec = self._total_tokens / elapsed if elapsed > 0 else 0.0

       return {
           'sequences_per_sec': seqs_per_sec,
           'tokens_per_sec': tokens_per_sec,
           'elapsed_seconds': elapsed,
           'total_sequences': self._total_sequences,
           'total_tokens': self._total_tokens,
       }
   ```

6. **Update `get_statistics` to include DataLoader metrics:**
   - Add DataLoader stats to the returned dict under `'dataloader'` key
  </action>
  <verify>
```bash
python -c "
import time
from virnucpro.utils.gpu_monitor import NvitopMonitor

# NOTE: queue_depth is informational only - DataLoader doesn't expose actual queue fill level
# The monitor uses wait_time heuristic to infer queue state instead

# Create monitor (may not have GPU, that's fine)
monitor = NvitopMonitor(device_ids=[0], log_interval=1.0)

# Test DataLoader metrics recording
monitor.start_inference_timer()
for i in range(5):
    monitor.record_dataloader_wait(
        wait_time_ms=2.5 + i * 0.5,
        batch_idx=i,
        sequences_in_batch=32,
        tokens_in_batch=1024,
        avg_sequence_length=32.0,
        max_sequence_length=64
    )

time.sleep(0.1)  # Small delay to get non-zero elapsed time

# Get statistics
dl_stats = monitor.get_dataloader_statistics()
assert 'avg_wait_time_ms' in dl_stats, 'Missing avg_wait_time_ms'
assert 'total_batches' in dl_stats, 'Missing total_batches'
assert 'packing_efficiency' in dl_stats, 'Missing packing_efficiency'
assert 'queue_state_distribution' in dl_stats, 'Missing queue_state_distribution'
assert dl_stats['total_batches'] == 5, f'Expected 5 batches, got {dl_stats[\"total_batches\"]}'

# Get throughput
throughput = monitor.get_throughput()
assert 'sequences_per_sec' in throughput, 'Missing sequences_per_sec'
assert 'tokens_per_sec' in throughput, 'Missing tokens_per_sec'
assert throughput['total_sequences'] == 160, f'Expected 160 seqs, got {throughput[\"total_sequences\"]}'
assert throughput['total_tokens'] == 5120, f'Expected 5120 tokens, got {throughput[\"total_tokens\"]}'

print(f'GPU monitor DataLoader tracking test passed')
print(f'  Avg wait: {dl_stats[\"avg_wait_time_ms\"]:.1f}ms')
print(f'  Throughput: {throughput[\"sequences_per_sec\"]:.0f} seqs/sec, {throughput[\"tokens_per_sec\"]:.0f} tokens/sec')
print(f'  Packing efficiency: {dl_stats[\"packing_efficiency\"]:.2%}')
print(f'  Queue state: {dl_stats[\"queue_state_distribution\"]}')
"
```
  </verify>
  <done>NvitopMonitor tracks DataLoader wait times, infers queue state heuristically, uses tiered bottleneck detection (50%/80%), and reports sequences/sec + tokens/sec throughput with packing efficiency</done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `DataLoaderMetrics` dataclass exists with wait_time_ms, batch composition, queue_state heuristic
2. `infer_queue_state()` helper infers queue state from wait time
3. `NvitopMonitor.record_dataloader_wait()` accepts wait time and batch metadata (no queue_depth param)
4. `NvitopMonitor.check_bottleneck()` uses tiered thresholds (50%/80%) and returns severity
5. `NvitopMonitor.get_dataloader_statistics()` returns packing_efficiency, queue_state_distribution, batch composition
6. `NvitopMonitor.get_throughput()` returns both sequences/sec and tokens/sec

NOTE: queue_depth/queue_state is heuristic only - DataLoader doesn't expose actual queue fill level
</verification>

<success_criteria>
- DataLoaderMetrics captures wait_time_ms, batch composition (avg/max seq length), heuristic queue_state
- infer_queue_state() heuristic: <1ms=full, >50ms=starved, else=normal
- record_dataloader_wait() stores metrics, infers queue state, logs every 10 batches
- check_bottleneck() uses tiered thresholds: <50% critical, <80% mild, ≥80% none
- get_dataloader_statistics() includes packing_efficiency, queue_state_distribution, avg_sequence_length
- get_throughput() returns sequences/sec AND tokens/sec (more stable for packed batches)
</success_criteria>

<output>
After completion, create `.planning/phases/05-async-dataloader-foundation/05-02-SUMMARY.md`
</output>
