---
phase: 01.1-parallel-translation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [virnucpro/pipeline/parallel_translate.py, virnucpro/utils/sequence.py]
autonomous: true

must_haves:
  truths:
    - "Translation uses multiprocessing with spawn context for safety"
    - "Worker function is top-level and picklable"
    - "Memory-efficient processing with Pool.imap() for 22M sequences"
  artifacts:
    - path: "virnucpro/pipeline/parallel_translate.py"
      provides: "Parallel translation orchestration"
      min_lines: 100
      exports: ["parallel_translate_sequences", "translate_sequence_worker"]
    - path: "virnucpro/utils/sequence.py"
      provides: "Updated identify_seq for parallel compatibility"
      contains: "identify_seq"
  key_links:
    - from: "parallel_translate.py"
      to: "sequence.identify_seq"
      via: "import and call"
      pattern: "from virnucpro.utils.sequence import identify_seq"
---

<objective>
Create parallel translation worker functions and orchestration for CPU multiprocessing.

Purpose: Enable six-frame translation to parallelize across CPU cores, reducing processing time from >10 minutes to under 2 minutes for 22M sequences.
Output: Worker function and parallel orchestration that processes sequences efficiently using multiprocessing.Pool with spawn context.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01.1-parallel-translation/01.1-RESEARCH.md

# Existing patterns to follow
@virnucpro/pipeline/work_queue.py
@virnucpro/utils/sequence.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create parallel translation worker and orchestration</name>
  <files>virnucpro/pipeline/parallel_translate.py</files>
  <action>
    Create parallel translation module with worker function and orchestration:

    1. Create translate_sequence_worker(record_data) as top-level function for picklability:
       - Accept (seqid, sequence) tuple
       - Call identify_seq() from virnucpro.utils.sequence
       - Return result or None for invalid sequences

    2. Create parallel_translate_sequences() function:
       - Accept input_file, output_nuc, output_pro paths, num_workers (default: cpu_count())
       - Use multiprocessing.get_context('spawn') to match existing GPU worker pattern
       - Create Pool with spawn context
       - Use Pool.imap() with chunksize=1000 for memory efficiency (not map() which loads all 22M results)
       - Create generator of (seqid, sequence) tuples from SeqIO.parse()
       - Process results lazily and write to output files as they arrive
       - Handle errors gracefully - log but continue processing

    3. Create parallel_translate_with_progress() variant:
       - Count sequences first for progress bar
       - Use tqdm or existing ProgressReporter pattern
       - Update progress bar as results arrive

    Follow patterns from work_queue.py for spawn context and parallel.py for worker structure.
    Use BioPython SeqIO for FASTA parsing (already in codebase).
    Avoid: Pool.map() (memory explosion), fork context (unsafe with threads), global state in workers.
  </action>
  <verify>File exists at virnucpro/pipeline/parallel_translate.py with translate_sequence_worker and parallel_translate_sequences functions</verify>
  <done>Parallel translation module created with worker function, basic orchestration, and progress variant</done>
</task>

<task type="auto">
  <name>Task 2: Add batch processing optimization</name>
  <files>virnucpro/pipeline/parallel_translate.py</files>
  <action>
    Enhance parallel_translate.py with batch processing optimizations:

    1. Add translate_batch_worker() function:
       - Accept list of (seqid, sequence) tuples instead of single
       - Process batch and return list of results
       - Reduces serialization overhead for 22M sequences

    2. Add create_sequence_batches() generator:
       - Yield batches of sequences (batch_size parameter, default 100)
       - Memory-efficient batching from SeqIO iterator
       - Ensure last batch is properly handled even if smaller

    3. Add parallel_translate_batched() function:
       - Use translate_batch_worker with batched sequences
       - Calculate optimal chunksize based on num_workers and total batches
       - Still use imap() for lazy evaluation
       - Flatten batch results when writing to files

    4. Add get_optimal_settings() helper:
       - Determine num_workers (default: cpu_count(), respect user override)
       - Calculate batch_size based on average sequence length
       - Calculate chunksize for Pool.imap()
       - Return (num_workers, batch_size, chunksize) tuple

    This reduces serialization overhead from 22M to ~220K operations (100x reduction).
  </action>
  <verify>grep -E "translate_batch_worker|create_sequence_batches|parallel_translate_batched" virnucpro/pipeline/parallel_translate.py</verify>
  <done>Batch processing optimization added with batch worker, sequence batching, and optimal settings calculation</done>
</task>

</tasks>

<verification>
- Module exists at virnucpro/pipeline/parallel_translate.py
- Contains translate_sequence_worker function (top-level, picklable)
- Contains parallel_translate_sequences using spawn context and Pool.imap()
- Contains progress reporting variant
- Contains batch processing optimizations
- Uses existing patterns from work_queue.py and parallel.py
</verification>

<success_criteria>
- parallel_translate.py module created with worker functions
- Spawn context explicitly used (matching GPU worker pattern)
- Pool.imap() used for memory efficiency (not map())
- Progress reporting integrated
- Batch processing reduces serialization overhead
</success_criteria>

<output>
After completion, create `.planning/phases/01.1-parallel-translation/01.1-01-SUMMARY.md`
</output>