---
phase: 01.1-parallel-translation
plan: 01
subsystem: pipeline
tags: [multiprocessing, cpu-parallelism, translation, six-frame, pool-imap, spawn-context]

# Dependency graph
requires:
  - phase: 01-esm2-multi-gpu
    provides: Spawn context pattern and worker orchestration from work_queue.py
provides:
  - Parallel translation worker functions with spawn context
  - Memory-efficient batch processing for 22M sequences
  - Pool.imap() lazy evaluation pattern for CPU-bound translation
affects: [01.1-02, 01.1-03]

# Tech tracking
tech-stack:
  added: []
  patterns: [batch-worker-pattern, lazy-evaluation-imap, optimal-settings-calculation]

key-files:
  created: [virnucpro/pipeline/parallel_translate.py]
  modified: []

key-decisions:
  - "batch-processing-100x-reduction: Use batch worker to reduce serialization from 22M to 220K operations (100x improvement)"
  - "spawn-context-consistency: Use spawn context matching GPU worker pattern for consistency and safety"
  - "imap-for-memory: Use Pool.imap() instead of Pool.map() for lazy evaluation with 22M sequences"
  - "optimal-settings-helper: Provide get_optimal_settings() to calculate num_workers, batch_size, chunksize based on data characteristics"

patterns-established:
  - "translate_batch_worker: Top-level batch worker processes multiple sequences in single call"
  - "create_sequence_batches: Generator pattern for memory-efficient batching from iterators"
  - "get_optimal_settings: Calculate parameters based on sequence length and worker count"

# Metrics
duration: 2.1min
completed: 2026-01-23
---

# Phase 01.1 Plan 01: Parallel Translation Worker Summary

**CPU multiprocessing with spawn context and Pool.imap() for memory-efficient six-frame translation, reducing serialization overhead from 22M to 220K operations (100x)**

## Performance

- **Duration:** 2.1 min
- **Started:** 2026-01-23T13:00:36Z
- **Completed:** 2026-01-23T13:02:40Z
- **Tasks:** 2
- **Files modified:** 1

## Accomplishments

- Created parallel translation module with worker function and orchestration
- Implemented batch processing optimization reducing serialization overhead 100x
- Used spawn context and Pool.imap() matching existing GPU worker patterns
- Added optimal settings calculation for parameter tuning

## Task Commits

Each task was committed atomically:

1. **Task 1: Create parallel translation worker and orchestration** - `37dd4b8` (feat)
   - translate_sequence_worker as top-level picklable function
   - parallel_translate_sequences with spawn context and Pool.imap()
   - parallel_translate_with_progress variant

2. **Task 2: Add batch processing optimization** - `89b99c9` (feat)
   - translate_batch_worker for batch processing
   - create_sequence_batches generator for batching
   - parallel_translate_batched function
   - get_optimal_settings helper

## Files Created/Modified

- `virnucpro/pipeline/parallel_translate.py` - Parallel translation orchestration with worker functions, batch processing, and optimal settings calculation (431 lines)

## Decisions Made

**batch-processing-100x-reduction:** Used translate_batch_worker to process sequences in batches (default 100), reducing serialization overhead from 22M individual operations to ~220K batch operations (100x reduction). Rationale: Multiprocessing pickle overhead is significant - batching amortizes this cost across multiple sequences.

**spawn-context-consistency:** Explicitly used spawn context via `multiprocessing.get_context('spawn')` to match existing GPU worker pattern from work_queue.py. Rationale: Consistency with Phase 1 patterns, safety with BioPython (may use NumPy threads), and preparation for Python 3.14 default.

**imap-for-memory:** Used Pool.imap() instead of Pool.map() for lazy evaluation. Rationale: Pool.map() would eagerly load all 22M results into memory before returning, causing memory exhaustion. Pool.imap() yields results as workers complete, enabling streaming writes.

**optimal-settings-helper:** Created get_optimal_settings() to calculate num_workers, batch_size, and chunksize based on sequence length and worker count. Rationale: Optimal parameters vary with data characteristics - automated calculation reduces need for manual tuning.

## Deviations from Plan

None - plan executed exactly as written.

## Issues Encountered

None - implementation proceeded smoothly following established patterns from work_queue.py and parallel.py.

## User Setup Required

None - no external service configuration required.

## Next Phase Readiness

**Ready for Phase 01.1-02 (CLI Integration):**
- Parallel translation functions ready to integrate into pipeline
- Worker functions tested for picklability
- Spawn context matches existing GPU worker pattern
- Multiple orchestration options available (basic, batched, with progress)

**For Phase 01.1-03 (Performance Testing):**
- Batch size and chunksize parameters exposed for tuning
- get_optimal_settings() provides starting point for profiling
- All functions return (processed, valid_orfs) tuple for verification

**No blockers identified.**

---
*Phase: 01.1-parallel-translation*
*Completed: 2026-01-23*
