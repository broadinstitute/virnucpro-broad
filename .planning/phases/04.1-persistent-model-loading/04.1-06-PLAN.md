---
phase: 04.1-persistent-model-loading
plan: 06
type: execute
wave: 3
depends_on: [04.1-04, 04.1-05]
files_modified: [tests/integration/test_persistent_workers.py]
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Integration tests match actual PersistentWorkerPool API"
    - "Tests verify models are NOT reloaded per file"
    - "Tests confirm output equivalence between persistent and standard workers"
  artifacts:
    - path: "tests/integration/test_persistent_workers.py"
      provides: "Fixed integration tests that actually run"
      min_lines: 400
  key_links:
    - from: "test_persistent_workers.py"
      to: "PersistentWorkerPool"
      via: "uses correct API methods"
      pattern: "pool\\.create_pool\\(\\)"
---

<objective>
Fix integration tests to match actual PersistentWorkerPool API and add verification tests.

Purpose: Close the gap where tests use wrong API (pool._create_pool instead of create_pool, wrong __init__ params) and add tests to verify models aren't reloaded.
Output: Corrected integration tests that properly test persistent worker functionality.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-persistent-model-loading/04.1-VERIFICATION.md

# Test file to fix
@tests/integration/test_persistent_workers.py

# Implementation to test
@virnucpro/pipeline/persistent_pool.py
@virnucpro/pipeline/work_queue.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix PersistentWorkerPool test API usage</name>
  <files>tests/integration/test_persistent_workers.py</files>
  <action>
    Fix tests in TestPersistentPoolInitialization class to use correct API.

    Current issues:
    - Line 28: __init__ expects model_type, not initializer/worker_func
    - Line 49: calls pool._create_pool() but method is create_pool()
    - Line 51: close_pool() should be close()

    Correct pattern for test_pool_creation():
    ```python
    def test_pool_creation():
        """Test persistent pool can be created and closed"""
        pool = PersistentWorkerPool(
            num_gpus=2,
            model_type='esm2'  # Not initializer/worker_func
        )

        # Create the pool
        pool.create_pool()  # Not _create_pool()

        # Verify pool exists
        assert pool.pool is not None
        assert pool.pool._processes is not None

        # Close the pool
        pool.close()  # Not close_pool()

        # Verify cleanup
        assert pool.pool is None
    ```

    Fix test_pool_model_loading():
    ```python
    def test_pool_model_loading():
        """Test models are loaded in pool initialization"""
        pool = PersistentWorkerPool(
            num_gpus=1,
            model_type='dnabert'
        )

        pool.create_pool()

        # Process a dummy job to verify model is loaded
        import tempfile
        from pathlib import Path

        with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
            f.write('>seq1\nACGT\n')
            test_file = Path(f.name)

        try:
            output_dir = Path(tempfile.mkdtemp())
            result = pool.process_job(0, [test_file], output_dir=output_dir)
            assert len(result) == 1
            assert result[0].exists()
        finally:
            test_file.unlink()
            pool.close()
    ```

    Update all tests in this class to use correct API.
  </action>
  <verify>grep -n "pool.create_pool()\|model_type=" tests/integration/test_persistent_workers.py | head -5</verify>
  <done>TestPersistentPoolInitialization tests use correct PersistentWorkerPool API</done>
</task>

<task type="auto">
  <name>Task 2: Add test verifying models are NOT reloaded</name>
  <files>tests/integration/test_persistent_workers.py</files>
  <action>
    Add new test class to verify the key behavior - models are loaded once and reused.

    Add TestModelPersistence class:
    ```python
    class TestModelPersistence:
        """Test that models are truly persistent and not reloaded"""

        @pytest.mark.gpu
        def test_models_not_reloaded_per_file(self, caplog):
            """Verify models are loaded once and reused for multiple files"""
            import logging
            caplog.set_level(logging.INFO)

            pool = PersistentWorkerPool(
                num_gpus=1,
                model_type='dnabert'
            )

            pool.create_pool()

            # Create multiple test files
            test_files = []
            output_dir = Path(tempfile.mkdtemp())

            try:
                for i in range(3):
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
                        f.write(f'>seq{i}\nACGTACGT\n')
                        test_files.append(Path(f.name))

                # Process all files
                for i, test_file in enumerate(test_files):
                    result = pool.process_job(0, [test_file], output_dir=output_dir, batch_size=256)
                    assert len(result) == 1

                    # Check logs - model loading should only appear once
                    if i == 0:
                        # First file - model should be loaded
                        assert "Loading DNABERT-S model" in caplog.text or "Using pre-loaded dnabert model" in caplog.text
                    else:
                        # Subsequent files - should NOT see model loading
                        caplog.clear()
                        result = pool.process_job(0, [test_file], output_dir=output_dir, batch_size=256)
                        assert "Loading DNABERT-S model" not in caplog.text
                        assert "from_pretrained" not in caplog.text

            finally:
                for f in test_files:
                    f.unlink(missing_ok=True)
                pool.close()

        @pytest.mark.gpu
        def test_persistent_vs_standard_performance(self):
            """Compare processing time: persistent should be faster on multiple files"""
            import time

            # Create test files
            test_files = []
            output_dir = Path(tempfile.mkdtemp())

            try:
                for i in range(5):
                    with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
                        f.write(f'>seq{i}\n' + 'ACGT' * 100 + '\n')
                        test_files.append(Path(f.name))

                # Time persistent pool
                start = time.time()
                pool = PersistentWorkerPool(num_gpus=1, model_type='dnabert')
                pool.create_pool()

                for test_file in test_files:
                    pool.process_job(0, [test_file], output_dir=output_dir, batch_size=256)

                pool.close()
                persistent_time = time.time() - start

                # Time standard approach (would reload model each time)
                # Note: This is a simulation - in reality standard workers also use pools
                # but they recreate pools more frequently

                # Log the times for debugging
                print(f"Persistent pool time: {persistent_time:.2f}s")
                # Assert persistent is working (should complete in reasonable time)
                assert persistent_time < 60  # Should process 5 small files in under 60s

            finally:
                for f in test_files:
                    f.unlink(missing_ok=True)
    ```

    This verifies the core goal - models stay loaded across multiple jobs.
  </action>
  <verify>grep -n "class TestModelPersistence\|test_models_not_reloaded_per_file" tests/integration/test_persistent_workers.py</verify>
  <done>Added TestModelPersistence class with tests verifying models aren't reloaded</done>
</task>

<task type="auto">
  <name>Task 3: Fix BatchQueueManager persistent pool tests</name>
  <files>tests/integration/test_persistent_workers.py</files>
  <action>
    Fix TestBatchQueueManagerPersistentPool to test actual workflow.

    Update test_queue_manager_with_persistent_pool():
    ```python
    def test_queue_manager_with_persistent_pool(self):
        """Test BatchQueueManager creates and uses persistent pool"""
        from virnucpro.pipeline.work_queue import BatchQueueManager
        from virnucpro.pipeline.parallel_dnabert import process_dnabert_files_worker

        # Create queue manager with persistent pool enabled
        queue_manager = BatchQueueManager(
            num_gpus=1,
            worker_func=process_dnabert_files_worker,
            use_persistent_pool=True
        )

        # Create the persistent pool (this was missing!)
        queue_manager.create_persistent_pool()

        try:
            # Verify pool was created
            assert queue_manager.persistent_pool is not None
            assert queue_manager.persistent_pool.pool is not None

            # Create test file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
                f.write('>test\nACGTACGT\n')
                test_file = Path(f.name)

            try:
                output_dir = Path(tempfile.mkdtemp())
                file_assignments = {0: [test_file]}

                # Process files using persistent pool
                processed, failed = queue_manager.process_files(
                    file_assignments,
                    output_dir=output_dir,
                    batch_size=256
                )

                # Verify processing succeeded
                assert len(processed) == 1
                assert len(failed) == 0
                assert processed[0].exists()

            finally:
                test_file.unlink(missing_ok=True)

        finally:
            # Clean up persistent pool
            queue_manager.close_persistent_pool()
            assert queue_manager.persistent_pool is None
    ```

    Add test for fallback behavior:
    ```python
    def test_queue_manager_fallback_without_create(self, caplog):
        """Test queue manager falls back to standard pool if create not called"""
        import logging
        caplog.set_level(logging.WARNING)

        queue_manager = BatchQueueManager(
            num_gpus=1,
            worker_func=process_dnabert_files_worker,
            use_persistent_pool=True
        )

        # Intentionally NOT calling create_persistent_pool()

        # Create test file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
            f.write('>test\nACGT\n')
            test_file = Path(f.name)

        try:
            output_dir = Path(tempfile.mkdtemp())
            file_assignments = {0: [test_file]}

            # Should fall back to standard pool
            processed, failed = queue_manager.process_files(
                file_assignments,
                output_dir=output_dir,
                batch_size=256
            )

            # Should see warning about fallback
            assert "Persistent pool requested but not created" in caplog.text

        finally:
            test_file.unlink(missing_ok=True)
    ```

    These tests verify the complete lifecycle and fallback behavior.
  </action>
  <verify>grep -n "queue_manager.create_persistent_pool\|queue_manager.close_persistent_pool" tests/integration/test_persistent_workers.py</verify>
  <done>Fixed BatchQueueManager tests to properly create/close persistent pools and test fallback</done>
</task>

</tasks>

<verification>
- All tests use correct PersistentWorkerPool API (model_type param, create_pool method)
- New tests verify models are NOT reloaded between files
- BatchQueueManager tests properly create and close persistent pools
- Fallback behavior is tested when create_persistent_pool not called
- Tests are marked with @pytest.mark.gpu for proper CI handling
</verification>

<success_criteria>
- Tests call pool.create_pool() not pool._create_pool()
- PersistentWorkerPool initialized with model_type='esm2' or 'dnabert'
- Model persistence verified through log analysis
- Queue manager lifecycle properly tested
- All tests can actually run (no API mismatches)
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-persistent-model-loading/04.1-06-SUMMARY.md`
</output>