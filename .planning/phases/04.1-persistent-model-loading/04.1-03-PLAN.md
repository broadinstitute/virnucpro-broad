---
phase: 04.1-persistent-model-loading
plan: 03
type: execute
wave: 3
depends_on: [04.1-01, 04.1-02]
files_modified: [virnucpro/pipeline/prediction.py, virnucpro/cli.py, tests/integration/test_persistent_workers.py]
autonomous: true

must_haves:
  truths:
    - "Pipeline can use persistent workers when enabled"
    - "CLI exposes flag to enable persistent model loading"
    - "Tests verify persistent workers maintain output correctness"
  artifacts:
    - path: "virnucpro/pipeline/prediction.py"
      provides: "Pipeline integration with persistent workers"
      contains: "use_persistent_pool"
    - path: "virnucpro/cli.py"
      provides: "CLI flag for persistent model loading"
      contains: "--persistent-models"
    - path: "tests/integration/test_persistent_workers.py"
      provides: "Integration tests for persistent workers"
      min_lines: 100
  key_links:
    - from: "cli.py"
      to: "prediction.py"
      via: "persistent_models parameter passed"
      pattern: "persistent_models.*=.*ctx.obj"
    - from: "prediction.py"
      to: "work_queue.py"
      via: "BatchQueueManager with use_persistent_pool"
      pattern: "BatchQueueManager.*use_persistent_pool"
---

<objective>
Integrate persistent worker pools into the pipeline and CLI with comprehensive testing.

Purpose: Enable users to opt into persistent model loading via CLI flag and ensure the feature works correctly.
Output: Pipeline and CLI integration with persistent workers, plus integration tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-persistent-model-loading/04.1-01-SUMMARY.md
@.planning/phases/04.1-persistent-model-loading/04.1-02-SUMMARY.md

# Current pipeline and CLI implementation
@virnucpro/pipeline/prediction.py
@virnucpro/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate persistent workers into pipeline</name>
  <files>virnucpro/pipeline/prediction.py</files>
  <action>
    Modify the main prediction pipeline to support persistent worker pools.

    Changes to make:
    1. Add persistent_models parameter to run_prediction() function signature at line 26 (default: False)

    2. Integrate with PersistentWorkerPool or BatchQueueManager based on persistent_models flag:

       Option A - Replace BatchQueueManager calls (lines 449, 628):
       - If persistent_models is True:
         * Import and create PersistentWorkerPool directly
         * Use persistent pool's process methods
         * Keep pool alive between DNABERT and ESM processing
       - Else: use existing BatchQueueManager

       Option B - Modify BatchQueueManager internally:
       - Pass use_persistent_pool=persistent_models to BatchQueueManager constructor
       - Let BatchQueueManager handle the pool type internally
       - This approach maintains cleaner separation of concerns

       Choose Option B for better encapsulation - modify lines 449 and 628:
       ```python
       queue_manager = BatchQueueManager(
           ...,
           use_persistent_pool=persistent_models
       )
       ```

    3. Add pool lifecycle management:
       - If using persistent pools, ensure they're properly closed in finally blocks
       - Handle exceptions gracefully to avoid GPU memory leaks

    4. Add necessary imports at the top of the file

    Ensure:
    - Default behavior unchanged (persistent_models=False)
    - Pools properly closed even on errors
    - Progress reporting still works with persistent pools
  </action>
  <verify>Run grep -n "persistent_models\|use_persistent_pool" virnucpro/pipeline/prediction.py to confirm integration</verify>
  <done>Pipeline integrated with persistent worker pools, supporting both modes</done>
</task>

<task type="auto">
  <name>Task 2: Add CLI flag for persistent models</name>
  <files>virnucpro/cli.py</files>
  <action>
    Add a CLI flag to enable persistent model loading.

    Changes:
    1. Add to predict command options (around other performance flags):
       ```python
       @click.option(
           '--persistent-models/--no-persistent-models',
           default=False,
           help='Keep models loaded in GPU memory between pipeline stages (reduces loading overhead but uses more memory)'
       )
       ```

    2. Pass persistent_models to run_prediction() function in the pipeline call

    3. Add logging to indicate when persistent models are enabled:
       - "Using persistent model loading - models will remain in GPU memory"

    Documentation in help text should mention:
    - Reduces model loading overhead between stages
    - Uses more GPU memory continuously
    - Recommended for processing multiple samples in sequence
  </action>
  <verify>Run virnucpro predict --help | grep -A2 "persistent-models" to see the new flag</verify>
  <done>CLI flag added for enabling persistent model loading</done>
</task>

<task type="auto">
  <name>Task 3: Create integration tests for persistent workers</name>
  <files>tests/integration/test_persistent_workers.py</files>
  <action>
    Create comprehensive integration tests for persistent worker functionality.

    Test cases to implement:
    1. test_persistent_pool_initialization():
       - Create PersistentWorkerPool
       - Verify workers initialized with models loaded
       - Check memory configuration set correctly

    2. test_persistent_vs_standard_output_equivalence():
       - Process same files with persistent and standard workers
       - Compare outputs to ensure identical results
       - Verify both ESM-2 and DNABERT-S produce same features

    3. test_memory_management():
       - Process multiple file batches
       - Verify torch.cuda.empty_cache() called periodically
       - Check memory doesn't grow unbounded

    4. test_pool_lifecycle():
       - Create pool, process files, close pool
       - Verify resources properly released
       - Check no GPU memory leaks after closure

    5. test_cli_persistent_flag():
       - Test --persistent-models flag enables feature
       - Test --no-persistent-models uses standard workers
       - Verify logging indicates correct mode

    Use pytest fixtures for:
    - Sample FASTA files
    - GPU availability checks (add @pytest.mark.gpu decorator, skip if no GPU)
    - Cleanup after tests

    Import patterns from existing tests in tests/integration/
  </action>
  <verify>Run pytest tests/integration/test_persistent_workers.py::test_persistent_pool_initialization -v -k "not gpu" to verify test structure (will skip GPU tests in CI)</verify>
  <done>Integration tests created covering persistent worker functionality, output equivalence, and memory management</done>
</task>

</tasks>

<verification>
- Pipeline supports persistent worker pools via parameter
- CLI exposes --persistent-models flag
- Integration tests verify correctness and memory management
- Backward compatibility maintained (default: standard workers)
</verification>

<success_criteria>
- Users can enable persistent models via CLI flag
- Pipeline correctly uses persistent pools when enabled
- Tests confirm output equivalence with standard workers
- Memory management prevents fragmentation issues
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-persistent-model-loading/04.1-03-SUMMARY.md`
</output>