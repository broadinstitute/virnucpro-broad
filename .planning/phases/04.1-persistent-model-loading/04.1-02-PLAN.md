---
phase: 04.1-persistent-model-loading
plan: 02
type: execute
wave: 2
depends_on: [04.1-01]
files_modified: [virnucpro/pipeline/parallel_esm.py, virnucpro/pipeline/parallel_dnabert.py]
autonomous: true

must_haves:
  truths:
    - "Workers can process files with pre-loaded models"
    - "Model loading happens in worker initializer, not in worker function"
    - "Workers implement periodic cache clearing during processing"
  artifacts:
    - path: "virnucpro/pipeline/parallel_esm.py"
      provides: "Persistent ESM-2 worker functions"
      exports: ["init_esm_worker", "process_esm_files_persistent"]
    - path: "virnucpro/pipeline/parallel_dnabert.py"
      provides: "Persistent DNABERT-S worker functions"
      exports: ["init_dnabert_worker", "process_dnabert_files_persistent"]
  key_links:
    - from: "parallel_esm.py"
      to: "models/esm2_flash.py"
      via: "load_esm2_model in initializer"
      pattern: "load_esm2_model.*in.*init_esm_worker"
    - from: "parallel_dnabert.py"
      to: "models/dnabert_flash.py"
      via: "load_dnabert_model in initializer"
      pattern: "load_dnabert_model.*in.*init_dnabert_worker"
---

<objective>
Implement persistent worker functions for ESM-2 and DNABERT-S that reuse pre-loaded models.

Purpose: Create worker functions that use models loaded during pool initialization rather than loading models on each invocation.
Output: Persistent worker functions for both ESM-2 and DNABERT-S feature extraction.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-persistent-model-loading/04.1-01-SUMMARY.md

# Current worker implementations to adapt
@virnucpro/pipeline/parallel_esm.py
@virnucpro/pipeline/parallel_dnabert.py
@virnucpro/models/esm2_flash.py
@virnucpro/models/dnabert_flash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add persistent ESM-2 worker functions</name>
  <files>virnucpro/pipeline/parallel_esm.py</files>
  <action>
    Add persistent worker functions for ESM-2 that use pre-loaded models.

    New functions to add:
    1. Module-level globals for model storage:
       - _esm_model = None
       - _batch_converter = None
       - _device = None

    2. init_esm_worker(device_id, model_name="esm2_t36_3B_UR50D", log_level=logging.INFO):
       - Configure PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' BEFORE any CUDA calls
       - Set up logging in worker
       - Initialize CUDA context with torch.device(f'cuda:{device_id}')
       - Load ESM-2 model using load_esm2_model() from virnucpro.models.esm2_flash
       - Store model and batch_converter in module globals
       - Log successful initialization

    3. process_esm_files_persistent(file_subset, device_id, **kwargs):
       - Use pre-loaded model from globals (no loading)
       - Process files using extract_esm_features() with cached model
       - Implement periodic cache clearing: torch.cuda.empty_cache() every 10 files
       - Return (processed_files, failed_files) tuple
       - Include progress queue reporting if available

    Keep existing process_esm_files_worker() unchanged for backward compatibility.
    Reference research pattern from lines 263-352.
  </action>
  <verify>Check that init_esm_worker and process_esm_files_persistent functions exist in parallel_esm.py</verify>
  <done>Persistent ESM-2 worker functions implemented with model caching and periodic cache clearing</done>
</task>

<task type="auto">
  <name>Task 2: Add persistent DNABERT-S worker functions</name>
  <files>virnucpro/pipeline/parallel_dnabert.py</files>
  <action>
    Add persistent worker functions for DNABERT-S that use pre-loaded models.

    New functions to add:
    1. Module-level globals for model storage:
       - _dnabert_model = None
       - _tokenizer = None
       - _device = None

    2. init_dnabert_worker(device_id, model_name="zhihan1996/DNABERT-S", log_level=logging.INFO):
       - Configure PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' BEFORE any CUDA calls
       - Set up logging in worker
       - Initialize CUDA context with torch.device(f'cuda:{device_id}')
       - Load DNABERT-S model using load_dnabert_model() from virnucpro.models.dnabert_flash
       - Store model and tokenizer in module globals
       - Log successful initialization

    3. process_dnabert_files_persistent(file_subset, device_id, **kwargs):
       - Use pre-loaded model from globals (no loading)
       - Process files using extract_dnabert_features() with cached model
       - Implement periodic cache clearing: torch.cuda.empty_cache() every 10 files
       - Return (processed_files, failed_files) tuple
       - Include progress queue reporting if available

    Keep existing process_dnabert_files_worker() unchanged for backward compatibility.
    Mirror the ESM-2 pattern for consistency.
  </action>
  <verify>Check that init_dnabert_worker and process_dnabert_files_persistent functions exist in parallel_dnabert.py</verify>
  <done>Persistent DNABERT-S worker functions implemented with model caching and periodic cache clearing</done>
</task>

</tasks>

<verification>
- Both ESM-2 and DNABERT-S have persistent worker functions
- Models loaded once in initializer, reused in worker functions
- Periodic cache clearing implemented every 10 files
- Module globals store models for worker lifetime
</verification>

<success_criteria>
- Persistent worker functions created for both model types
- Models loaded in initializer, not in worker function
- Cache clearing prevents memory fragmentation
- Backward compatibility maintained with existing workers
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-persistent-model-loading/04.1-02-SUMMARY.md`
</output>