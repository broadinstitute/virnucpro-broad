# Phase 4.1: Persistent Model Loading - Research

**Researched:** 2026-01-24
**Domain:** PyTorch multiprocessing with persistent GPU workers
**Confidence:** HIGH

## Summary

Persistent model loading keeps PyTorch models in GPU memory across multiple job invocations by using long-lived worker processes with `maxtasksperchild=None` and loading models during pool initialization rather than per-task. This eliminates model loading overhead (2-5 minutes per ESM-2 3B model load) but introduces memory management complexity and fragmentation risks.

The current architecture loads models fresh in each worker's `process_files_worker()` function when processing file subsets. For job-based workloads where the same worker processes multiple independent file sets, keeping models loaded between jobs can eliminate significant overhead. However, this requires careful memory management to prevent fragmentation-induced OOM errors in long-running workers.

**Primary recommendation:** Use `Pool(initializer=load_model_fn)` with `maxtasksperchild=None` to load models once per worker process, combined with periodic CUDA cache clearing and expandable segments to manage fragmentation.

## Standard Stack

The established libraries/tools for persistent worker patterns:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| multiprocessing | stdlib | Process pool with initializers | Python's built-in approach for persistent workers |
| PyTorch | 2.2+ | Model serving and CUDA management | Already in stack, provides CUDA memory APIs |
| torch.cuda | part of PyTorch | Memory management and cache control | Essential for preventing fragmentation |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| TorchServe | latest | Production model serving | If building dedicated model server (out of scope) |
| Ray Serve | 2.x | Distributed model serving | If scaling beyond single-machine (out of scope) |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Pool initializer | Load per-task | Simpler but 2-5 min overhead per job |
| maxtasksperchild=None | maxtasksperchild=N | Periodic process refresh prevents fragmentation but reloads models |
| Manual memory mgmt | Let PyTorch handle it | Simpler but risks OOM from fragmentation |

**Installation:**
No new dependencies required - uses existing PyTorch and stdlib multiprocessing.

## Architecture Patterns

### Recommended Project Structure
Current structure already supports this - no changes needed:
```
virnucpro/pipeline/
├── base_worker.py       # BaseEmbeddingWorker abstraction
├── work_queue.py        # BatchQueueManager orchestration
├── parallel_esm.py      # ESM-2 worker functions
└── parallel_dnabert.py  # DNABERT-S worker functions
```

### Pattern 1: Pool Initializer with Model Caching
**What:** Load models during worker initialization, store in module-level globals
**When to use:** Processing multiple independent file sets with same worker processes
**Example:**
```python
# Source: Python multiprocessing documentation + PyTorch best practices
import torch
import multiprocessing as mp

# Module-level globals (set by Pool initializer)
_model = None
_tokenizer = None
_device = None

def init_worker(device_id):
    """Initialize worker with persistent model."""
    global _model, _tokenizer, _device

    _device = torch.device(f'cuda:{device_id}')

    # Load model once during worker initialization
    from virnucpro.models.esm2_flash import load_esm2_model
    _model, batch_converter = load_esm2_model(
        model_name="esm2_t36_3B_UR50D",
        device=str(_device)
    )
    _tokenizer = batch_converter

    # Configure memory management for long-running workers
    if torch.cuda.is_available():
        torch.cuda.set_per_process_memory_fraction(0.9, _device)
        # Enable expandable segments to reduce fragmentation
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

def worker_fn(file_subset, device_id, **kwargs):
    """Worker function reuses pre-loaded model."""
    global _model, _tokenizer, _device

    # Model already loaded - no initialization overhead
    processed = []
    for file in file_subset:
        # Process using cached model
        result = process_file(file, _model, _tokenizer, _device)
        processed.append(result)

        # Periodic cache clearing to prevent fragmentation
        if len(processed) % 10 == 0:
            torch.cuda.empty_cache()

    return processed

# Create pool with persistent workers
ctx = mp.get_context('spawn')
with ctx.Pool(
    num_gpus,
    initializer=init_worker,
    initargs=(device_id,),
    maxtasksperchild=None  # Workers persist for pool lifetime
) as pool:
    # Workers process multiple jobs without reloading models
    results = pool.starmap(worker_fn, job_assignments)
```

### Pattern 2: Job Queue with Persistent Workers
**What:** Single worker pool handles multiple independent jobs sequentially
**When to use:** Processing multiple samples (jobs) in a single pipeline run
**Example:**
```python
# Source: Inferred from current BatchQueueManager pattern
class PersistentWorkerQueue:
    """Manage persistent workers across multiple jobs."""

    def __init__(self, num_workers, model_type):
        self.ctx = mp.get_context('spawn')
        self.pool = self.ctx.Pool(
            num_workers,
            initializer=self._init_worker,
            initargs=(model_type,),
            maxtasksperchild=None  # Persistent workers
        )

    def process_job(self, file_assignments, **kwargs):
        """Process single job with persistent workers."""
        return self.pool.starmap(worker_fn, file_assignments)

    def process_multiple_jobs(self, jobs):
        """Process multiple jobs sequentially without reloading models."""
        results = []
        for job in jobs:
            result = self.process_job(job['files'])
            results.append(result)
            # Optional: periodic cache clearing between jobs
            self._clear_worker_caches()
        return results

    def close(self):
        """Clean shutdown of persistent workers."""
        self.pool.close()
        self.pool.join()
```

### Pattern 3: Memory Management for Long-Running Workers
**What:** Periodic cache clearing and expandable segments to prevent fragmentation
**When to use:** Always with persistent workers processing variable-length batches
**Example:**
```python
# Source: PyTorch CUDA memory management best practices
import os
import torch

def configure_persistent_worker_memory(device):
    """Configure memory settings for long-running workers."""
    # Enable expandable segments (PyTorch 2.2+)
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

    # Set memory fraction to leave headroom
    torch.cuda.set_per_process_memory_fraction(0.9, device)

    # Disable memory caching across allocations (optional, for extreme cases)
    # torch.cuda.empty_cache()  # Only call periodically, not every batch

def process_with_periodic_clearing(files, model, interval=10):
    """Process files with periodic cache clearing."""
    processed = []

    for i, file in enumerate(files):
        result = process_file(file, model)
        processed.append(result)

        # Clear cache every N files to prevent fragmentation
        if (i + 1) % interval == 0:
            torch.cuda.empty_cache()

    return processed
```

### Anti-Patterns to Avoid
- **Loading models in worker function body:** Creates overhead on every task, defeats purpose of persistent workers
- **maxtasksperchild=1:** Forces worker replacement after each task, reloading models constantly
- **No cache clearing:** Long-running workers accumulate fragmentation, leading to OOM at <70% GPU utilization
- **Sharing model objects via pickle:** Breaks with spawn context, use module globals set by initializer instead

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Model serving framework | Custom request router | TorchServe | Handles batching, versioning, multi-model serving (out of scope for batch jobs) |
| Worker lifecycle management | Manual process spawning | multiprocessing.Pool | Handles worker creation, task distribution, error handling |
| CUDA memory management | Manual allocator | PyTorch caching allocator + config | Handles block reuse, fragmentation mitigation with expandable_segments |
| Inter-process communication | Custom Queue impl | multiprocessing Queue/SimpleQueue | Handles serialization, inheritance vs pickling |

**Key insight:** Persistent workers are a well-established pattern in Python multiprocessing. The complexity comes from GPU memory management, not the worker pattern itself. Use stdlib Pool with proper CUDA configuration rather than building custom worker orchestration.

## Common Pitfalls

### Pitfall 1: Memory Fragmentation in Long-Running Workers
**What goes wrong:** Workers accumulate fragmented CUDA memory, leading to OOM errors at 50-70% GPU utilization despite available memory
**Why it happens:** PyTorch's caching allocator reuses blocks but can't always find contiguous blocks for new allocations. Variable batch sizes exacerbate this. Multi-stream workers (from Phase 4) fragment memory across streams.
**How to avoid:**
- Set `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` before any CUDA calls
- Call `torch.cuda.empty_cache()` periodically (every 10-100 batches, not every batch)
- Sort sequences by length to minimize batch size variability
- Use `maxtasksperchild=N` (e.g., 50-100) as fallback if fragmentation persists
**Warning signs:** OOM errors with `nvidia-smi` showing 50-70% memory usage, increasing memory usage over time with fixed workload

### Pitfall 2: Model Loading Before Pool Creation
**What goes wrong:** Parent process loads model on CUDA before spawning workers, causing "Cannot re-initialize CUDA in forked subprocess" or workers inheriting partial CUDA state
**Why it happens:** Spawn context isolates CUDA but any CUDA calls in parent before spawn create issues
**How to avoid:**
- Load models ONLY in worker initializer function, never in parent
- Move all model imports inside worker functions if they trigger CUDA
- Verify no `torch.cuda.*` calls before `Pool()` creation
**Warning signs:** "Cannot re-initialize CUDA in forked subprocess" errors, workers hanging on startup

### Pitfall 3: Persistent Workers with maxtasksperchild Default Behavior Confusion
**What goes wrong:** Assuming `maxtasksperchild=None` means workers are recreated, or setting it to 1 thinking it enables persistence
**Why it happens:** Inverted mental model - `None` = infinite tasks (persistent), `1` = one task then restart
**How to avoid:**
- Set `maxtasksperchild=None` explicitly for persistent workers
- Document intent clearly in code comments
- Monitor worker process lifecycle with logging in initializer
**Warning signs:** Models reloading unexpectedly, initialization logs appearing repeatedly

### Pitfall 4: Worker Process Memory Not Released Between Jobs
**What goes wrong:** Processing job A on GPU 0, then job B on GPU 1 leaves GPU 0 memory allocated
**Why it happens:** CUDA memory allocated by a process persists until process terminates, even if worker switches GPUs
**How to avoid:**
- Assign each worker to a fixed GPU (one worker per GPU)
- Don't dynamically reassign workers to different GPUs
- If GPU reassignment needed, use `maxtasksperchild=1` to force process refresh
**Warning signs:** Memory accumulation across all GPUs, workers holding memory on GPUs they're not currently using

### Pitfall 5: Queue Objects Pickled Instead of Inherited
**What goes wrong:** Passing `multiprocessing.Queue` as worker function argument triggers "Queue objects should only be shared between processes through inheritance" error
**Why it happens:** Spawn context can't pickle Queue objects safely
**How to avoid:**
- Pass Queue via Pool `initializer` so it's inherited, not pickled
- Store Queue in module-level global within worker
- Use SimpleQueue if inheritance not possible
**Warning signs:** RuntimeError about Queue sharing, workers failing to report progress

## Code Examples

Verified patterns from official sources:

### Worker Initialization with Model Loading
```python
# Source: Python multiprocessing docs + PyTorch spawn context best practices
import multiprocessing as mp
import torch
import os

# Module-level globals for workers
_model = None
_batch_converter = None
_device = None

def init_persistent_worker(device_id, model_name, log_level):
    """
    Initialize worker with persistent model.

    Called once per worker at pool creation time.
    All CUDA operations happen here, not in parent process.
    """
    global _model, _batch_converter, _device

    # Setup logging in worker
    import logging
    logging.basicConfig(level=log_level)
    logger = logging.getLogger(__name__)

    # Configure memory management BEFORE any CUDA calls
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

    # Initialize CUDA context
    _device = torch.device(f'cuda:{device_id}')

    # Load model (2-5 minute overhead paid ONCE per worker)
    logger.info(f"Worker {device_id}: Loading {model_name} (one-time initialization)")
    from virnucpro.models.esm2_flash import load_esm2_model
    _model, _batch_converter = load_esm2_model(
        model_name=model_name,
        device=str(_device),
        logger_instance=logger
    )

    logger.info(f"Worker {device_id}: Model loaded and ready")

def process_files_persistent(file_subset, device_id, **kwargs):
    """
    Process files using pre-loaded model.

    Model already in GPU memory - no loading overhead.
    """
    global _model, _batch_converter, _device

    import logging
    logger = logging.getLogger(__name__)

    processed = []
    failed = []

    # Process files with cached model
    for i, file in enumerate(file_subset):
        try:
            # Model already loaded - immediate processing
            result = extract_features(file, _model, _batch_converter, _device)
            processed.append(result)

            # Periodic cache clearing to prevent fragmentation
            if (i + 1) % 10 == 0:
                torch.cuda.empty_cache()
                logger.debug(f"Worker {device_id}: Cleared cache after {i+1} files")

        except Exception as e:
            logger.error(f"Worker {device_id}: Error on {file}: {e}")
            failed.append((file, str(e)))

    return processed, failed

# Usage
ctx = mp.get_context('spawn')
pool = ctx.Pool(
    num_workers,
    initializer=init_persistent_worker,
    initargs=(device_id, "esm2_t36_3B_UR50D", logging.INFO),
    maxtasksperchild=None  # Workers persist for pool lifetime
)

# Process multiple jobs without reloading models
job1_results = pool.starmap(process_files_persistent, job1_assignments)
job2_results = pool.starmap(process_files_persistent, job2_assignments)
# Model still loaded in workers - no reload overhead

pool.close()
pool.join()
```

### Memory Management for Persistent Workers
```python
# Source: PyTorch CUDA memory management best practices
import torch
import os

def configure_worker_memory(device, enable_expandable=True):
    """
    Configure CUDA memory for long-running workers.

    Call this BEFORE loading models in worker initializer.
    """
    if enable_expandable:
        # Reduce fragmentation by allowing segment expansion
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

    # Reserve 10% headroom for fragmentation
    torch.cuda.set_per_process_memory_fraction(0.9, device)

def should_clear_cache(batch_idx, interval=10):
    """
    Determine if cache clearing is needed.

    Clear periodically but not every batch (overhead vs benefit tradeoff).
    """
    return (batch_idx + 1) % interval == 0

# In worker function
for i, batch in enumerate(batches):
    results = model(batch)

    if should_clear_cache(i, interval=10):
        torch.cuda.empty_cache()
```

### Current Pattern vs Persistent Pattern
```python
# CURRENT PATTERN: Load model per worker invocation
def process_esm_files_worker(file_subset, device_id, **kwargs):
    """Current: Model loaded fresh for each file subset."""
    device = torch.device(f'cuda:{device_id}')

    # Model loading overhead: 2-5 minutes for ESM-2 3B
    model, batch_converter = load_esm2_model(
        model_name="esm2_t36_3B_UR50D",
        device=str(device)
    )

    # Process files
    for file in file_subset:
        extract_esm_features(file, model, batch_converter, device)

    # Model unloaded when function returns

# PERSISTENT PATTERN: Load model once during pool init
_model = None
_batch_converter = None

def init_worker(device_id):
    global _model, _batch_converter
    device = torch.device(f'cuda:{device_id}')

    # Model loading overhead: ONCE per worker lifetime
    _model, _batch_converter = load_esm2_model(
        model_name="esm2_t36_3B_UR50D",
        device=str(device)
    )

def process_esm_files_persistent(file_subset, device_id, **kwargs):
    """Persistent: Reuse pre-loaded model."""
    global _model, _batch_converter

    # No model loading - immediate processing
    for file in file_subset:
        extract_esm_features(file, _model, _batch_converter, _model.device)

    # Model stays loaded for next task

# Pool with persistent workers
pool = mp.Pool(
    num_workers,
    initializer=init_worker,
    initargs=(device_id,),
    maxtasksperchild=None  # Key difference
)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Load model per request | Persistent workers with model caching | TorchServe 2020+ | Eliminates per-request overhead |
| maxtasksperchild=1 default | maxtasksperchild=None for inference | Python 3.2+ | Enables persistent workers |
| Manual memory mgmt | expandable_segments config | PyTorch 2.2 (2024) | Mitigates fragmentation in long-running workers |
| persistent_workers in DataLoader | Standard for training | PyTorch 1.9 (2021) | Reduces epoch startup overhead |

**Deprecated/outdated:**
- Fork start method with CUDA: Deprecated since PyTorch 1.9, use spawn or forkserver
- Loading models in parent before spawn: Causes CUDA context issues, load in worker initializer
- Ignoring fragmentation: Modern workloads need expandable_segments or periodic cache clearing

## Open Questions

Things that couldn't be fully resolved:

1. **Optimal cache clearing interval**
   - What we know: Should be periodic (not every batch), typical values 10-100 batches
   - What's unclear: Optimal value depends on workload (batch size variability, sequence lengths)
   - Recommendation: Start with interval=10, monitor GPU memory over time, increase if overhead detected

2. **maxtasksperchild=None vs maxtasksperchild=N tradeoff**
   - What we know: None = persistent (no reload), N = periodic refresh (prevents fragmentation)
   - What's unclear: Best N value for ESM-2 3B workload balancing reload overhead vs fragmentation
   - Recommendation: Start with None + expandable_segments, fall back to N=50-100 if fragmentation persists

3. **Current job structure: single job or multiple jobs?**
   - What we know: Phase context shows "re-loading overhead between jobs" but unclear if pipeline processes multiple independent jobs per run
   - What's unclear: Does `virnucpro predict` process one sample (single job) or multiple samples (multiple jobs)?
   - Recommendation: If single job: benefit only if workers process file subsets sequentially. If multiple jobs: major benefit from persistent models

4. **Model initialization overhead magnitude**
   - What we know: ESM-2 3B is large, loading involves downloading weights + GPU transfer
   - What's unclear: Actual timing (2 min? 5 min? 10 min?) for ESM-2 3B and DNABERT-S on target hardware
   - Recommendation: Benchmark model loading time to quantify potential savings

## Sources

### Primary (HIGH confidence)
- [Python multiprocessing documentation](https://docs.python.org/3/library/multiprocessing.html) - Pool initializer, maxtasksperchild, spawn context
- [PyTorch multiprocessing best practices](https://docs.pytorch.org/docs/stable/notes/multiprocessing.html) - CUDA context, spawn requirements, tensor sharing
- [PyTorch CUDA memory management](https://docs.pytorch.org/docs/stable/notes/cuda.html) - Caching allocator, expandable segments
- [Super Fast Python: Pool Initializer](https://superfastpython.com/multiprocessing-pool-initializer/) - Worker initialization patterns
- [Super Fast Python: maxtasksperchild](https://superfastpython.com/multiprocessing-pool-max-tasks-per-child-in-python/) - Worker lifecycle control

### Secondary (MEDIUM confidence)
- [PyTorch CUDA Caching Allocator Guide](https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html) - Fragmentation causes and mitigation
- [PyTorch Forums: CUDA memory fragmentation](https://discuss.pytorch.org/t/mitigating-cuda-gpu-memory-fragmentation-and-oom-issues/108203) - Real-world fragmentation solutions
- [TorchServe Batch Inference](https://docs.pytorch.org/serve/batch_inference_with_ts.html) - Production model serving patterns
- [AWS ML Blog: TorchServe at scale](https://aws.amazon.com/blogs/machine-learning/deploying-pytorch-models-for-inference-at-scale-using-torchserve/) - Batch job queue patterns
- [PyTorch Lightning: Persistent Workers](https://lightning.ai/docs/pytorch/stable/advanced/speed.html) - Training-focused but applicable patterns

### Tertiary (LOW confidence - needs validation)
- [Medium: Multiprocessing in PyTorch 2026](https://thelinuxcode.com/multiprocessing-in-python-and-pytorch-faster-training-safer-pipelines-realworld-patterns/) - Recent best practices, needs verification against official docs
- [GitHub Issues: CUDA memory leaks](https://github.com/pytorch/pytorch/issues/44156) - Community-reported issues, not all resolved

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Based on official Python and PyTorch documentation
- Architecture patterns: HIGH - Verified against Python multiprocessing docs and PyTorch best practices
- Memory management: HIGH - Based on official PyTorch CUDA documentation and expandable_segments feature
- Pitfalls: MEDIUM-HIGH - Combination of official docs and verified community reports

**Research date:** 2026-01-24
**Valid until:** 30 days (Python multiprocessing is stable, PyTorch CUDA APIs evolve slowly)

**Key unknowns requiring investigation:**
1. Current job structure (single vs multiple jobs per pipeline run)
2. Model loading overhead magnitude (benchmark needed)
3. Optimal cache clearing interval for this workload
4. Whether current architecture already processes multiple file subsets per worker (check BatchQueueManager usage)
