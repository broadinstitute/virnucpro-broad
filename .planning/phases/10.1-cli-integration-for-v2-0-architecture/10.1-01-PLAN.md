---
phase: 10.1-cli-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/cli/predict.py
  - tests/unit/test_cli_predict.py
autonomous: true

must_haves:
  truths:
    - "python -m virnucpro predict --parallel calls run_multi_gpu_inference for ESM-2 embedding stage"
    - "python -m virnucpro predict --parallel calls run_multi_gpu_inference for DNABERT-S embedding stage"
    - "python -m virnucpro predict --v1-fallback uses v1.0 pipeline even when --parallel is set"
    - "CLI logs which architecture version is active (v1.0 or v2.0)"
    - "RuntimeConfig is constructed from CLI flags (--resume, --force-resume) and passed to v2.0 API"
    - "CLI integration tests verify v2.0 routing without GPU"
  artifacts:
    - path: "virnucpro/cli/predict.py"
      provides: "Updated predict command routing to v2.0 API"
      contains: "run_multi_gpu_inference"
    - path: "tests/unit/test_cli_predict.py"
      provides: "CLI integration tests for v2.0 routing"
      min_lines: 80
  key_links:
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/multi_gpu_inference.py"
      via: "import and call run_multi_gpu_inference"
      pattern: "from virnucpro\\.pipeline\\.multi_gpu_inference import run_multi_gpu_inference"
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/runtime_config.py"
      via: "construct RuntimeConfig from CLI flags"
      pattern: "RuntimeConfig"
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/prediction.py"
      via: "v1.0 fallback still calls run_prediction"
      pattern: "run_prediction"
---

<objective>
Update the predict CLI command to route `--parallel` through v2.0 async architecture (run_multi_gpu_inference) instead of v1.0 multi-worker code. Add `--v1-fallback` flag for backward compatibility. Write CLI integration tests verifying routing.

Purpose: This is the critical blocker for Phase 10. Without this change, `python -m virnucpro predict --parallel` still uses v1.0 workers (parallel_esm.py, parallel_dnabert.py), and Phase 10 Plan 05 (v1.0 vs v2.0 comparison) would accidentally compare v1.0 to v1.0.

Output: Updated `virnucpro/cli/predict.py` with v2.0 routing, `tests/unit/test_cli_predict.py` with integration tests.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10.1-cli-integration-for-v2-0-architecture/10.1-RESEARCH.md
@virnucpro/cli/predict.py
@virnucpro/cli/main.py
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/runtime_config.py
@virnucpro/pipeline/prediction.py
@virnucpro/pipeline/gpu_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update predict.py to route --parallel to v2.0 API</name>
  <files>virnucpro/cli/predict.py</files>
  <action>
Modify `virnucpro/cli/predict.py` to route the `--parallel` flag through v2.0 architecture:

1. **Add `--v1-fallback` CLI option** (after the `--parallel` option, around line 58):
   ```python
   @click.option('--v1-fallback',
                 is_flag=True,
                 help='Use v1.0 multi-worker architecture instead of v2.0 async DataLoader (deprecated)')
   ```
   Add `v1_fallback` to the function signature.

2. **Replace the single `run_prediction()` call** (lines 256-287) with conditional routing logic. The key change: when `parallel=True` and `v1_fallback=False`, call `run_multi_gpu_inference()` for BOTH the ESM-2 and DNABERT-S embedding stages, then continue with the rest of the pipeline (merging, prediction, consensus) via `run_prediction()` with `skip_embedding=True`.

   However, looking at `run_prediction()`, it orchestrates the entire 9-stage pipeline (chunking, translation, splitting, DNABERT-S, ESM-2, merging, prediction, consensus). The v2.0 API (`run_multi_gpu_inference`) only handles one model at a time (ESM-2 OR DNABERT-S per call). The simplest correct approach: modify `prediction.py` indirectly by changing what `predict.py` passes, OR refactor the routing.

   **Simplest correct approach:** Replace the entire try block body (lines 249-295) with:

   ```python
   # Determine architecture version
   use_v2 = parallel and not v1_fallback

   if use_v2:
       logger.info("Architecture: v2.0 (async DataLoader + sequence packing)")
       logger.info("  Features: FP16, FlashAttention varlen, stride-based sharding")

       from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference
       from virnucpro.pipeline.runtime_config import RuntimeConfig

       # Build RuntimeConfig from CLI flags
       runtime_config = RuntimeConfig(
           enable_checkpointing=resume or force_resume,
           checkpoint_dir=output_dir / '.checkpoints',
           force_restart=force_resume,
       )

       # Run full pipeline with v2.0 embedding stages
       # prediction.py still handles chunking, translation, splitting, merging, prediction, consensus
       # but embedding stages (DNABERT-S + ESM-2) route through v2.0
       from virnucpro.pipeline.prediction import run_prediction
       exit_code = run_prediction(
           input_file=Path(input_file),
           model_path=Path(model_path),
           expected_length=expected_length,
           output_dir=output_dir,
           device=device_obj,
           dnabert_batch_size=dnabert_batch_size,
           parallel=parallel,
           batch_size=batch_size,
           num_workers=num_workers,
           cleanup_intermediate=cleanup,
           resume=resume,
           show_progress=not no_progress,
           config=config,
           toks_per_batch=esm_batch_size,
           translation_threads=threads,
           merge_threads=threads,
           quiet=not verbose,
           gpus=gpus,
           skip_checkpoint_validation=skip_checkpoint_validation,
           force_resume=force_resume,
           dataloader_workers=dataloader_workers,
           pin_memory=pin_memory,
           expandable_segments=expandable_segments,
           cache_clear_interval=cache_clear_interval,
           cuda_streams=cuda_streams,
           persistent_models=persistent_models,
           use_v2_architecture=True,   # NEW PARAMETER
           runtime_config=runtime_config,  # NEW PARAMETER
       )
   else:
       if v1_fallback:
           logger.info("Architecture: v1.0 (multi-worker-per-GPU, --v1-fallback)")
       else:
           logger.info("Architecture: v1.0 (single-GPU or non-parallel mode)")

       from virnucpro.pipeline.prediction import run_prediction
       exit_code = run_prediction(
           input_file=Path(input_file),
           ...same params as current code, no new params...
       )
   ```

   Wait -- this approach requires modifying `prediction.py` to accept `use_v2_architecture` and `runtime_config`, which adds a second file to this task. The cleaner approach that avoids touching prediction.py:

   **Cleanest approach:** In `predict.py`, when `use_v2=True`, still call `run_prediction()` with the same parameters BUT also set environment variable `VIRNUCPRO_USE_V2=1` that `prediction.py` can check internally. However, this is fragile.

   **Best approach (matching existing pattern in codebase):** Add `use_v2_architecture` and `runtime_config` parameters to `run_prediction()` in `prediction.py`. In the DNABERT-S and ESM-2 stages, when `use_v2_architecture=True`, call `run_multi_gpu_inference()` instead of the v1.0 workers. This is the surgically correct change.

   **In predict.py:**
   - Add import at top: `import os` (already present conditionally)
   - After the configuration logging block (line 246), add architecture version detection and logging
   - Modify the `run_prediction()` call to include two new kwargs:
     - `use_v2_architecture=use_v2` (True when parallel and not v1_fallback)
     - `runtime_config=runtime_config` (RuntimeConfig object, only when use_v2)
   - Construct `runtime_config` only when `use_v2=True`:
     ```python
     runtime_config = None
     if use_v2:
         from virnucpro.pipeline.runtime_config import RuntimeConfig
         runtime_config = RuntimeConfig(
             enable_checkpointing=resume or force_resume,
             checkpoint_dir=output_dir / '.checkpoints',
             force_restart=force_resume,
         )
     ```

   **In prediction.py (ALSO MODIFIED by this task since they are tightly coupled):**
   - Add `use_v2_architecture: bool = False` and `runtime_config: Optional['RuntimeConfig'] = None` to `run_prediction()` signature
   - In Stage 5 (DNABERT-S, line 367), when `use_v2_architecture and parallel`:
     ```python
     if use_v2_architecture and parallel:
         logger.info("Using v2.0 async architecture for DNABERT-S")
         from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference
         dnabert_model_config = {
             'model_type': 'dnabert',
             'enable_fp16': True,
         }
         dnabert_output_dir = output_dir / 'dnabert_v2'
         dnabert_output, dnabert_failed = run_multi_gpu_inference(
             fasta_files=nucleotide_files,
             output_dir=dnabert_output_dir,
             model_config=dnabert_model_config,
             runtime_config=runtime_config,
         )
         if dnabert_failed:
             logger.warning(f"v2.0 DNABERT-S: {len(dnabert_failed)} workers failed")
         # v2.0 produces embeddings.h5, need to extract per-file feature tensors
         # Store the output path for merging stage
         nucleotide_feature_files = [dnabert_output]
     ```
   - In Stage 6 (ESM-2, line 562), when `use_v2_architecture and parallel`:
     ```python
     if use_v2_architecture and parallel:
         logger.info("Using v2.0 async architecture for ESM-2")
         from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference
         esm_model_config = {
             'model_type': 'esm2',
             'model_name': 'esm2_t36_3B_UR50D',
             'enable_fp16': True,
         }
         esm_output_dir = output_dir / 'esm_v2'
         esm_output, esm_failed = run_multi_gpu_inference(
             fasta_files=protein_files,
             output_dir=esm_output_dir,
             model_config=esm_model_config,
             runtime_config=runtime_config,
         )
         if esm_failed:
             logger.warning(f"v2.0 ESM-2: {len(esm_failed)} workers failed")
         protein_feature_files = [esm_output]
         failed_files = [(f"rank_{r}", "worker failed") for r in esm_failed]
     ```

   **IMPORTANT NOTES:**
   - The v2.0 API produces a single merged `embeddings.h5`, not per-file `.pt` files like v1.0. The downstream merging stage (Stage 7) expects per-file DNABERT and ESM feature files. This means the v2.0 output format (HDF5 with all embeddings) is different from v1.0 output format (.pt per file). For Phase 10.1, the integration point is at the embedding stage level -- the rest of the pipeline (merging, prediction, consensus) still uses v1.0 patterns. The v2.0 embeddings.h5 file contains sequence_ids and embeddings that need to be converted to the .pt format expected by downstream stages, OR the downstream stages need to be updated.

   **REVISED SIMPLEST APPROACH:** Since the v2.0 pipeline produces HDF5 output while the v1.0 pipeline uses .pt files, and the merge/prediction/consensus stages expect .pt files, the cleanest Phase 10.1 integration is:
   - In `predict.py`: detect `use_v2` and log it
   - In `prediction.py`: for embedding stages only, when `use_v2_architecture=True`, call `run_multi_gpu_inference()` and then convert the HDF5 output to per-file .pt format to maintain compatibility with downstream stages
   - Add an `_h5_to_feature_tensors()` helper function that reads the embeddings.h5 and groups by original file, saving per-file .pt tensors

   Actually -- reviewing the pipeline more carefully, the merge stage merges DNABERT and ESM features per-sequence into combined tensors for prediction. The v2.0 architecture processes one model type at a time. The correct approach:

   **FINAL APPROACH (keeping it simple):**
   In `predict.py`, add `--v1-fallback` flag and routing logic:
   - When `parallel=True` and `v1_fallback=False`: set `use_v2=True`
   - Pass `use_v2_architecture=use_v2` and `runtime_config` to `run_prediction()`

   In `prediction.py`, add the two new parameters and modify only the ESM-2 and DNABERT-S stages:
   - When `use_v2_architecture` is True, replace the v1.0 multi-worker code blocks in Stage 5 and Stage 6 with calls to `run_multi_gpu_inference()`
   - After `run_multi_gpu_inference()` returns the merged HDF5, read it with h5py and save per-sequence embeddings as .pt files to match the expected format for the merge stage
   - This is a thin adapter that converts v2.0 HDF5 output to v1.0 .pt format

   Specifically:
   ```python
   def _convert_h5_to_pt_files(h5_path: Path, output_dir: Path, suffix: str) -> List[Path]:
       """Convert v2.0 HDF5 embeddings to v1.0 per-file .pt format."""
       import h5py
       import torch

       output_dir.mkdir(parents=True, exist_ok=True)
       pt_files = []

       with h5py.File(h5_path, 'r') as f:
           ids = [s.decode() if isinstance(s, bytes) else s for s in f['sequence_ids'][:]]
           embeddings = f['embeddings'][:]

       # Group embeddings and save as single .pt file (same format as v1.0)
       results = {}
       for seq_id, emb in zip(ids, embeddings):
           results[seq_id] = torch.tensor(emb)

       output_file = output_dir / f"all_sequences_{suffix}.pt"
       torch.save(results, output_file)
       pt_files.append(output_file)

       return pt_files
   ```

   Add this helper to prediction.py. In the DNABERT-S and ESM-2 v2.0 branches, call `run_multi_gpu_inference()` then `_convert_h5_to_pt_files()`.

   **FILES MODIFIED BY THIS TASK:**
   - `virnucpro/cli/predict.py` -- add `--v1-fallback`, routing logic, RuntimeConfig construction
   - `virnucpro/pipeline/prediction.py` -- add `use_v2_architecture` and `runtime_config` params, v2.0 branches in Stage 5/6, `_convert_h5_to_pt_files()` helper
  </action>
  <verify>
    python -c "from virnucpro.cli.predict import predict; print('predict imports OK')"
    python -c "from virnucpro.pipeline.prediction import run_prediction; import inspect; sig = inspect.signature(run_prediction); assert 'use_v2_architecture' in sig.parameters; print('use_v2_architecture param OK')"
  </verify>
  <done>
    `predict.py` has `--v1-fallback` flag. When `--parallel` is set without `--v1-fallback`, `use_v2_architecture=True` is passed to `run_prediction()`. `prediction.py` Stage 5 and Stage 6 call `run_multi_gpu_inference()` when v2.0 is active, with HDF5-to-PT adapter for downstream compatibility. RuntimeConfig is constructed from CLI flags (--resume, --force-resume). Architecture version is logged clearly.
  </done>
</task>

<task type="auto">
  <name>Task 2: CLI integration tests for v2.0 routing</name>
  <files>tests/unit/test_cli_predict.py</files>
  <action>
Create `tests/unit/test_cli_predict.py` with integration tests verifying the v2.0 routing logic:

Module docstring: "CLI integration tests for predict command v2.0 routing. Verifies that --parallel routes to v2.0 architecture (run_multi_gpu_inference) and --v1-fallback routes to v1.0 (run_prediction without v2). Uses mocking to avoid GPU/model dependencies."

1. **test_parallel_routes_to_v2** (default behavior):
   - Use `@patch('virnucpro.pipeline.prediction.run_prediction')` to mock run_prediction
   - Mock return value: 0 (success)
   - Use CliRunner to invoke: `cli ['predict', str(fasta_path), '--parallel', '--output-dir', str(tmp_path)]`
   - Need to also mock `validate_and_get_device`, `detect_cuda_devices` (return [0, 1]), and `Config.load`
   - Assert `run_prediction` was called with `use_v2_architecture=True`
   - Assert `run_prediction` was called with `runtime_config` that is a RuntimeConfig instance

2. **test_v1_fallback_routes_to_v1**:
   - Same mocking setup as above
   - Invoke with `['predict', str(fasta_path), '--parallel', '--v1-fallback', '--output-dir', str(tmp_path)]`
   - Assert `run_prediction` was called with `use_v2_architecture=False` (or without the parameter since default is False)

3. **test_single_gpu_routes_to_v1**:
   - Mock `detect_cuda_devices` to return [0] (single GPU)
   - Invoke WITHOUT `--parallel`: `['predict', str(fasta_path), '--output-dir', str(tmp_path)]`
   - Assert `run_prediction` was called without `use_v2_architecture=True`

4. **test_parallel_constructs_runtime_config**:
   - Mock run_prediction
   - Invoke with `['predict', str(fasta_path), '--parallel', '--resume', '--output-dir', str(tmp_path)]`
   - Capture the runtime_config argument passed to run_prediction
   - Assert `runtime_config.enable_checkpointing is True`

5. **test_parallel_force_resume_sets_config**:
   - Mock run_prediction
   - Invoke with `['predict', str(fasta_path), '--parallel', '--force-resume', '--output-dir', str(tmp_path)]`
   - Capture runtime_config
   - Assert `runtime_config.force_restart is True`
   - Assert `runtime_config.enable_checkpointing is True`

6. **test_v2_architecture_logged**:
   - Mock run_prediction
   - Invoke with `--parallel`
   - Check that result.output contains "v2.0" or "async" (architecture version logged)

**Fixtures:**
- `tmp_fasta`: Create a minimal FASTA file in tmp_path for CLI invocation
- Use `@pytest.fixture` for common mocking setup

**Imports:**
- pytest, Path from pathlib
- CliRunner from click.testing
- cli from virnucpro.cli.main
- patch from unittest.mock
- RuntimeConfig from virnucpro.pipeline.runtime_config

**Important:** All tests run on CPU without GPU. Use mocking for:
- `virnucpro.pipeline.prediction.run_prediction` -- mock to return 0
- `virnucpro.cli.predict.validate_and_get_device` -- mock to return torch.device('cpu')
- `virnucpro.cli.predict.detect_cuda_devices` -- mock to return [0, 1]
- `virnucpro.cli.predict.Config` -- mock config with sensible defaults via MagicMock with `get` returning appropriate defaults

**Test naming convention:** `test_{what}_{condition}_{expected_outcome}` per project convention.
  </action>
  <verify>
    pytest tests/unit/test_cli_predict.py -v --tb=short 2>&1 | tail -20
  </verify>
  <done>
    6 CLI tests pass. Tests verify: --parallel routes to v2.0, --v1-fallback routes to v1.0, single-GPU stays v1.0, RuntimeConfig constructed from --resume and --force-resume flags, architecture version logged. All tests run on CPU with mocking.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.cli.predict import predict; print('CLI imports OK')"`
- `pytest tests/unit/test_cli_predict.py -v --tb=short`
- `python -m virnucpro predict --help` shows `--v1-fallback` option
- Grep: `grep -n "run_multi_gpu_inference\|use_v2_architecture\|v1.fallback" virnucpro/cli/predict.py virnucpro/pipeline/prediction.py`
</verification>

<success_criteria>
`predict.py` routes `--parallel` to v2.0 architecture by default (run_multi_gpu_inference for embedding stages). `--v1-fallback` preserves v1.0 behavior. RuntimeConfig constructed from CLI flags. Architecture version logged. 6 CLI integration tests pass on CPU. HDF5-to-PT adapter ensures downstream pipeline stages work with v2.0 embeddings.
</success_criteria>

<output>
After completion, create `.planning/phases/10.1-cli-integration-for-v2-0-architecture/10.1-01-SUMMARY.md`
</output>
