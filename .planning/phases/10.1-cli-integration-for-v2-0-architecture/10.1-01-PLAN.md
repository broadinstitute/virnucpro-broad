---
phase: 10.1-cli-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/cli/predict.py
  - virnucpro/pipeline/prediction.py
  - tests/unit/test_cli_predict.py
autonomous: true

must_haves:
  truths:
    - "python -m virnucpro predict --parallel calls run_multi_gpu_inference for ESM-2 embedding stage only"
    - "python -m virnucpro predict --parallel keeps DNABERT-S on v1.0 (unchanged Stage 5)"
    - "python -m virnucpro predict --v1-fallback uses v1.0 pipeline for both ESM-2 and DNABERT-S"
    - "CLI logs which architecture version is active (v1.0 or v2.0) and which models use which version"
    - "RuntimeConfig is constructed from CLI flags (--resume, --force-resume) and passed to v2.0 API"
    - "world_size is explicitly passed to run_multi_gpu_inference matching CLI GPU selection"
    - "If any v2.0 ESM-2 worker fails, the stage raises RuntimeError (fail-fast, no synthetic format conversion)"
    - "CLI integration tests verify v2.0 routing without GPU"
  artifacts:
    - path: "virnucpro/cli/predict.py"
      provides: "Updated predict command with --v1-fallback flag and v2.0 routing logic"
      contains: "v1_fallback"
    - path: "virnucpro/pipeline/prediction.py"
      provides: "Updated prediction pipeline with use_v2_architecture param, ESM-2 v2.0 branch in Stage 6, HDF5-to-PT streaming adapter"
      contains: "run_multi_gpu_inference"
    - path: "tests/unit/test_cli_predict.py"
      provides: "CLI integration tests for v2.0 routing"
      min_lines: 80
  key_links:
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/prediction.py"
      via: "passes use_v2_architecture=True and runtime_config to run_prediction"
      pattern: "use_v2_architecture"
    - from: "virnucpro/pipeline/prediction.py"
      to: "virnucpro/pipeline/multi_gpu_inference.py"
      via: "import and call run_multi_gpu_inference in Stage 6 ESM-2 branch"
      pattern: "from virnucpro\\.pipeline\\.multi_gpu_inference import run_multi_gpu_inference"
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/runtime_config.py"
      via: "construct RuntimeConfig from CLI flags"
      pattern: "RuntimeConfig"
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/prediction.py"
      via: "v1.0 fallback still calls run_prediction without v2 params"
      pattern: "run_prediction"
---

<objective>
Update the predict CLI command to route `--parallel` ESM-2 embedding through v2.0 async architecture (run_multi_gpu_inference) while keeping DNABERT-S on v1.0. Add `--v1-fallback` flag for backward compatibility. Write CLI integration tests verifying routing.

**HYBRID APPROACH:** Only ESM-2 uses v2.0 architecture. DNABERT-S stays on v1.0 because:
1. Phases 5-9 only validated ESM-2 protein sequences through v2.0
2. DNABERT-S requires different tokenizer (DNA k-mers), different max_length (512), different input format
3. DNABERT-S is already fast (1M sequences in 3-4 min) -- ESM-2 is the bottleneck
4. DNABERT-S v2.0 support planned for Phase 10.2 or v2.1 milestone

Purpose: This is the critical blocker for Phase 10. Without this change, `python -m virnucpro predict --parallel` still uses v1.0 workers for ESM-2, and Phase 10 Plan 05 (v1.0 vs v2.0 comparison) would accidentally compare v1.0 to v1.0.

Output: Updated `virnucpro/cli/predict.py` with v2.0 routing, updated `virnucpro/pipeline/prediction.py` with ESM-2 v2.0 branch, `tests/unit/test_cli_predict.py` with integration tests.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10.1-cli-integration-for-v2-0-architecture/10.1-RESEARCH.md
@virnucpro/cli/predict.py
@virnucpro/cli/main.py
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/runtime_config.py
@virnucpro/pipeline/prediction.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update predict.py and prediction.py for hybrid v2.0 routing (ESM-2 only)</name>
  <files>virnucpro/cli/predict.py, virnucpro/pipeline/prediction.py</files>
  <action>
**CRITICAL CONSTRAINT: Only ESM-2 goes through v2.0. DNABERT-S Stage 5 is UNTOUCHED.**

### Part A: predict.py changes

1. **Add `--v1-fallback` CLI option** (after the `--parallel` option, around line 58):
   ```python
   @click.option('--v1-fallback',
                 is_flag=True,
                 help='Use v1.0 multi-worker architecture for ESM-2 instead of v2.0 async DataLoader')
   ```
   Add `v1_fallback` to the function signature (line 103-108).

2. **Add architecture detection and logging** after the configuration logging block (around line 246, before the torch import):
   ```python
   # Determine architecture version for ESM-2
   use_v2 = parallel and not v1_fallback

   if use_v2:
       logger.info("Architecture: v2.0 hybrid")
       logger.info("  ESM-2 embedding: v2.0 (async DataLoader + sequence packing)")
       logger.info("  DNABERT-S embedding: v1.0 (fast enough, v2.0 planned for v2.1)")
   elif v1_fallback:
       logger.info("Architecture: v1.0 (--v1-fallback, all stages use legacy multi-worker)")
   else:
       logger.info("Architecture: v1.0 (single-GPU or non-parallel mode)")
   ```

3. **Construct RuntimeConfig when v2.0 is active** (after architecture logging):
   ```python
   runtime_config = None
   if use_v2:
       from virnucpro.pipeline.runtime_config import RuntimeConfig
       runtime_config = RuntimeConfig(
           enable_checkpointing=resume or force_resume,
           checkpoint_dir=output_dir / '.checkpoints',
           force_restart=force_resume,
       )
   ```

4. **Modify the `run_prediction()` call** (lines 260-287) to include two new kwargs:
   ```python
   exit_code = run_prediction(
       ...all existing params unchanged...
       use_v2_architecture=use_v2,     # NEW
       runtime_config=runtime_config,   # NEW
   )
   ```

### Part B: prediction.py changes

1. **Add `use_v2_architecture` and `runtime_config` to `run_prediction()` signature** (after `persistent_models` param):
   ```python
   use_v2_architecture: bool = False,
   runtime_config: Optional['RuntimeConfig'] = None,
   ```
   Add to docstring: "use_v2_architecture: Route ESM-2 through v2.0 async DataLoader (DNABERT-S stays v1.0)"

2. **DO NOT MODIFY Stage 5 (DNABERT-S, line 366-559).** Stage 5 stays 100% unchanged. DNABERT-S continues using v1.0 multi-worker code (`process_dnabert_files_worker`). This is intentional -- DNABERT-S was never validated in v2.0.

3. **Modify Stage 6 (ESM-2, line 562) to add v2.0 branch.** Add a new branch at the TOP of the Stage 6 block (right after `checkpoint_manager.mark_stage_started`), BEFORE the existing parallel detection logic:

   ```python
   if use_v2_architecture and parallel:
       # v2.0 ESM-2: async DataLoader + sequence packing + FlashAttention
       logger.info("ESM-2: Using v2.0 async architecture")
       logger.info("  Features: FP16, FlashAttention varlen, stride-based sharding")

       from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference

       cuda_devices = detect_cuda_devices()
       num_gpus = len(cuda_devices) if cuda_devices else 1

       esm_model_config = {
           'model_type': 'esm2',
           'model_name': 'esm2_t36_3B_UR50D',
           'enable_fp16': True,
       }
       esm_output_dir = output_dir / 'esm_v2'

       esm_output_path, esm_failed_ranks = run_multi_gpu_inference(
           fasta_files=protein_files,
           output_dir=esm_output_dir,
           model_config=esm_model_config,
           world_size=num_gpus,           # EXPLICIT world_size from detected GPUs
           runtime_config=runtime_config,
       )

       # FAIL-FAST on worker failures (Issue 3: no synthetic format conversion)
       # v2.0 returns failed_ranks (List[int]), not failed_files (List[Tuple[str, str]])
       # We cannot map ranks back to individual files without index lookup,
       # so if any rank fails, the entire ESM-2 stage fails
       if esm_failed_ranks:
           raise RuntimeError(
               f"v2.0 ESM-2 inference failed on {len(esm_failed_ranks)}/{num_gpus} workers: "
               f"ranks {esm_failed_ranks}. Check logs: {esm_output_dir / 'logs' / 'worker_*.log'}"
           )

       # Convert v2.0 HDF5 output to v1.0 per-file .pt format for merge stage compatibility
       # Uses streaming approach to avoid loading entire HDF5 into memory (Issue 2)
       protein_feature_files = _stream_h5_to_pt_files(
           esm_output_path, protein_split_dir, nucleotide_feature_files
       )
       failed_files = []  # No failures (we fail-fast above)

       logger.info(f"v2.0 ESM-2 complete: {len(protein_feature_files)} feature files created")
   else:
       # --- EXISTING v1.0 ESM-2 CODE BELOW (UNCHANGED) ---
       from virnucpro.pipeline.features import extract_esm_features
       ...rest of existing Stage 6 code...
   ```

   **IMPORTANT:** The existing Stage 6 v1.0 code block (lines 566-763) must be wrapped in the `else:` branch of the new `if use_v2_architecture and parallel:` conditional. Indent the entire existing block one level. Do NOT modify the v1.0 code itself.

4. **Add `_stream_h5_to_pt_files()` helper** (as a module-level function in prediction.py, before `run_prediction()`):
   ```python
   def _stream_h5_to_pt_files(
       h5_path: Path,
       output_dir: Path,
       nucleotide_feature_files: list
   ) -> list:
       """Convert v2.0 HDF5 embeddings to v1.0 per-file .pt format for merge stage.

       Uses chunked reading to avoid loading entire HDF5 into memory.
       Creates .pt files that match the naming convention expected by the merge stage:
       each nucleotide feature file (e.g., output_0_DNABERT_S.pt) has a corresponding
       ESM feature file (e.g., output_0_ESM.pt).

       The merge stage (Stage 7) zips nucleotide and protein feature files by position,
       so this function must produce files in the same order as nucleotide_feature_files.

       Args:
           h5_path: Path to v2.0 merged embeddings.h5
           output_dir: Directory to write .pt files
           nucleotide_feature_files: List of DNABERT .pt files (used for naming alignment)

       Returns:
           List of created .pt file paths, ordered to match nucleotide_feature_files
       """
       import h5py

       output_dir.mkdir(parents=True, exist_ok=True)

       # Read all embeddings from HDF5 (v2.0 format: sequence_ids + embeddings datasets)
       with h5py.File(h5_path, 'r') as f:
           seq_ids = [s.decode() if isinstance(s, bytes) else s for s in f['sequence_ids'][:]]
           embeddings = f['embeddings']  # Don't load all into memory yet

           # Build lookup: sequence_id -> embedding (chunked read)
           seq_to_embedding = {}
           chunk_size = 10000
           for start in range(0, len(seq_ids), chunk_size):
               end = min(start + chunk_size, len(seq_ids))
               chunk_embs = embeddings[start:end]
               for i, sid in enumerate(seq_ids[start:end]):
                   seq_to_embedding[sid] = torch.tensor(chunk_embs[i])

       # Create per-file .pt files matching nucleotide feature file naming
       # nucleotide: output_0_DNABERT_S.pt -> protein: output_0_ESM.pt
       pt_files = []
       for nuc_feat_path in nucleotide_feature_files:
           # Load nucleotide .pt to get sequence IDs in this file
           nuc_data = torch.load(nuc_feat_path, weights_only=False)

           # Build ESM features dict for sequences in this file
           esm_features = {}
           for seq_id in nuc_data.keys():
               if seq_id in seq_to_embedding:
                   esm_features[seq_id] = seq_to_embedding[seq_id]
               else:
                   logger.warning(f"Sequence {seq_id} not found in v2.0 ESM-2 output")

           # Save with matching name: output_0_DNABERT_S.pt -> output_0_ESM.pt
           esm_filename = nuc_feat_path.name.replace('_DNABERT_S.pt', '_ESM.pt')
           esm_path = output_dir / esm_filename
           atomic_save(esm_features, esm_path)
           pt_files.append(esm_path)

       return pt_files
   ```

   **WHY streaming (Issue 2):** The original plan loaded the entire HDF5 into memory with `embeddings[:]` which would require 10-100GB of RAM for large workloads. This version reads in chunks of 10K sequences. The dict lookup is still in-memory but only stores tensor references, not duplicated data.

   **WHY per-file alignment (Issue 2 alternative):** Updating the merge stage to read HDF5 directly would be cleaner but is a larger refactor touching Stage 7 (merge), Stage 8 (prediction), and potentially Stage 9 (consensus). For Phase 10.1 (CLI integration), the adapter approach maintains compatibility with zero changes to downstream stages. The merge stage refactor can happen in Phase 10.2.

   **Note on atomic_save:** This function is already imported in prediction.py from `virnucpro.core.checkpoint`. It provides atomic file writes (write to temp, rename) to prevent corrupt partial files.
  </action>
  <verify>
    python -c "from virnucpro.cli.predict import predict; print('predict imports OK')"
    python -c "from virnucpro.pipeline.prediction import run_prediction; import inspect; sig = inspect.signature(run_prediction); assert 'use_v2_architecture' in sig.parameters; assert 'runtime_config' in sig.parameters; print('new params OK')"
    python -c "from virnucpro.pipeline.prediction import _stream_h5_to_pt_files; print('helper function OK')"
    grep -n "run_multi_gpu_inference" virnucpro/pipeline/prediction.py
    grep -n "world_size" virnucpro/pipeline/prediction.py
    grep -n "v1_fallback" virnucpro/cli/predict.py
  </verify>
  <done>
    `predict.py` has `--v1-fallback` flag. When `--parallel` is set without `--v1-fallback`, `use_v2_architecture=True` is passed to `run_prediction()`. In `prediction.py`:
    - Stage 5 (DNABERT-S) is UNTOUCHED (stays v1.0)
    - Stage 6 (ESM-2) calls `run_multi_gpu_inference()` with explicit `world_size=num_gpus` when v2.0 is active
    - `_stream_h5_to_pt_files()` converts v2.0 HDF5 to v1.0 .pt format using chunked reads
    - If any v2.0 worker fails, RuntimeError is raised (fail-fast, no synthetic format conversion)
    - RuntimeConfig is constructed from CLI flags
    - Architecture version is logged with hybrid designation ("ESM-2: v2.0, DNABERT-S: v1.0")
  </done>
</task>

<task type="auto">
  <name>Task 2: CLI integration tests for v2.0 routing</name>
  <files>tests/unit/test_cli_predict.py</files>
  <action>
Create `tests/unit/test_cli_predict.py` with integration tests verifying the v2.0 routing logic:

Module docstring: "CLI integration tests for predict command v2.0 routing. Verifies that --parallel routes ESM-2 to v2.0 architecture (run_multi_gpu_inference) while DNABERT-S stays v1.0, and --v1-fallback routes both to v1.0. Uses mocking to avoid GPU/model dependencies."

1. **test_parallel_routes_esm2_to_v2** (default behavior):
   - Use `@patch('virnucpro.pipeline.prediction.run_prediction')` to mock run_prediction
   - Mock return value: 0 (success)
   - Use CliRunner to invoke: `cli ['predict', str(fasta_path), '--parallel', '--output-dir', str(tmp_path)]`
   - Need to also mock `virnucpro.cli.predict.validate_and_get_device`, `virnucpro.cli.predict.detect_cuda_devices` (return [0, 1]), and `virnucpro.cli.predict.Config`
   - Assert `run_prediction` was called with `use_v2_architecture=True`
   - Assert `run_prediction` was called with `runtime_config` that is a RuntimeConfig instance

2. **test_v1_fallback_routes_all_to_v1**:
   - Same mocking setup as above
   - Invoke with `['predict', str(fasta_path), '--parallel', '--v1-fallback', '--output-dir', str(tmp_path)]`
   - Assert `run_prediction` was called with `use_v2_architecture=False`
   - Assert `run_prediction` was called with `runtime_config=None`

3. **test_single_gpu_routes_to_v1**:
   - Mock `detect_cuda_devices` to return [0] (single GPU)
   - Invoke WITHOUT `--parallel`: `['predict', str(fasta_path), '--output-dir', str(tmp_path)]`
   - Assert `run_prediction` was called with `use_v2_architecture=False`

4. **test_parallel_constructs_runtime_config_with_resume**:
   - Mock run_prediction
   - Invoke with `['predict', str(fasta_path), '--parallel', '--resume', '--output-dir', str(tmp_path)]`
   - Capture the runtime_config argument passed to run_prediction
   - Assert `runtime_config.enable_checkpointing is True`

5. **test_parallel_force_resume_sets_config**:
   - Mock run_prediction
   - Invoke with `['predict', str(fasta_path), '--parallel', '--force-resume', '--output-dir', str(tmp_path)]`
   - Capture runtime_config
   - Assert `runtime_config.force_restart is True`
   - Assert `runtime_config.enable_checkpointing is True`

6. **test_hybrid_architecture_logged**:
   - Mock run_prediction
   - Invoke with `--parallel`
   - Check that result.output contains "v2.0 hybrid" (hybrid architecture logged)
   - Check that result.output contains "DNABERT-S" and "v1.0" (confirms DNABERT stays v1.0)

7. **test_v1_fallback_architecture_logged**:
   - Mock run_prediction
   - Invoke with `--parallel --v1-fallback`
   - Check that result.output contains "v1.0" and "--v1-fallback"

**Fixtures:**
- `tmp_fasta`: Create a minimal FASTA file in tmp_path for CLI invocation
- Use `@pytest.fixture` for common mocking setup

**Imports:**
- pytest, Path from pathlib
- CliRunner from click.testing
- cli from virnucpro.cli.main
- patch, MagicMock from unittest.mock
- RuntimeConfig from virnucpro.pipeline.runtime_config

**Important:** All tests run on CPU without GPU. Use mocking for:
- `virnucpro.pipeline.prediction.run_prediction` -- mock to return 0
- `virnucpro.cli.predict.validate_and_get_device` -- mock to return torch.device('cpu')
- `virnucpro.cli.predict.detect_cuda_devices` -- mock to return [0, 1] (or [0] for single-GPU test)
- `virnucpro.cli.predict.Config` -- mock config with `get` side_effect returning appropriate defaults

**Test naming convention:** `test_{what}_{condition}_{expected_outcome}` per project convention.
  </action>
  <verify>
    pytest tests/unit/test_cli_predict.py -v --tb=short 2>&1 | tail -20
  </verify>
  <done>
    7 CLI tests pass. Tests verify: --parallel routes ESM-2 to v2.0 (hybrid), --v1-fallback routes all to v1.0, single-GPU stays v1.0, RuntimeConfig constructed from --resume and --force-resume flags, hybrid architecture logged ("v2.0 hybrid" with DNABERT-S v1.0 noted), v1-fallback architecture logged. All tests run on CPU with mocking.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.cli.predict import predict; print('CLI imports OK')"`
- `pytest tests/unit/test_cli_predict.py -v --tb=short`
- `python -m virnucpro predict --help` shows `--v1-fallback` option
- `grep -n "run_multi_gpu_inference\|use_v2_architecture\|v1.fallback\|world_size" virnucpro/cli/predict.py virnucpro/pipeline/prediction.py`
- Verify Stage 5 (DNABERT-S) is unchanged: `grep -A2 "Stage 5.*DNABERT" virnucpro/pipeline/prediction.py` shows no v2.0 references
- Verify world_size is explicit: `grep "world_size=" virnucpro/pipeline/prediction.py`
</verification>

<success_criteria>
`predict.py` routes `--parallel` ESM-2 to v2.0 architecture by default (run_multi_gpu_inference with explicit world_size). DNABERT-S stays on v1.0 (Stage 5 untouched). `--v1-fallback` preserves full v1.0 behavior. RuntimeConfig constructed from CLI flags. Architecture version logged as "v2.0 hybrid". If any v2.0 ESM-2 worker fails, RuntimeError is raised (fail-fast). HDF5-to-PT adapter uses chunked reads to avoid memory issues. 7 CLI integration tests pass on CPU.
</success_criteria>

<output>
After completion, create `.planning/phases/10.1-cli-integration-for-v2-0-architecture/10.1-01-SUMMARY.md`
</output>
