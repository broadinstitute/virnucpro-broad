# Phase 10.1: CLI Integration for v2.0 Architecture - Research

**Researched:** 2026-02-06
**Domain:** Click CLI refactoring and integration testing
**Confidence:** HIGH

## Summary

Phase 10.1 integrates v2.0 async architecture (`run_multi_gpu_inference`) into the existing Click CLI by updating `virnucpro/cli/predict.py` to call the new API instead of v1.0 parallel workers (`parallel_esm.py`, `parallel_dnabert.py`). This is urgent because Phase 10 benchmarks would accidentally compare v1.0 to v1.0 if the CLI isn't updated first.

The current CLI architecture uses Click 8.0+ with command groups registered in `virnucpro/cli/main.py`. The `predict` command currently calls `run_prediction()` from `pipeline/prediction.py`, which internally dispatches to v1.0 multi-worker-per-GPU code (`process_esm_files_worker`, `process_dnabert_files_worker`). The v2.0 architecture is complete (Phases 5-9) but isolated in `pipeline/multi_gpu_inference.py` with the `run_multi_gpu_inference()` entry point.

**Primary recommendation:** Update `predict.py` to conditionally route `--parallel` flag to `run_multi_gpu_inference()` for ESM-2 embedding step, add new `benchmark` subcommands for Phase 10 test exposure, and create migration guide documenting CLI changes.

## Standard Stack

The established libraries/tools for this domain:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| Click | >=8.0.0 | CLI framework with command groups | Industry standard for Python CLIs, used by Flask, pip, Docker Compose |
| pytest | Latest | Testing framework with fixtures | Standard for Python testing, already used in project |
| click.testing.CliRunner | (part of Click) | CLI integration testing | Official Click testing tool, isolated filesystem support |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| subprocess | stdlib | Running external commands | For v1.0 vs v2.0 comparison benchmarks (git worktree) |
| pathlib.Path | stdlib | File path handling | Already used throughout project |
| logging | stdlib | Structured logging | Already integrated in virnucpro |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Click | argparse | argparse is stdlib but verbose and lacks command groups; Click is already used |
| Click | Typer | Typer is modern (type-hints-based) but adds dependency; Click already integrated |
| CliRunner | Manual subprocess | CliRunner provides isolation and structured Result objects; subprocess too low-level |

**Installation:**
All dependencies already installed in project. No new packages required.

## Architecture Patterns

### Recommended Project Structure
Current structure is already correct:
```
virnucpro/cli/
├── main.py              # Click group, command registration
├── predict.py           # Predict command (NEEDS UPDATE)
├── benchmark.py         # Benchmark command (EXISTS, expand for Phase 10)
├── profile.py           # Profile command (existing)
└── utils.py             # Utils command group (existing)
```

### Pattern 1: Click Command Groups
**What:** Commands registered with `cli.add_command()` in main.py
**When to use:** Already implemented correctly
**Example:**
```python
# virnucpro/cli/main.py
@click.group()
@click.pass_context
def cli(ctx, ...):
    ctx.ensure_object(dict)
    ctx.obj['logger'] = setup_logging(...)
    ctx.obj['config'] = Config.load(...)

# Register commands
cli.add_command(predict.predict)
cli.add_command(benchmark.benchmark)
```

### Pattern 2: CLI Integration Testing with CliRunner
**What:** Click provides `click.testing.CliRunner` for invoking commands in tests
**When to use:** Integration tests for predict command after v2.0 migration
**Example:**
```python
# Source: https://click.palletsprojects.com/en/stable/testing/
from click.testing import CliRunner
from virnucpro.cli.main import cli

def test_predict_v2_integration():
    runner = CliRunner()
    with runner.isolated_filesystem():
        # Create test FASTA
        Path("test.fasta").write_text(">seq1\nATCG\n")
        # Invoke predict command
        result = runner.invoke(cli, ['predict', 'test.fasta', '--parallel'])
        assert result.exit_code == 0
        assert "v2.0" in result.output  # Verify v2.0 path used
```

### Pattern 3: Context Object for Shared State
**What:** Click's `@click.pass_context` passes shared state (logger, config) to commands
**When to use:** Already implemented correctly for logger and config sharing
**Example:**
```python
@click.command()
@click.pass_context
def predict(ctx, ...):
    logger = ctx.obj['logger']
    config = ctx.obj['config']
```

### Pattern 4: Conditional API Routing
**What:** Route to different backends based on flags or feature detection
**When to use:** Maintain backward compatibility while introducing v2.0 API
**Example:**
```python
if parallel and use_v2_architecture:
    # Route to v2.0
    from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference
    output_path, failed = run_multi_gpu_inference(...)
else:
    # Route to v1.0 (legacy)
    from virnucpro.pipeline.prediction import run_prediction
    run_prediction(...)
```

### Anti-Patterns to Avoid
- **Mixing business logic in CLI**: Keep CLI thin, delegate to pipeline modules
- **Hardcoding paths**: Use Path objects and config for all paths
- **Ignoring exit codes**: Return proper exit codes (0=success, 1=error, 2=partial)
- **Duplicate option parsing**: Share common options via decorators or context

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| CLI argument parsing | Custom argparse wrapper | Click decorators | Click handles type coercion, validation, help text generation |
| Temporary test files | Manual mkdtemp cleanup | `CliRunner.isolated_filesystem()` | Automatic cleanup, isolated environment |
| Command invocation in tests | Direct function calls | `CliRunner.invoke()` | Simulates real CLI execution with exit codes |
| Logging setup | Custom formatter per command | Shared context object | Logger configured once in main.py, passed via ctx.obj |
| Config loading | Per-command YAML parsing | Shared Config in context | Config loaded once, reused across commands |

**Key insight:** Click's testing module is specifically designed for CLI integration tests. CliRunner provides isolation, captures stdout/stderr, and returns structured Result objects with exit codes. Don't bypass it with subprocess or direct function calls.

## Common Pitfalls

### Pitfall 1: Breaking Backward Compatibility Without Migration Path
**What goes wrong:** Users' scripts break when CLI interface changes
**Why it happens:** v2.0 is major refactor, tempting to change everything at once
**How to avoid:**
- Keep existing `--parallel` flag working (route to v2.0 internally)
- Add `--v1-fallback` flag for users who need old behavior
- Document changes in migration guide
- Log which architecture is being used (v1.0 vs v2.0)
**Warning signs:** Tests pass but user scripts fail with "unrecognized option" errors

### Pitfall 2: Not Testing CLI Integration End-to-End
**What goes wrong:** Unit tests pass but CLI crashes with import errors or argument mismatches
**Why it happens:** Testing pipeline functions directly doesn't exercise CLI layer
**How to avoid:**
- Use `CliRunner.invoke()` to test actual CLI execution
- Test with `--parallel` flag to verify v2.0 routing
- Test exit codes (0, 1, 2) for different outcomes
- Test with realistic FASTA files, not just mocks
**Warning signs:** "Works in Python but fails from command line"

### Pitfall 3: Mixing v1.0 and v2.0 Code Paths
**What goes wrong:** Predict command calls v2.0 for ESM but v1.0 for DNABERT, creating inconsistency
**Why it happens:** Incremental migration without clear boundaries
**How to avoid:**
- Route entire embedding pipeline (both ESM and DNABERT) to v2.0 when `--parallel` flag set
- Or, introduce `--v2` flag explicitly to opt into new architecture
- Log clearly which architecture is active
- Add assertion: if v2.0 enabled, verify both models use new API
**Warning signs:** Benchmarks show unexpected performance, logs show mixed v1/v2 calls

### Pitfall 4: Not Exposing Phase 10 Tests via CLI
**What goes wrong:** Tests exist but users can't run them without pytest knowledge
**Why it happens:** Benchmark tests are pytest-only, no CLI wrapper
**How to avoid:**
- Extend `virnucpro benchmark` command with Phase 10 test suites
- Add `--suite` option: scaling, throughput, memory, equivalence, all
- Add `--data-size` option: tiny, small, medium, large
- Route to pytest subprocess with appropriate markers
**Warning signs:** Users ask "How do I run the benchmarks?" and answer is complex pytest command

### Pitfall 5: Forgetting RuntimeConfig for Checkpointing
**What goes wrong:** CLI calls `run_multi_gpu_inference()` without RuntimeConfig, checkpointing disabled
**Why it happens:** RuntimeConfig is new in Phase 9, easy to forget
**How to avoid:**
- Create RuntimeConfig in predict command with user options
- Map CLI flags to RuntimeConfig fields:
  - `--resume` → `enable_checkpointing=True`
  - `--checkpoint-dir` → `checkpoint_dir=Path(...)`
  - `--force-restart` → `force_restart=True`
- Pass RuntimeConfig to `run_multi_gpu_inference(runtime_config=...)`
**Warning signs:** Checkpointing works in tests but not via CLI

## Code Examples

Verified patterns from official sources:

### Click Command with Context
```python
# Source: https://click.palletsprojects.com/en/stable/commands/
# Current pattern in virnucpro/cli/predict.py (already correct)
@click.command()
@click.argument('input_file', type=click.Path(exists=True))
@click.option('--parallel', is_flag=True, help='Enable multi-GPU')
@click.pass_context
def predict(ctx, input_file, parallel, ...):
    logger = ctx.obj['logger']
    config = ctx.obj['config']
    # Business logic here
```

### CLI Testing with CliRunner
```python
# Source: https://click.palletsprojects.com/en/stable/testing/
from click.testing import CliRunner
from virnucpro.cli.main import cli

def test_predict_calls_v2():
    runner = CliRunner()
    with runner.isolated_filesystem():
        # Setup test data
        Path("input.fasta").write_text(">seq1\nMKLLISTER\n")

        # Invoke CLI
        result = runner.invoke(cli, [
            'predict', 'input.fasta',
            '--parallel',
            '--output-dir', 'output'
        ])

        # Verify
        assert result.exit_code == 0
        assert Path("output/embeddings.h5").exists()
```

### Conditional API Routing (Proposed for predict.py)
```python
# Proposed pattern for virnucpro/cli/predict.py
if parallel:
    logger.info("Using v2.0 async architecture with sequence packing")

    from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference
    from virnucpro.pipeline.runtime_config import RuntimeConfig

    # Build model_config (architecture only)
    model_config = {
        'model_type': 'esm2',
        'model_name': 'esm2_t36_3B_UR50D',
        'enable_fp16': not os.environ.get('VIRNUCPRO_DISABLE_FP16')
    }

    # Build runtime_config (operational params)
    runtime_config = RuntimeConfig(
        enable_checkpointing=resume,
        checkpoint_dir=output_dir / '.checkpoints',
        force_restart=force_resume,
        timeout_per_attempt=7200.0
    )

    # Call v2.0 API
    output_path, failed_ranks = run_multi_gpu_inference(
        fasta_files=[esm_input_path],
        output_dir=esm_output_dir,
        model_config=model_config,
        runtime_config=runtime_config
    )

    if failed_ranks:
        logger.warning(f"Partial completion: {len(failed_ranks)} workers failed")
else:
    logger.info("Using v1.0 multi-worker architecture (legacy)")
    # Call v1.0 pipeline...
```

### Benchmark CLI Subprocess Invocation
```python
# Source: virnucpro/cli/benchmark.py (already implemented pattern)
def benchmark(ctx, suite, data_size, ...):
    cmd = [
        'pytest',
        'tests/benchmarks/',
        '-m', suite,  # Use pytest markers
        '-v', '-s'
    ]

    result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)

    # Display output
    print(result.stdout)

    # Parse JSON report
    report_path = find_latest_report(output_dir)
    display_summary(report_path, logger)

    sys.exit(result.returncode)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Multi-worker-per-GPU (v1.0) | Async DataLoader + packing (v2.0) | Phase 5-9 (2026-02) | 4.5x speedup expected |
| Hardcoded batch sizes | Dynamic token budget | Phase 6 (2026-02) | GPU-aware batch sizing |
| FP32 only | FP16 mixed precision | Phase 8 (2026-02) | 1.5-1.8x speedup |
| No checkpointing | Incremental checkpoints | Phase 9 (2026-02) | Resumable on spot instance preemption |
| pytest-only benchmarks | CLI-exposed benchmarks | Phase 10.1 (NOW) | User-friendly performance validation |

**Deprecated/outdated:**
- `parallel_esm.py`, `parallel_dnabert.py`: v1.0 workers replaced by `multi_gpu_inference.py` in v2.0
- `--esm-batch-size`, `--dnabert-batch-size`: In v2.0, token budget auto-calculated from GPU memory

## Open Questions

Things that couldn't be fully resolved:

1. **Should v1.0 code be removed or kept as fallback?**
   - What we know: v2.0 is complete (Phases 5-9), v1.0 still in codebase
   - What's unclear: User preference for explicit v1.0 fallback vs clean removal
   - Recommendation: Keep v1.0 code initially with `--v1-fallback` flag, deprecate in v2.1, remove in v3.0

2. **How to handle DNABERT-S in v2.0 pipeline?**
   - What we know: v2.0 architecture built for ESM-2 (protein), DNABERT is DNA
   - What's unclear: Whether `run_multi_gpu_inference()` supports DNABERT or needs separate path
   - Recommendation: Check if `model_config={'model_type': 'dnabert'}` is supported, otherwise route DNABERT to v1.0 temporarily

3. **Should benchmark CLI generate reports or just run pytest?**
   - What we know: Current benchmark.py invokes pytest subprocess, parses JSON reports
   - What's unclear: Whether to generate markdown reports automatically or require manual review
   - Recommendation: Auto-generate markdown reports in `tests/reports/` for easy sharing

## Sources

### Primary (HIGH confidence)
- Click Official Documentation (https://click.palletsprojects.com/en/stable/) - Command groups, testing
- Click Testing Module (https://click.palletsprojects.com/en/stable/testing/) - CliRunner patterns
- Existing codebase: `virnucpro/cli/main.py`, `virnucpro/cli/predict.py`, `virnucpro/cli/benchmark.py`
- v2.0 API: `virnucpro/pipeline/multi_gpu_inference.py` - run_multi_gpu_inference entry point
- RuntimeConfig: `virnucpro/pipeline/runtime_config.py` - Phase 9 checkpointing config

### Secondary (MEDIUM confidence)
- Real Python Click Tutorial (https://realpython.com/python-click/) - Command group best practices
- Better Stack Click Guide (https://betterstack.com/community/guides/scaling-python/click-explained/) - Composable CLIs 2025
- Phase 10 Plans (.planning/phases/10-*/10-*-PLAN.md) - Test requirements and structure

### Tertiary (LOW confidence)
- None - all findings verified against official docs or existing codebase

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Click already integrated, patterns verified in codebase
- Architecture: HIGH - Existing CLI structure is correct, clear refactoring path
- Pitfalls: HIGH - Common CLI migration issues well-documented

**Research date:** 2026-02-06
**Valid until:** 30 days (Click is stable, v2.0 API frozen after Phase 9)
