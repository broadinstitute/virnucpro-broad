---
phase: 10.1-cli-integration
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/prediction.py
  - virnucpro/pipeline/gpu_worker.py
  - tests/unit/test_cli_predict.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "v2.0 resume detects v1.0 checkpoint format (.done markers without shard_* dirs) and raises clear error"
    - "world_size is validated against torch.cuda.device_count() before calling run_multi_gpu_inference"
    - "HDF5-to-PT conversion uses try/finally to clean up partial .pt files on failure"
  artifacts:
    - path: "virnucpro/pipeline/gpu_worker.py"
      provides: "v1.0 checkpoint format detection with helpful error message"
      contains: "v1.0 checkpoint format detected"
    - path: "virnucpro/pipeline/prediction.py"
      provides: "world_size validation and temp file cleanup in _stream_h5_to_pt_files"
      contains: "world_size"
    - path: "tests/unit/test_cli_predict.py"
      provides: "Tests for checkpoint format detection, world_size validation, and cleanup"
      contains: "test_v1_checkpoint_format_detected"
  key_links:
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/pipeline/checkpoint_writer.py"
      via: "v1.0 format check before resume_from_checkpoints call"
      pattern: "v1.0 checkpoint format"
    - from: "virnucpro/pipeline/prediction.py"
      to: "virnucpro/pipeline/multi_gpu_inference.py"
      via: "world_size validation before run_multi_gpu_inference call"
      pattern: "world_size.*device_count"
---

<objective>
Close 3 medium-priority gaps from Phase 10.1 verification that affect user experience:

1. **Gap 4 (Checkpoint format compatibility):** Detect v1.0 checkpoint format (.pt + .done markers without shard_* subdirectories) in gpu_worker.py before attempting resume, and raise a clear error message instead of cryptic failure.
2. **Gap 5 (World size validation):** Validate that `world_size` does not exceed `torch.cuda.device_count()` before calling `run_multi_gpu_inference()`, with a clear error message.
3. **Gap 6 (Temp file cleanup):** Wrap `_stream_h5_to_pt_files()` call in prediction.py with try/finally to clean up partial .pt files if conversion crashes mid-way.

Purpose: These gaps affect user experience when switching between v1.0 and v2.0 architectures, or when GPU configuration doesn't match expectations. Without these, users get cryptic errors instead of actionable messages.

Output: Updated `virnucpro/pipeline/gpu_worker.py` with checkpoint format detection, updated `virnucpro/pipeline/prediction.py` with world_size validation and cleanup, updated `tests/unit/test_cli_predict.py` with tests for new validations.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10.1-cli-integration-for-v2-0-architecture/10.1-01-SUMMARY.md
@virnucpro/pipeline/prediction.py
@virnucpro/pipeline/gpu_worker.py
@virnucpro/pipeline/checkpoint_writer.py
@tests/unit/test_cli_predict.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add checkpoint format detection and world_size validation</name>
  <files>virnucpro/pipeline/gpu_worker.py, virnucpro/pipeline/prediction.py</files>
  <action>
### Part A: Checkpoint format detection in gpu_worker.py (Gap 4)

In `virnucpro/pipeline/gpu_worker.py`, add a v1.0 checkpoint format check BEFORE the `resume_from_checkpoints` call (around line 179-183).

The v2.0 checkpoint format uses `shard_*/batch_*.pt` subdirectories under the checkpoint dir. The v1.0 format uses `.done` marker files and per-file `.pt` files (like `gpu0_ESM.pt.done`) directly in the output directory. If a user runs v1.0, fails, then tries to resume with v2.0, the checkpoint directory will contain .done files but no shard_* subdirectories.

Add this check right before the `resume_from_checkpoints` call (around line 179):

```python
# Check for v1.0 checkpoint format (Gap 4: format compatibility)
if enable_checkpointing and not force_restart and checkpoint_dir:
    checkpoint_base = Path(checkpoint_dir)
    if checkpoint_base.exists():
        # v1.0 uses .done markers; v2.0 uses shard_*/batch_*.pt
        done_files = list(checkpoint_base.glob("*.done"))
        shard_dirs = list(checkpoint_base.glob("shard_*"))
        if done_files and not shard_dirs:
            raise RuntimeError(
                f"v1.0 checkpoint format detected in {checkpoint_base}: "
                f"found {len(done_files)} .done marker files but no shard_* directories. "
                f"v2.0 cannot resume from v1.0 checkpoints. "
                f"Options: (1) Use --v1-fallback to resume with v1.0, "
                f"(2) Use --force-resume to start fresh with v2.0, "
                f"(3) Delete {checkpoint_base} and re-run."
            )
```

The `Path` is already imported in gpu_worker.py (line `from pathlib import Path`). No new imports needed.

**Important:** This check happens BEFORE `resume_from_checkpoints()` is called, so it won't interfere with normal v2.0 resume flow.

### Part B: World size validation in prediction.py (Gap 5)

In `virnucpro/pipeline/prediction.py`, in the Stage 6 v2.0 branch (around line 642-643 where `cuda_devices` and `num_gpus` are set), add a validation check:

After `num_gpus = len(cuda_devices) if cuda_devices else 1`, add:

```python
# Validate world_size against actual GPU count (Gap 5)
import torch as _torch
actual_gpu_count = _torch.cuda.device_count() if _torch.cuda.is_available() else 0
if num_gpus > actual_gpu_count and actual_gpu_count > 0:
    raise RuntimeError(
        f"Requested {num_gpus} GPUs but only {actual_gpu_count} available. "
        f"Use --gpus to specify available GPU IDs, or set CUDA_VISIBLE_DEVICES."
    )
```

Note: `torch` is already imported at the top of prediction.py (line 8), so use `torch` directly instead of `_torch`. The check guards against `actual_gpu_count > 0` so it doesn't fail on CPU-only systems where `detect_cuda_devices()` might return a mock list.

### Part C: Temp file cleanup in prediction.py (Gap 6)

In the Stage 6 v2.0 branch of `virnucpro/pipeline/prediction.py`, wrap the `_stream_h5_to_pt_files()` call with try/finally to clean up partial .pt files on failure. Replace the existing conversion block (around lines 670-677) with:

```python
# Convert v2.0 HDF5 output to v1.0 per-file .pt format for merge stage compatibility
# Uses streaming approach to avoid loading entire HDF5 into memory (Issue 2)
# Wrap in try/finally to clean up partial files on crash (Gap 6)
esm_pt_files_created = []
try:
    conversion_start = time.monotonic()
    protein_feature_files = _stream_h5_to_pt_files(
        esm_output_path, protein_split_dir, nucleotide_feature_files
    )
    conversion_elapsed = time.monotonic() - conversion_start
    esm_pt_files_created = list(protein_feature_files)  # Track for cleanup
    failed_files = []  # No failures (we fail-fast above)

    logger.info(f"v2.0 ESM-2 complete: {len(protein_feature_files)} feature files created")
    logger.info(
        f"HDF5-to-PT conversion: {conversion_elapsed:.1f}s for "
        f"{len(protein_feature_files)} files"
    )
    if conversion_elapsed > 60:
        logger.warning(
            f"HDF5-to-PT conversion took {conversion_elapsed:.1f}s - "
            f"consider Phase 10.2 merge stage HDF5 refactor"
        )
except Exception:
    # Clean up any partial .pt files created before the crash
    for pt_file in esm_pt_files_created:
        try:
            if Path(pt_file).exists():
                Path(pt_file).unlink()
                logger.debug(f"Cleaned up partial file: {pt_file}")
        except OSError:
            pass  # Best-effort cleanup
    raise  # Re-raise the original exception
```

**Note:** This incorporates the telemetry timing from Plan 03 Task 1. If Plan 03 Task 1 has already been executed, this replaces the simple timing block with the try/finally version. If Plan 03 Task 1 has NOT been executed yet, this provides both the timing and the cleanup in one shot. Either way the result is the same code.

**Important:** The `raise` at the end of the except block re-raises the original exception, so the pipeline's error handling in the predict command still catches it. The cleanup is best-effort (catches OSError to avoid masking the real error).
  </action>
  <verify>
    grep -n "v1.0 checkpoint format\|world_size.*device_count\|esm_pt_files_created" virnucpro/pipeline/gpu_worker.py virnucpro/pipeline/prediction.py
    python -c "from virnucpro.pipeline.gpu_worker import gpu_worker; print('gpu_worker imports OK')"
    python -c "from virnucpro.pipeline.prediction import run_prediction; print('prediction imports OK')"
  </verify>
  <done>
    `gpu_worker.py` detects v1.0 checkpoints (.done files without shard_* dirs) and raises RuntimeError with 3 actionable options. `prediction.py` validates world_size against actual GPU count before v2.0 inference. `_stream_h5_to_pt_files()` call wrapped in try/finally for partial file cleanup.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tests for checkpoint format detection, world size validation, and cleanup</name>
  <files>tests/unit/test_cli_predict.py</files>
  <action>
Add three tests to `tests/unit/test_cli_predict.py` covering the new validation logic.

### Test 1: v1.0 checkpoint format detection

```python
def test_v1_checkpoint_format_detected(tmp_path):
    """Test that v2.0 resume detects v1.0 checkpoint format and raises clear error.

    Gap 4: Users who run v1.0, fail, then try v2.0 --resume should get actionable
    error instead of cryptic failure.
    """
    # Create v1.0-style checkpoint directory (has .done files, no shard_* dirs)
    checkpoint_dir = tmp_path / "checkpoints"
    checkpoint_dir.mkdir()
    (checkpoint_dir / "gpu0_ESM.pt").touch()
    (checkpoint_dir / "gpu0_ESM.pt.done").touch()
    (checkpoint_dir / "gpu1_ESM.pt").touch()
    (checkpoint_dir / "gpu1_ESM.pt.done").touch()

    # Simulate the v1.0 format check from gpu_worker
    done_files = list(checkpoint_dir.glob("*.done"))
    shard_dirs = list(checkpoint_dir.glob("shard_*"))

    assert len(done_files) == 2, "Should find v1.0 .done markers"
    assert len(shard_dirs) == 0, "Should not find v2.0 shard dirs"

    # Verify the detection logic would trigger
    assert done_files and not shard_dirs, \
        "v1.0 format detection should trigger: .done files exist, no shard_* dirs"


def test_v2_checkpoint_format_not_flagged(tmp_path):
    """Test that v2.0 checkpoint format is NOT flagged as v1.0.

    Ensures normal v2.0 resume flow isn't broken by the v1.0 detection logic.
    """
    # Create v2.0-style checkpoint directory (has shard_* dirs)
    checkpoint_dir = tmp_path / "checkpoints"
    checkpoint_dir.mkdir()
    shard_dir = checkpoint_dir / "shard_0"
    shard_dir.mkdir()
    (shard_dir / "batch_0000.pt").touch()
    (shard_dir / "batch_0001.pt").touch()

    done_files = list(checkpoint_dir.glob("*.done"))
    shard_dirs = list(checkpoint_dir.glob("shard_*"))

    assert len(done_files) == 0, "Should not find .done markers in v2.0 format"
    assert len(shard_dirs) == 1, "Should find v2.0 shard dir"

    # Verify the detection logic would NOT trigger
    should_trigger = done_files and not shard_dirs
    assert not should_trigger, \
        "v1.0 format detection should NOT trigger for v2.0 checkpoints"
```

### Test 2: World size validation

```python
def test_world_size_validation_rejects_excess_gpus():
    """Test that requesting more GPUs than available raises clear error.

    Gap 5: world_size must be validated against actual GPU count to prevent
    confusing failures deep in the multi-GPU coordination code.
    """
    # The validation logic: num_gpus > actual_gpu_count and actual_gpu_count > 0
    # Simulate: user requests 4 GPUs but only 2 available
    num_gpus = 4
    actual_gpu_count = 2

    should_error = num_gpus > actual_gpu_count and actual_gpu_count > 0
    assert should_error, \
        "Should reject when requesting more GPUs than available"

    # Simulate: user requests 2 GPUs, 4 available (should NOT error)
    num_gpus = 2
    actual_gpu_count = 4

    should_error = num_gpus > actual_gpu_count and actual_gpu_count > 0
    assert not should_error, \
        "Should allow when requesting fewer GPUs than available"

    # Simulate: CPU-only system (actual_gpu_count=0, should NOT error)
    num_gpus = 2
    actual_gpu_count = 0

    should_error = num_gpus > actual_gpu_count and actual_gpu_count > 0
    assert not should_error, \
        "Should not error on CPU-only systems (mocked GPU detection)"
```

### Test 3: Temp file cleanup

```python
def test_h5_to_pt_cleanup_on_failure(tmp_path):
    """Test that partial .pt files are cleaned up when conversion fails mid-way.

    Gap 6: If _stream_h5_to_pt_files crashes after creating some .pt files,
    the cleanup logic should remove them to prevent orphaned files.
    """
    # Create some .pt files that would be "partially created"
    output_dir = tmp_path / "esm_output"
    output_dir.mkdir()

    partial_files = []
    for i in range(3):
        pt_file = output_dir / f"output_{i}_ESM.pt"
        pt_file.write_bytes(b"partial data")
        partial_files.append(pt_file)

    # Verify files exist before cleanup
    assert all(f.exists() for f in partial_files), "Partial files should exist"

    # Simulate cleanup logic from prediction.py try/finally
    for pt_file in partial_files:
        try:
            if Path(pt_file).exists():
                Path(pt_file).unlink()
        except OSError:
            pass

    # Verify cleanup
    assert not any(f.exists() for f in partial_files), \
        "All partial .pt files should be cleaned up"
```

**Note on test approach:** These tests validate the detection LOGIC (conditions, file patterns) rather than calling the full gpu_worker or prediction pipeline, which would require GPU mocking. This is intentional -- the logic tests ensure the conditions are correct, while the actual integration is covered by the existing 8 CLI tests.
  </action>
  <verify>
    pytest tests/unit/test_cli_predict.py -v --tb=short 2>&1 | tail -20
  </verify>
  <done>
    5 new tests added: v1.0 checkpoint format detection (2 tests), world_size validation (1 test), temp file cleanup (1 test), plus the regression test from Plan 03 is already present. All 13+ tests pass (8 existing + 5 new). Tests validate detection logic without GPU requirements.
  </done>
</task>

</tasks>

<verification>
- `grep -n "v1.0 checkpoint format" virnucpro/pipeline/gpu_worker.py` shows format detection
- `grep -n "world_size.*device_count\|actual_gpu_count" virnucpro/pipeline/prediction.py` shows world size validation
- `grep -n "esm_pt_files_created\|except.*cleanup\|finally" virnucpro/pipeline/prediction.py` shows cleanup logic
- `pytest tests/unit/test_cli_predict.py -v --tb=short` all tests pass
- `python -c "from virnucpro.pipeline.gpu_worker import gpu_worker; print('OK')"` clean import
</verification>

<success_criteria>
gpu_worker.py detects v1.0 checkpoint format (.done markers without shard_* directories) and raises RuntimeError with actionable options (--v1-fallback, --force-resume, or delete). prediction.py validates world_size against actual GPU count before v2.0 inference. _stream_h5_to_pt_files call wrapped in try/finally for partial file cleanup on crash. 5 new tests validate detection logic, world_size conditions, and cleanup behavior. All existing tests still pass.
</success_criteria>

<output>
After completion, create `.planning/phases/10.1-cli-integration-for-v2-0-architecture/10.1-04-SUMMARY.md`
</output>
