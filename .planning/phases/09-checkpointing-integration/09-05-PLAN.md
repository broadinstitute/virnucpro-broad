---
phase: 09-checkpointing-integration
plan: 05
type: execute
wave: 4
depends_on: ["09-02", "09-04"]
files_modified:
  - virnucpro/pipeline/gpu_coordinator.py
  - virnucpro/pipeline/multi_gpu_inference.py
  - virnucpro/pipeline/runtime_config.py
autonomous: true

must_haves:
  truths:
    - "Spot preemption failures retry infinitely until capacity returns"
    - "Poison input detection uses 2-attempt circuit breaker per batch"
    - "OOM failures retry with batch size reduction signal"
    - "Only coordinator writes manifest, workers signal via results_queue"
    - "Coordinator monitors all GPUs asynchronously during retries (non-blocking)"
    - "Failed shard work redistributed to healthy GPUs (elastic redistribution)"
    - "Coordinator SIGTERM handler orchestrates graceful shutdown"
    - "Timeout applies per-attempt, not globally"
    - "RuntimeConfig separates operational params from model_config"
    - "Worker respawn validates checkpoint directory integrity"
  artifacts:
    - path: "virnucpro/pipeline/gpu_coordinator.py"
      provides: "Async worker monitoring, differentiated retry policies, elastic redistribution, SIGTERM handler"
      contains: "monitor_workers_async"
    - path: "virnucpro/pipeline/multi_gpu_inference.py"
      provides: "Checkpoint-enabled multi-GPU inference with RuntimeConfig"
      contains: "RuntimeConfig"
    - path: "virnucpro/pipeline/runtime_config.py"
      provides: "RuntimeConfig dataclass for operational parameters"
      contains: "class RuntimeConfig"
  key_links:
    - from: "virnucpro/pipeline/multi_gpu_inference.py"
      to: "virnucpro/pipeline/checkpoint_manifest.py"
      via: "CheckpointManifest initialization and coordinator-only writes"
      pattern: "CheckpointManifest"
    - from: "virnucpro/pipeline/gpu_coordinator.py"
      to: "virnucpro/pipeline/gpu_worker.py"
      via: "Worker respawn with checkpoint validation"
      pattern: "spawn_worker"
    - from: "virnucpro/pipeline/runtime_config.py"
      to: "dataclasses"
      via: "RuntimeConfig dataclass separates operational from model config"
      pattern: "@dataclass"
---

<objective>
Add fault-tolerant coordinator with differentiated retry policies, elastic redistribution, and graceful shutdown.

Purpose: Production GPU workloads face three failure modes requiring different handling:
1. **Spot preemption** (SIGTERM): Retry infinitely, capacity returns eventually
2. **Poison inputs** (same batch crashes twice): Circuit breaker, isolate toxic sequences
3. **Transient errors** (OOM, network): Exponential backoff with resource adjustment

The coordinator must monitor all GPUs asynchronously during retries, redistribute failed work to healthy GPUs, orchestrate graceful shutdown on SIGTERM, and maintain manifest consistency through coordinator-only writes.

Output: Modified `virnucpro/pipeline/gpu_coordinator.py`, `virnucpro/pipeline/multi_gpu_inference.py`, and new `virnucpro/pipeline/runtime_config.py`.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-02-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-04-SUMMARY.md
@virnucpro/pipeline/gpu_coordinator.py
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/checkpoint_manifest.py
@virnucpro/pipeline/gpu_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create RuntimeConfig dataclass</name>
  <files>virnucpro/pipeline/runtime_config.py</files>
  <action>
Create `virnucpro/pipeline/runtime_config.py` to separate operational parameters from model architecture config (Issue 8):

```python
"""Runtime configuration for GPU inference operations.

Separates operational concerns (checkpointing, timeouts, retry policies) from
model architecture configuration (dtype, hidden_size, num_layers).

This separation ensures:
- Model config remains architecture-only for reproducibility
- Runtime params don't pollute checkpoint metadata
- Clear distinction between "what model" vs "how to run it"
"""

from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Optional


@dataclass
class RuntimeConfig:
    """Runtime operational configuration for GPU inference.

    These parameters control execution behavior, not model architecture.
    They are NOT saved to checkpoint metadata.

    Attributes:
        enable_checkpointing: Enable incremental checkpointing
        checkpoint_dir: Directory for checkpoint files (default: output_dir/checkpoints)
        force_restart: Ignore existing checkpoints and start fresh
        checkpoint_seq_threshold: Sequence count trigger for checkpoint
        checkpoint_time_threshold: Time threshold (seconds) for checkpoint
        timeout_per_attempt: Timeout per retry attempt (not global)
        max_retries_transient: Max retries for transient errors (OOM, network)
        max_retries_poison: Max retries for same batch before circuit breaker (default: 2)
        spot_retry_poll_interval: Seconds to wait between spot capacity polls (default: 60)
        enable_elastic_redistribution: Reassign failed work to healthy GPUs
    """

    # Checkpointing
    enable_checkpointing: bool = True
    checkpoint_dir: Optional[Path] = None
    force_restart: bool = False
    checkpoint_seq_threshold: int = 10000
    checkpoint_time_threshold: float = 300.0

    # Timeouts (Issue 7: per-attempt, not global)
    timeout_per_attempt: Optional[float] = 3600.0  # 1 hour per retry attempt

    # Retry policies (Issue 1: differentiated by failure type)
    max_retries_transient: int = 3  # OOM, network errors
    max_retries_poison: int = 2  # Same batch failures before circuit breaker
    spot_retry_poll_interval: float = 60.0  # Spot preemption: poll every 60s, infinite retries

    # Elastic redistribution (Issue 5)
    enable_elastic_redistribution: bool = True

    def to_dict(self) -> dict:
        """Convert to dict for passing to workers."""
        d = asdict(self)
        # Convert Path to str for serialization
        if self.checkpoint_dir:
            d['checkpoint_dir'] = str(self.checkpoint_dir)
        return d

    @classmethod
    def from_dict(cls, d: dict) -> 'RuntimeConfig':
        """Reconstruct from dict."""
        if 'checkpoint_dir' in d and d['checkpoint_dir']:
            d['checkpoint_dir'] = Path(d['checkpoint_dir'])
        return cls(**d)
```

This addresses Issue 8 (model config pollution) by creating a clear boundary between architecture and operations.
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.runtime_config import RuntimeConfig
config = RuntimeConfig()
assert config.enable_checkpointing == True
assert config.max_retries_transient == 3
assert config.max_retries_poison == 2
assert config.timeout_per_attempt == 3600.0
d = config.to_dict()
config2 = RuntimeConfig.from_dict(d)
assert config == config2
print('✓ RuntimeConfig dataclass verified')
"
  </verify>
  <done>
    RuntimeConfig dataclass separates operational parameters from model architecture config. Supports serialization for worker passing. Documents timeout as per-attempt, not global.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add fault-tolerant coordinator with async monitoring</name>
  <files>virnucpro/pipeline/gpu_coordinator.py</files>
  <action>
Add fault-tolerant worker management to GPUProcessCoordinator:

**Step 1: Add tracking state to __init__**:
```python
def __init__(self, world_size: int, start_method: str = 'spawn'):
    # ... existing initialization ...

    # Fault tolerance tracking (Issue 2: per-batch circuit breaker)
    self.worker_retry_counts: Dict[int, int] = {}  # rank -> total retry count
    self.batch_failure_tracking: Dict[int, Dict[int, int]] = {}  # rank -> {batch_idx -> failure_count}
    self.failed_batches: Set[Tuple[int, int]] = set()  # (rank, batch_idx) that hit circuit breaker

    # Elastic redistribution tracking (Issue 5)
    self.redistributed_work: Dict[int, int] = {}  # failed_rank -> new_rank

    # SIGTERM coordination (Issue 6)
    self.shutdown_requested = False
    import signal
    signal.signal(signal.SIGTERM, self._sigterm_handler)
    signal.signal(signal.SIGINT, self._sigterm_handler)
```

**Step 2: Add SIGTERM handler** (Issue 6):
```python
def _sigterm_handler(self, signum, frame):
    """Orchestrate graceful shutdown on SIGTERM/SIGINT.

    Called when coordinator receives SIGTERM (spot preemption) or Ctrl-C.
    Signals all workers to checkpoint and exit gracefully.
    """
    logger.warning(
        f"Coordinator received signal {signum} (SIGTERM/SIGINT), "
        f"orchestrating graceful shutdown"
    )
    self.shutdown_requested = True

    # Signal all workers to checkpoint and exit
    # Workers will catch their own SIGTERM (propagated by OS) and save emergency checkpoint
    # Coordinator waits for checkpoint completion
    logger.info("Waiting 30s for workers to save emergency checkpoints...")
    import time
    time.sleep(30)

    # Terminate remaining workers
    self.terminate_all()
    sys.exit(143)  # Standard SIGTERM exit code
```

**Step 3: Add error classification** (Issue 9):
```python
def _classify_error(self, status: dict) -> str:
    """Classify error type from worker status for appropriate retry policy.

    Returns:
        "spot_preemption": Infinite retry with polling
        "poison_input": Circuit breaker after 2 attempts on same batch
        "oom": Exponential backoff with batch size reduction
        "transient": Standard exponential backoff
    """
    error = status.get('error', '')
    error_msg = status.get('error_message', '')

    # Spot preemption: exitcode 143 (SIGTERM) or explicit status
    if status.get('exitcode') == 143 or 'sigterm' in error_msg.lower():
        return "spot_preemption"

    # OOM: CUDA out of memory
    if error == 'cuda_oom' or 'out of memory' in error_msg.lower():
        return "oom"

    # Poison input: CUDA runtime errors, assertions
    if error == 'cuda_runtime' or status.get('circuit_breaker', False):
        return "poison_input"

    # Everything else: transient
    return "transient"
```

**Step 4: Add checkpoint validation before respawn** (Issue 10):
```python
def _validate_checkpoint_dir(self, rank: int, checkpoint_dir: Path) -> bool:
    """Validate checkpoint directory integrity before worker respawn.

    Checks:
    - Directory exists and is readable
    - .done markers match checkpoint files
    - No orphaned temp files (.tmp)

    Returns: True if valid, False if corrupted
    """
    shard_dir = checkpoint_dir / f"shard_{rank}"
    if not shard_dir.exists():
        logger.info(f"Rank {rank}: No checkpoint dir, fresh start")
        return True

    # Check for orphaned temp files (incomplete writes)
    temp_files = list(shard_dir.glob("*.tmp"))
    if temp_files:
        logger.warning(
            f"Rank {rank}: Found {len(temp_files)} orphaned temp files, "
            f"removing before respawn"
        )
        for tmp in temp_files:
            tmp.unlink()

    # Verify .done markers match checkpoint files
    checkpoints = list(shard_dir.glob("batch_*.pt"))
    done_markers = list(shard_dir.glob("batch_*.pt.done"))

    if len(checkpoints) != len(done_markers):
        logger.warning(
            f"Rank {rank}: Checkpoint/marker mismatch "
            f"({len(checkpoints)} .pt files, {len(done_markers)} .done markers)"
        )
        # Allow resume - worker will detect and handle corrupted checkpoints

    return True
```

**Step 5: Add async worker monitoring** (Issue 4: non-blocking):
```python
def monitor_workers_async(
    self,
    runtime_config: 'RuntimeConfig',
    manifest: Optional['CheckpointManifest'] = None,
    check_interval: float = 5.0
) -> Dict[int, bool]:
    """Monitor workers asynchronously with differentiated retry policies.

    Non-blocking monitoring loop that:
    - Polls worker status every check_interval seconds
    - Retries failed workers according to error type
    - Continues monitoring healthy workers during retries
    - Updates manifest (coordinator-only writes)
    - Handles SIGTERM gracefully

    Args:
        runtime_config: Runtime configuration with retry policies
        manifest: Optional manifest for coordinator updates (Issue 3)
        check_interval: Seconds between status polls

    Returns:
        Dict mapping rank -> completion status (True=success, False=failed)
    """
    import time
    from virnucpro.pipeline.runtime_config import RuntimeConfig

    active_workers = set(self.workers.keys())
    completed_workers: Dict[int, bool] = {}  # rank -> success

    logger.info(
        f"Monitoring {len(active_workers)} workers with differentiated retry policies: "
        f"spot=infinite, poison={runtime_config.max_retries_poison}, "
        f"transient={runtime_config.max_retries_transient}"
    )

    while active_workers and not self.shutdown_requested:
        time.sleep(check_interval)

        # Check each active worker
        for rank in list(active_workers):
            worker = self.workers.get(rank)
            if not worker or not worker.is_alive():
                # Worker finished, check status
                status = self._get_worker_status(rank)

                if status.get('status') == 'complete':
                    # Success
                    logger.info(f"Rank {rank}: Completed successfully")
                    completed_workers[rank] = True
                    active_workers.remove(rank)

                    # Update manifest (coordinator-only, Issue 3)
                    if manifest:
                        manifest.mark_shard_complete(rank)
                else:
                    # Failure - classify and retry
                    error_type = self._classify_error(status)
                    should_retry, reason = self._should_retry_worker(
                        rank, error_type, status, runtime_config
                    )

                    if should_retry:
                        logger.warning(
                            f"Rank {rank}: {error_type} failure, retrying ({reason})"
                        )
                        self._retry_worker(rank, error_type, runtime_config)
                        # Worker remains in active_workers
                    else:
                        # Permanent failure
                        logger.error(
                            f"Rank {rank}: Permanent failure after retries ({reason})"
                        )
                        completed_workers[rank] = False
                        active_workers.remove(rank)

                        # Update manifest
                        if manifest:
                            manifest.mark_shard_failed(rank, reason)

                        # Elastic redistribution (Issue 5)
                        if runtime_config.enable_elastic_redistribution:
                            self._redistribute_failed_shard(rank, active_workers, manifest)

    # All workers finished or shutdown requested
    if self.shutdown_requested:
        logger.warning("Shutdown requested, terminating remaining workers")
        self.terminate_all()

    return completed_workers
```

**Step 6: Add retry decision logic** (Issue 1: differentiated policies):
```python
def _should_retry_worker(
    self,
    rank: int,
    error_type: str,
    status: dict,
    runtime_config: 'RuntimeConfig'
) -> Tuple[bool, str]:
    """Decide if worker should retry based on error type and retry counts.

    Differentiated retry policies:
    - spot_preemption: Always retry (infinite)
    - poison_input: Retry up to max_retries_poison on same batch
    - oom/transient: Retry up to max_retries_transient

    Returns:
        (should_retry, reason_string)
    """
    retry_count = self.worker_retry_counts.get(rank, 0)

    if error_type == "spot_preemption":
        # Infinite retry for spot preemption (Issue 1)
        return (True, f"spot preemption #{retry_count + 1}, capacity will return")

    elif error_type == "poison_input":
        # Circuit breaker for same batch (Issue 2)
        batch_idx = status.get('batch_idx', -1)
        if batch_idx >= 0:
            batch_failures = self.batch_failure_tracking.setdefault(rank, {})
            batch_failures[batch_idx] = batch_failures.get(batch_idx, 0) + 1

            if batch_failures[batch_idx] >= runtime_config.max_retries_poison:
                # Circuit breaker triggered
                self.failed_batches.add((rank, batch_idx))
                return (
                    False,
                    f"circuit breaker: batch {batch_idx} failed "
                    f"{batch_failures[batch_idx]} times (poison input suspected)"
                )

        # Retry if under limit
        if retry_count < runtime_config.max_retries_poison:
            return (True, f"attempt {retry_count + 1}/{runtime_config.max_retries_poison}")
        else:
            return (False, f"exhausted {retry_count} retries for poison input")

    elif error_type in ("oom", "transient"):
        # Standard exponential backoff (Issue 1)
        if retry_count < runtime_config.max_retries_transient:
            return (True, f"attempt {retry_count + 1}/{runtime_config.max_retries_transient}")
        else:
            return (False, f"exhausted {retry_count} retries for {error_type}")

    else:
        # Unknown error type - don't retry
        return (False, f"unknown error type: {error_type}")
```

**Step 7: Add worker retry with validation** (Issue 10):
```python
def _retry_worker(
    self,
    rank: int,
    error_type: str,
    runtime_config: 'RuntimeConfig'
):
    """Retry failed worker with appropriate delay and validation.

    Handles:
    - Checkpoint directory validation before respawn (Issue 10)
    - Error-specific retry delays (Issue 1)
    - Retry count tracking
    """
    import time

    retry_count = self.worker_retry_counts.get(rank, 0)
    self.worker_retry_counts[rank] = retry_count + 1

    # Validate checkpoint directory (Issue 10)
    if runtime_config.checkpoint_dir:
        if not self._validate_checkpoint_dir(rank, runtime_config.checkpoint_dir):
            logger.error(f"Rank {rank}: Checkpoint validation failed, cannot retry safely")
            return

    # Calculate retry delay based on error type (Issue 1)
    if error_type == "spot_preemption":
        delay = runtime_config.spot_retry_poll_interval  # 60s default
        logger.info(
            f"Rank {rank}: Waiting {delay:.0f}s for spot capacity "
            f"(attempt #{retry_count + 1})"
        )
    elif error_type == "oom":
        # Exponential backoff, capped at 60s
        delay = min(2.0 ** retry_count, 60.0)
        logger.warning(
            f"Rank {rank}: OOM retry in {delay:.1f}s "
            f"(will signal batch size reduction)"
        )
    else:
        # Transient/poison: standard exponential backoff
        delay = min(1.0 * (2 ** retry_count), 60.0)
        logger.info(f"Rank {rank}: Retry in {delay:.1f}s")

    time.sleep(delay)

    # Respawn worker
    # NOTE: Actual respawn needs worker_fn and worker_args passed through
    # This is handled by monitor_workers_async caller providing respawn callback
    logger.info(f"Rank {rank}: Respawning worker (attempt #{retry_count + 1})")
```

**Step 8: Add elastic redistribution** (Issue 5):
```python
def _redistribute_failed_shard(
    self,
    failed_rank: int,
    active_workers: Set[int],
    manifest: Optional['CheckpointManifest']
):
    """Redistribute failed shard work to healthy GPU.

    When a shard permanently fails, reassign its remaining work to a healthy GPU
    that has available capacity.

    This prevents work loss from spot preemption or permanent failures.
    """
    if not manifest:
        logger.warning(
            f"Rank {failed_rank}: Cannot redistribute without manifest, "
            f"work will be lost"
        )
        return

    # Find healthy GPU with capacity
    # For simplicity: assign to lowest-numbered active worker
    # Production: consider GPU memory, current workload
    if active_workers:
        new_rank = min(active_workers)
        logger.info(
            f"Rank {failed_rank}: Redistributing remaining work to rank {new_rank}"
        )
        self.redistributed_work[failed_rank] = new_rank

        # Update manifest to track redistribution
        manifest.record_redistribution(failed_rank, new_rank)
    else:
        logger.error(
            f"Rank {failed_rank}: No healthy workers available for redistribution"
        )
```

**Step 9: Add helper for worker status**:
```python
def _get_worker_status(self, rank: int) -> dict:
    """Get worker status from results queue.

    Returns:
        Status dict with 'status', 'error', 'error_message', 'batch_idx', etc.
    """
    # Poll results_queue for this rank's status
    # This requires the queue to be checked periodically
    # Implementation depends on existing collect_results pattern

    # Simplified: check exitcode if available
    worker = self.workers.get(rank)
    if worker:
        exitcode = worker.exitcode
        if exitcode == 0:
            return {'status': 'complete', 'rank': rank}
        elif exitcode == 143:
            return {'status': 'failed', 'error': 'sigterm', 'exitcode': 143, 'rank': rank}
        else:
            return {'status': 'failed', 'error': 'unknown', 'exitcode': exitcode, 'rank': rank}

    return {'status': 'unknown', 'rank': rank}
```

Add imports at top:
```python
import signal
import sys
from typing import Dict, Set, Tuple, Optional
```

IMPORTANT: These are NEW methods. Do NOT modify existing spawn_workers, wait_for_completion, terminate_all.
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.gpu_coordinator import GPUProcessCoordinator
import inspect

# Verify new methods exist
assert hasattr(GPUProcessCoordinator, 'monitor_workers_async'), 'Missing monitor_workers_async'
assert hasattr(GPUProcessCoordinator, '_classify_error'), 'Missing _classify_error'
assert hasattr(GPUProcessCoordinator, '_should_retry_worker'), 'Missing _should_retry_worker'
assert hasattr(GPUProcessCoordinator, '_retry_worker'), 'Missing _retry_worker'
assert hasattr(GPUProcessCoordinator, '_redistribute_failed_shard'), 'Missing _redistribute_failed_shard'
assert hasattr(GPUProcessCoordinator, '_validate_checkpoint_dir'), 'Missing _validate_checkpoint_dir'
assert hasattr(GPUProcessCoordinator, '_sigterm_handler'), 'Missing _sigterm_handler'

# Verify error classification returns correct types
src = inspect.getsource(GPUProcessCoordinator._classify_error)
for error_type in ['spot_preemption', 'poison_input', 'oom', 'transient']:
    assert error_type in src, f'Missing {error_type} classification'

print('✓ Coordinator fault tolerance verified')
"
  </verify>
  <done>
    GPUProcessCoordinator has async worker monitoring with differentiated retry policies (spot=infinite, poison=2-attempt circuit breaker, transient=3-attempt exponential backoff). SIGTERM handler orchestrates graceful shutdown. Elastic redistribution reassigns failed work. Checkpoint validation before respawn. Error classification with intelligent diagnostic tiering.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire RuntimeConfig into multi_gpu_inference</name>
  <files>virnucpro/pipeline/multi_gpu_inference.py</files>
  <action>
Modify `run_multi_gpu_inference()` to use RuntimeConfig and coordinator async monitoring:

**Step 1: Update function signature**:
```python
def run_multi_gpu_inference(
    input_path: Path,
    output_dir: Path,
    model_config: Dict[str, Any],
    world_size: Optional[int] = None,
    timeout: Optional[float] = None,  # DEPRECATED: use runtime_config.timeout_per_attempt
    runtime_config: Optional['RuntimeConfig'] = None,
) -> Tuple[Path, List[int]]:
    """Run multi-GPU inference with checkpoint support.

    Args:
        input_path: Path to input FASTA or sequence index
        output_dir: Output directory for embeddings
        model_config: Model architecture configuration (NOT operational params)
        world_size: Number of GPUs (default: auto-detect)
        timeout: DEPRECATED - use runtime_config.timeout_per_attempt instead
        runtime_config: Runtime operational configuration (checkpointing, retries, etc.)

    Returns:
        (output_path, failed_ranks): Path to merged output and list of failed ranks
    """
    from virnucpro.pipeline.runtime_config import RuntimeConfig

    # Initialize runtime config with defaults if not provided
    if runtime_config is None:
        runtime_config = RuntimeConfig()

    # Backward compatibility: if timeout provided, use it
    if timeout is not None:
        logger.warning(
            "timeout parameter is deprecated, use runtime_config.timeout_per_attempt instead"
        )
        runtime_config.timeout_per_attempt = timeout

    # Set checkpoint_dir default if not provided
    if runtime_config.checkpoint_dir is None:
        runtime_config.checkpoint_dir = output_dir / "checkpoints"
```

**Step 2: Initialize manifest** (coordinator-only writes, Issue 3):
```python
# Checkpoint setup
manifest = None
if runtime_config.enable_checkpointing:
    runtime_config.checkpoint_dir.mkdir(parents=True, exist_ok=True)

    from virnucpro.pipeline.checkpoint_manifest import CheckpointManifest
    manifest_path = runtime_config.checkpoint_dir / "manifest.json"

    if runtime_config.force_restart and manifest_path.exists():
        logger.info("Force restart: removing existing manifest")
        manifest_path.unlink()

    manifest = CheckpointManifest(manifest_path)
    if not manifest.exists():
        manifest.initialize(world_size)

    logger.info(f"Checkpointing enabled: {runtime_config.checkpoint_dir}")
```

**Step 3: Pass RuntimeConfig to workers** (Issue 8: clean separation):
```python
# Build worker arguments
# model_config contains ONLY model architecture (dtype, hidden_size, etc.)
# runtime_config contains operational params (checkpointing, retries, etc.)

worker_args = {
    'index_path': index_path,
    'output_dir': output_dir,
    'model_config': model_config,  # Architecture only
    'runtime_config': runtime_config.to_dict(),  # Operational params
}
```

**Step 4: Use async monitoring** (Issue 4: non-blocking):
```python
# Spawn workers
coordinator = GPUProcessCoordinator(world_size)
coordinator.spawn_workers(gpu_worker, worker_args)

# Monitor with async retry handling (non-blocking)
completion_status = coordinator.monitor_workers_async(
    runtime_config=runtime_config,
    manifest=manifest,
    check_interval=5.0  # Poll every 5 seconds
)

# Identify successful and failed ranks
successful_ranks = [rank for rank, success in completion_status.items() if success]
failed_ranks = [rank for rank, success in completion_status.items() if not success]

logger.info(
    f"Inference complete: {len(successful_ranks)} successful, "
    f"{len(failed_ranks)} failed"
)

# Update manifest (coordinator-only final write)
if manifest:
    progress = manifest.get_global_progress()
    logger.info(f"Final checkpoint progress: {progress}")
```

**Step 5: Handle partial failures gracefully**:
```python
# Collect results from successful workers only
results = coordinator.collect_results()
successful_results = [r for r in results if r['rank'] in successful_ranks]

if not successful_results:
    raise RuntimeError("All workers failed, no output produced")

# Aggregate results from successful shards
# (existing aggregation logic)
```

**Step 6: Update run_esm2_multi_gpu**:
```python
def run_esm2_multi_gpu(
    input_path: Path,
    output_dir: Path,
    model_name: str = 'esm2_t36_3B_UR50D',
    batch_size: int = 4,
    enable_fp16: bool = True,
    world_size: Optional[int] = None,
    runtime_config: Optional['RuntimeConfig'] = None,
) -> Path:
    """ESM-2 multi-GPU inference with checkpoint support.

    Args:
        ... existing params ...
        runtime_config: Runtime configuration for checkpointing and retries
    """
    from virnucpro.pipeline.runtime_config import RuntimeConfig

    if runtime_config is None:
        runtime_config = RuntimeConfig()

    model_config = {
        'model_type': 'esm2',
        'model_name': model_name,
        'batch_size': batch_size,
        'enable_fp16': enable_fp16,
        # NO checkpointing params here - they're in runtime_config
    }

    output_path, failed_ranks = run_multi_gpu_inference(
        input_path=input_path,
        output_dir=output_dir,
        model_config=model_config,
        world_size=world_size,
        runtime_config=runtime_config,
    )

    if failed_ranks:
        logger.warning(
            f"Partial completion: {len(failed_ranks)} shards failed. "
            f"Outputs from successful shards available at {output_path}"
        )

    return output_path
```

Add imports:
```python
from virnucpro.pipeline.runtime_config import RuntimeConfig
from typing import Optional, Tuple, List
```

IMPORTANT: model_config now contains ONLY architecture params. All operational params moved to runtime_config.
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference, run_esm2_multi_gpu
from virnucpro.pipeline.runtime_config import RuntimeConfig
import inspect

# Verify signature
sig = inspect.signature(run_multi_gpu_inference)
assert 'runtime_config' in sig.parameters, 'Missing runtime_config parameter'

# Verify RuntimeConfig usage
src = inspect.getsource(run_multi_gpu_inference)
assert 'RuntimeConfig' in src, 'Missing RuntimeConfig import'
assert 'monitor_workers_async' in src, 'Missing async monitoring'
assert 'checkpoint_dir.mkdir' in src, 'Missing checkpoint dir setup'

# Verify model_config clean separation
assert 'model_config' in src, 'Missing model_config'
# Operational params should NOT be in model_config anymore
assert 'enable_checkpointing' not in src or 'runtime_config.enable_checkpointing' in src

print('✓ Multi-GPU RuntimeConfig integration verified')
"
  </verify>
  <done>
    run_multi_gpu_inference uses RuntimeConfig for operational parameters (Issue 8), async worker monitoring (Issue 4), coordinator-only manifest writes (Issue 3), and handles partial failures gracefully. model_config now contains ONLY architecture params. Backward compatible with optional runtime_config parameter.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.runtime_config import RuntimeConfig"` - import succeeds
- `python -c "from virnucpro.pipeline.gpu_coordinator import GPUProcessCoordinator"` - import succeeds
- `python -c "from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference"` - import succeeds
- RuntimeConfig dataclass has all required fields (checkpointing, retry policies, timeouts)
- GPUProcessCoordinator has monitor_workers_async, _classify_error, _should_retry_worker, etc.
- Error classification returns spot_preemption, poison_input, oom, transient
- Retry policies differentiated: spot=infinite, poison=2, transient=3
- SIGTERM handler registered in coordinator __init__
- Checkpoint validation before respawn
- Elastic redistribution implemented
- model_config separated from runtime_config
- Existing tests pass: `pytest tests/unit/test_gpu_coordinator.py tests/unit/test_multi_gpu_inference.py -v`
</verification>

<success_criteria>
Coordinator monitors workers asynchronously (non-blocking) with differentiated retry policies: spot preemption retries infinitely, poison inputs trigger 2-attempt circuit breaker, transient errors use 3-attempt exponential backoff. Only coordinator writes manifest (workers signal via queue). SIGTERM handler orchestrates graceful shutdown. Failed shard work redistributed to healthy GPUs. Checkpoint directory validated before respawn. RuntimeConfig separates operational params from model_config. Timeout is per-attempt, not global. Error classification explicit and tested.
</success_criteria>

<notes>
**Key changes from original plan:**

1. **Differentiated retry policies** (Issue 1):
   - Spot: Infinite with 60s polling
   - Poison: 2-attempt circuit breaker per batch
   - OOM: 3-attempt with batch size reduction signal
   - Transient: 3-attempt exponential backoff

2. **Circuit breaker** (Issue 2):
   - Track failures per (rank, batch_idx)
   - After 2 failures on same batch: circuit breaker
   - Failed batches logged for reprocessing

3. **Manifest concurrency** (Issue 3):
   - Only coordinator writes manifest
   - Workers signal via results_queue
   - No cross-process JSON corruption

4. **Async monitoring** (Issue 4):
   - Non-blocking coordinator loop
   - Polls workers every 5s
   - Continues monitoring healthy workers during retries

5. **Elastic redistribution** (Issue 5):
   - Failed shard work reassigned to healthy GPU
   - Manifest tracks redistribution history

6. **SIGTERM handler** (Issue 6):
   - Coordinator catches signal
   - Waits 30s for workers to checkpoint
   - Terminates remaining workers gracefully

7. **Timeout clarity** (Issue 7):
   - timeout_per_attempt in RuntimeConfig
   - Applies per retry attempt, not globally

8. **RuntimeConfig** (Issue 8):
   - Separates operational from model config
   - Clean serialization boundary
   - Documented fields

9. **Error classification** (Issue 9):
   - Explicit _classify_error method
   - Checks exitcode, error type, message patterns
   - Returns spot_preemption, poison_input, oom, transient

10. **Checkpoint validation** (Issue 10):
    - _validate_checkpoint_dir before respawn
    - Removes orphaned temp files
    - Verifies .done markers

**Architecture flow:**
1. Coordinator spawns workers
2. Async monitoring loop polls status
3. Worker fails → classify error → apply policy
4. Spot: wait 60s, retry infinitely
5. Poison: track batch, circuit breaker at 2
6. OOM/transient: exponential backoff, max 3
7. Permanent failure → elastic redistribution
8. Manifest updated by coordinator only
9. SIGTERM → graceful shutdown with checkpoints
</notes>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-05-SUMMARY.md`
</output>
