---
phase: 09-checkpointing-integration
plan: 05
type: execute
wave: 4
depends_on: ["09-02", "09-04"]
files_modified:
  - virnucpro/pipeline/gpu_coordinator.py
  - virnucpro/pipeline/multi_gpu_inference.py
autonomous: true

must_haves:
  truths:
    - "Failed GPU workers are retried with exponential backoff (3 retries, 1s/2s/4s)"
    - "Other GPUs continue processing while failed worker retries"
    - "Coordinator manifest tracks global checkpoint state"
    - "run_multi_gpu_inference accepts checkpoint and force_restart parameters"
  artifacts:
    - path: "virnucpro/pipeline/gpu_coordinator.py"
      provides: "wait_for_completion_with_retry method for fault-tolerant worker management"
      contains: "wait_for_completion_with_retry"
    - path: "virnucpro/pipeline/multi_gpu_inference.py"
      provides: "Checkpoint-enabled multi-GPU inference entry point"
      contains: "enable_checkpointing"
  key_links:
    - from: "virnucpro/pipeline/multi_gpu_inference.py"
      to: "virnucpro/pipeline/checkpoint_manifest.py"
      via: "CheckpointManifest initialization and shard tracking"
      pattern: "CheckpointManifest"
    - from: "virnucpro/pipeline/gpu_coordinator.py"
      to: "virnucpro/pipeline/gpu_worker.py"
      via: "Worker respawn for retry with checkpoint resume"
      pattern: "spawn_worker"
---

<objective>
Add retry with exponential backoff to GPUProcessCoordinator and wire checkpoint manifest into multi_gpu_inference.

Purpose: The coordinator must handle GPU failures gracefully - retrying failed workers while others continue. The manifest tracks global progress so the orchestrator knows which shards completed and which need retry. The multi_gpu_inference entry point becomes the user-facing API for checkpointed inference.

Output: Modified `virnucpro/pipeline/gpu_coordinator.py` and `virnucpro/pipeline/multi_gpu_inference.py`.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-02-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-04-SUMMARY.md
@virnucpro/pipeline/gpu_coordinator.py
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/checkpoint_manifest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add retry logic to GPUProcessCoordinator</name>
  <files>virnucpro/pipeline/gpu_coordinator.py</files>
  <action>
Add two new methods to GPUProcessCoordinator:

1. **`spawn_worker(self, rank: int, worker_fn: Callable, worker_args: Tuple) -> None`** (single worker spawn):
   - Spawn a single worker process for the given rank
   - Same logic as spawn_workers but for one rank: creates Process via self.ctx.Process with _worker_wrapper
   - Store in self.workers[rank]
   - Log: `f"Spawned worker {rank} (CUDA_VISIBLE_DEVICES={rank})"`
   - This enables respawning individual failed workers for retry

2. **`wait_for_completion_with_retry(self, worker_fn: Callable, worker_args: Tuple, timeout: Optional[float] = None, max_retries: int = 3, base_delay: float = 1.0) -> Dict[int, bool]`**:
   - Wait for all workers to complete (same as wait_for_completion)
   - For any failed workers (exitcode != 0 or timed out):
     - Classify error type from results_queue: check for "CUDA out of memory" -> "OOM", "CUDA" -> "CUDA_ERROR", else "RUNTIME_ERROR", "spot preemption" -> "PREEMPTION"
     - Log based on error type:
       - OOM/CUDA_ERROR: full diagnostics (warning level)
       - PREEMPTION: minimal logging (info level, "expected transient failure")
       - Other: standard warning
     - Retry up to max_retries times with exponential backoff:
       - delay = min(base_delay * (2 ** attempt), 60.0) seconds (capped at 60s)
       - Log: `f"Retrying worker {rank} (attempt {attempt+1}/{max_retries}) in {delay:.1f}s"`
       - `time.sleep(delay)`
       - Respawn worker via `self.spawn_worker(rank, worker_fn, worker_args)`
       - Wait for completion (with timeout)
       - If succeeds: mark as successful, break
       - If fails again: continue to next retry
     - After all retries exhausted: log error with all attempts
   - Return completion status dict mapping rank -> bool (same as wait_for_completion)

Add imports: `import time` at top of file.

IMPORTANT: Do NOT modify existing methods (spawn_workers, wait_for_completion, collect_results, terminate_all). Only ADD new methods.

Add docstrings explaining the retry pattern and exponential backoff.
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.gpu_coordinator import GPUProcessCoordinator
assert hasattr(GPUProcessCoordinator, 'spawn_worker'), 'Missing spawn_worker'
assert hasattr(GPUProcessCoordinator, 'wait_for_completion_with_retry'), 'Missing retry method'
print('Coordinator retry methods verified')
"
  </verify>
  <done>
    GPUProcessCoordinator has spawn_worker for single-worker respawn and wait_for_completion_with_retry for exponential backoff retry logic. Error classification provides intelligent diagnostic tiering (full diagnostics for CUDA errors, minimal for preemption).
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire checkpoint manifest into multi_gpu_inference</name>
  <files>virnucpro/pipeline/multi_gpu_inference.py</files>
  <action>
Modify `run_multi_gpu_inference()` to support checkpointing:

1. **Add new parameters** to function signature:
   - `enable_checkpointing: bool = True` - enables incremental checkpointing
   - `force_restart: bool = False` - force fresh start ignoring prior checkpoints
   Keep existing parameters unchanged.

2. **Checkpoint setup** (after output_dir.mkdir, before index creation):
   ```python
   checkpoint_dir = output_dir / "checkpoints"
   if enable_checkpointing:
       checkpoint_dir.mkdir(parents=True, exist_ok=True)
       manifest = CheckpointManifest(checkpoint_dir / "manifest.json")
       if force_restart and manifest.exists():
           logger.info("Force restart: removing existing manifest")
           (checkpoint_dir / "manifest.json").unlink()
       if not manifest.exists():
           manifest.initialize(world_size)
       logger.info(f"Checkpointing enabled: {checkpoint_dir}")
   ```

3. **Pass checkpoint config in model_config** to workers:
   ```python
   model_config_with_ckpt = {
       **model_config,
       'enable_checkpointing': enable_checkpointing,
       'checkpoint_dir': str(checkpoint_dir),
       'force_restart': force_restart,
   }
   ```
   Use `model_config_with_ckpt` instead of `model_config` when spawning workers.

4. **Use retry-enabled wait**: Replace `coordinator.wait_for_completion(timeout=timeout)` with:
   ```python
   if enable_checkpointing:
       completion_status = coordinator.wait_for_completion_with_retry(
           worker_fn=gpu_worker,
           worker_args=(index_path, output_dir, model_config_with_ckpt),
           timeout=timeout,
           max_retries=3,
           base_delay=1.0
       )
   else:
       completion_status = coordinator.wait_for_completion(timeout=timeout)
   ```

5. **Update manifest after completion**: After identifying successful/failed ranks:
   ```python
   if enable_checkpointing:
       for rank in successful_ranks:
           manifest.mark_shard_complete(rank)
       for rank in failed_ranks:
           manifest.mark_shard_failed(rank, f"Worker {rank} failed after retries")
       progress = manifest.get_global_progress()
       logger.info(f"Checkpoint progress: {progress}")
   ```

6. **Update run_esm2_multi_gpu**: Add `enable_checkpointing: bool = True, force_restart: bool = False` parameters, pass through to run_multi_gpu_inference.

Add imports:
- `from virnucpro.pipeline.checkpoint_manifest import CheckpointManifest`

Update docstrings for both functions to document the new parameters.

IMPORTANT: Existing behavior when enable_checkpointing=False must be identical to current behavior. The default of True means NEW callers get checkpointing, but passing False reproduces old behavior exactly.
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference, run_esm2_multi_gpu
import inspect
sig = inspect.signature(run_multi_gpu_inference)
assert 'enable_checkpointing' in sig.parameters, 'Missing enable_checkpointing'
assert 'force_restart' in sig.parameters, 'Missing force_restart'
print('Multi-GPU checkpoint integration verified')
"
  </verify>
  <done>
    run_multi_gpu_inference supports enable_checkpointing and force_restart parameters. Manifest initialized before worker spawn, updated after completion. Retry logic used when checkpointing enabled. Backward compatible when checkpointing disabled.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.gpu_coordinator import GPUProcessCoordinator"` - import succeeds
- `python -c "from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference"` - import succeeds
- GPUProcessCoordinator has spawn_worker and wait_for_completion_with_retry
- run_multi_gpu_inference accepts enable_checkpointing and force_restart
- Existing tests pass: `pytest tests/unit/test_gpu_coordinator.py tests/unit/test_multi_gpu_inference.py -v`
</verification>

<success_criteria>
Coordinator retries failed workers with exponential backoff (1s, 2s, 4s delays, 3 attempts max). Multi-GPU inference initializes manifest, passes checkpoint config to workers, uses retry logic, and updates manifest on completion. Both functions backward compatible when checkpointing disabled.
</success_criteria>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-05-SUMMARY.md`
</output>
