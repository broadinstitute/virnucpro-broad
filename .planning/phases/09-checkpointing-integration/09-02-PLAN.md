---
phase: 09-checkpointing-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/checkpoint_manifest.py
autonomous: true

must_haves:
  truths:
    - "Coordinator manifest tracks checkpoint progress per shard with global metadata"
    - "Manifest uses atomic writes (temp + rename) with backup and corruption recovery"
    - "Manifest can identify which shards completed, failed, or are stale (zombie detection)"
    - "Manifest supports partial failure recovery with elastic shard redistribution"
    - "Manifest supports global checkpoint barriers for consistent sync points"
    - "Manifest validates checkpoint files exist before updating entries"
    - "Manifest coordinates via file-system atomicity, not in-process locks"
    - "Manifest provides detailed resume info per shard (batch, sequences, status)"
    - "Manifest supports archival and cleanup after successful completion"
  artifacts:
    - path: "virnucpro/pipeline/checkpoint_manifest.py"
      provides: "CheckpointManifest class for multi-GPU checkpoint coordination with fault tolerance"
      min_lines: 400
  key_links:
    - from: "virnucpro/pipeline/checkpoint_manifest.py"
      to: "json"
      via: "JSON manifest file for human-readable tracking with corruption recovery"
      pattern: "json\\.(load|dump)"
    - from: "virnucpro/pipeline/checkpoint_manifest.py"
      to: "fcntl"
      via: "POSIX file locking for cross-process synchronization"
      pattern: "fcntl\\.flock"
    - from: "virnucpro/pipeline/checkpoint_manifest.py"
      to: "virnucpro/pipeline/checkpoint_writer.py"
      via: "AsyncCheckpointWriter calls update_shard_checkpoint after successful write"
      pattern: "update_shard_checkpoint"
---

<objective>
Create the CheckpointManifest for multi-GPU checkpoint coordination with fault tolerance, elastic redistribution, and corruption recovery.

Purpose: In multi-GPU mode, each shard writes checkpoints independently via separate spawned processes. The manifest provides a global view of progress, enabling partial failure recovery (restart only failed GPUs), elastic shard redistribution (reassign orphaned work to healthy GPUs), staleness detection (identify zombie workers), global checkpoint barriers (consistent sync points), and completion validation. This is the coordination layer that connects independent per-shard checkpoints. The manifest tracks .pt checkpoint files (consistent with Phase 3 format decision).

The manifest uses file-system coordination (POSIX file locks + atomic rename) rather than in-process threading locks, because GPU workers run as separate spawned processes with independent memory spaces.

Output: `virnucpro/pipeline/checkpoint_manifest.py` with CheckpointManifest class and ManifestCorruptedError exception.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-RESEARCH.md
@.planning/phases/09-checkpointing-integration/09-01-PLAN.md
@virnucpro/pipeline/gpu_coordinator.py
@virnucpro/pipeline/multi_gpu_inference.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: CheckpointManifest class with fault tolerance and coordination</name>
  <files>virnucpro/pipeline/checkpoint_manifest.py</files>
  <action>
Create `virnucpro/pipeline/checkpoint_manifest.py` with:

**Module docstring** (must include Concurrency Model section):
```
"""Coordinator manifest for multi-GPU checkpoint tracking.

Tracks per-shard checkpoint progress to enable partial failure recovery,
elastic shard redistribution, and global completeness validation.
Uses JSON format consistent with Phase 7 sequence_index.json.
Checkpoint files referenced in manifest use .pt format (Phase 3 torch.save pattern).

## Concurrency Model

CheckpointManifest assumes a single coordinator process that initializes the
manifest and reads global state, plus multiple worker processes that write
per-shard checkpoints independently. Workers DO NOT directly update the
manifest — AsyncCheckpointWriter integration (Plan 09-01) handles this via
the writer's _write_checkpoint_sync method, which calls
manifest.update_shard_checkpoint() AFTER the checkpoint file and .done marker
are successfully written.

For cross-process safety, all manifest mutations use POSIX file locking
(fcntl.flock) to prevent concurrent writes from corrupting the JSON file.
This replaces the original threading.Lock design which was process-local and
provided no cross-process coordination.

For single-coordinator scenarios (typical), the coordinator is the only
writer and workers only write per-shard checkpoint files. The coordinator
periodically calls get_global_progress() to monitor worker health via
checkpoint timestamps.
"""
```

**ManifestCorruptedError** exception class:
- `class ManifestCorruptedError(Exception)` at module level
- Docstring: "Raised when manifest JSON cannot be loaded from primary, .tmp, or .backup files."
- Store `manifest_path` attribute for recovery instructions

**CheckpointManifest** class:

`__init__(self, manifest_path: Path, staleness_threshold_sec: int = 600)`:
- Store `manifest_path` as `Path`
- Store `staleness_threshold_sec` (default 600 = 10 minutes, used for zombie shard detection)
- Create `self._lock_path = manifest_path.with_suffix('.lock')` for cross-process file locking
- Do NOT create `threading.Lock` — use file-based locking for cross-process safety

`_acquire_file_lock(self) -> int`:
- Open `self._lock_path` with `open(str(self._lock_path), 'w')` and store the file descriptor
- Call `fcntl.flock(fd, fcntl.LOCK_EX)` for exclusive POSIX lock
- Return the file descriptor (caller must pass to `_release_file_lock`)
- Import `fcntl` at module level (POSIX only — this is a Linux HPC pipeline)

`_release_file_lock(self, fd: int)`:
- Call `fcntl.flock(fd, fcntl.LOCK_UN)`
- Call `os.close(fd)`

`initialize(self, world_size: int, input_fingerprint: str = "", model_config_hash: str = "") -> Dict`:
- Acquire file lock
- Create initial manifest JSON structure:
  ```json
  {
    "version": "2.0",
    "world_size": N,
    "created_at": "ISO timestamp",
    "input_fingerprint": "<SHA256 of concatenated FASTA files>",
    "model_config_hash": "<hash of model dtype/architecture/weights>",
    "global_checkpoints": [],
    "shards": {
      "0": {
        "status": "in_progress",
        "original_rank": 0,
        "assigned_rank": 0,
        "last_checkpoint_batch": -1,
        "total_sequences": 0,
        "last_checkpoint_time": null,
        "retry_count": 0,
        "error": null,
        "checkpoints": []
      },
      "1": { ... },
      ...
    }
  }
  ```
- Write to disk via `_save_manifest` (creates .backup before overwriting)
- Release file lock
- Return the manifest dict
- Note: `input_fingerprint` and `model_config_hash` are set by coordinator before spawning workers

`set_global_metadata(self, input_fingerprint: str, model_config_hash: str)`:
- Acquire file lock
- Load manifest
- Set `manifest["input_fingerprint"]` and `manifest["model_config_hash"]`
- Save manifest
- Release file lock
- Called by coordinator before spawning workers if not set during initialize()

`validate_run_compatibility(self, input_fingerprint: str, model_config_hash: str) -> Tuple[bool, List[str]]`:
- Load manifest (no lock needed, read-only)
- Compare `input_fingerprint` against manifest's. If different: add warning "Input fingerprint mismatch: manifest={X}, current={Y} — FASTA files may have changed"
- Compare `model_config_hash` against manifest's. If different: add warning "Model config hash mismatch: manifest={X}, current={Y} — model weights or config may have changed"
- Return `(is_compatible, warnings)` where `is_compatible = len(warnings) == 0`

`update_shard_checkpoint(self, rank: int, batch_idx: int, num_sequences: int, checkpoint_file: str, sequence_range: str = "")`:
- Acquire file lock
- Load current manifest
- VALIDATE: checkpoint file exists before updating. Build expected path: `self.manifest_path.parent / f"shard_{rank}" / checkpoint_file`. If path does not exist, release lock and raise `FileNotFoundError(f"Checkpoint {checkpoint_file} not found at expected path, cannot update manifest")`
- Update shard entry for `str(rank)`:
  - `last_checkpoint_batch = batch_idx`
  - `total_sequences += num_sequences`
  - `last_checkpoint_time = datetime.utcnow().isoformat()`
  - Append to `checkpoints` list: `{"batch_idx": batch_idx, "num_sequences": num_sequences, "file": checkpoint_file, "timestamp": "ISO", "sequence_range": sequence_range}`
  - The `sequence_range` field is informational only (e.g., "seq_4200-seq_4299"), helps debug which sequences were in a failed checkpoint
- Save manifest
- Release file lock

`mark_shard_complete(self, rank: int)`:
- Acquire file lock
- Load manifest, set `shards[str(rank)]["status"] = "complete"`, add `completed_at` timestamp
- Save manifest, release file lock

`mark_shard_failed(self, rank: int, error: str)`:
- Acquire file lock
- Load manifest
- Set `shards[str(rank)]["status"] = "failed"`, add `failed_at` timestamp, store `error` message
- Increment `retry_count` by 1
- Save manifest, release file lock

`get_shard_status(self, rank: int) -> Optional[Dict]`:
- Load manifest (no lock, read-only)
- Return the shard dict for given rank, or None if rank not found

`get_incomplete_shard_ranks(self) -> List[int]`:
- Load manifest (no lock, read-only)
- Return list of ranks where status is "in_progress" or "failed" (these need reprocessing)
- Replaces the old `get_resumable_shards` name for clarity

`get_resumable_shards(self) -> List[int]`:
- Alias for `get_incomplete_shard_ranks()` — preserves backward compatibility

`get_resume_info(self, rank: int) -> Dict`:
- Load manifest (no lock, read-only)
- Return detailed resume information for a specific shard:
  ```python
  {
      "rank": rank,
      "resume_from_batch": shard["last_checkpoint_batch"] + 1,
      "last_completed_batch": shard["last_checkpoint_batch"],
      "total_sequences_completed": shard["total_sequences"],
      "status": shard["status"],
      "assigned_rank": shard["assigned_rank"],
      "retry_count": shard["retry_count"],
      "last_checkpoint_time": shard["last_checkpoint_time"]
  }
  ```
- Returns empty dict with `"rank": rank, "status": "unknown"` if rank not found

`get_completed_shards(self) -> List[int]`:
- Load manifest (no lock, read-only)
- Return list of ranks where status is "complete"

`get_global_progress(self) -> Dict`:
- Load manifest (no lock, read-only)
- Compute staleness: for each shard with status "in_progress", if `last_checkpoint_time` is not null, check `(now - last_checkpoint_time).total_seconds() > self.staleness_threshold_sec`. Parse ISO timestamp with `datetime.fromisoformat()`.
- Return summary dict:
  ```python
  {
      "total_shards": world_size,
      "completed": count,
      "in_progress": count,
      "failed": count,
      "total_sequences_checkpointed": sum across all shards,
      "stale_shards": [list of rank ints exceeding staleness threshold],
      "input_fingerprint": manifest["input_fingerprint"],
      "model_config_hash": manifest["model_config_hash"]
  }
  ```

`is_shard_stale(self, rank: int, threshold_sec: Optional[int] = None) -> bool`:
- Load manifest (no lock, read-only)
- Use `threshold_sec` if provided, else `self.staleness_threshold_sec`
- Return True if shard status is "in_progress" AND last_checkpoint_time is not null AND elapsed time exceeds threshold
- Return False if shard not found, status is not "in_progress", or last_checkpoint_time is null (hasn't checkpointed yet — not necessarily stale, could be starting up)

`reassign_shard(self, shard_rank: int, new_assigned_rank: int)`:
- Acquire file lock
- Load manifest
- Update `shards[str(shard_rank)]["assigned_rank"] = new_assigned_rank`
- Log: `"Shard {shard_rank} reassigned from rank {old} to rank {new_assigned_rank}"`
- Save manifest, release file lock
- Note: `original_rank` is immutable (never changes), `assigned_rank` tracks who is currently responsible

`get_orphaned_shards(self) -> List[int]`:
- Load manifest (no lock, read-only)
- Return list of shard rank ints where `status == "failed"` AND `retry_count >= 3` (max retries exhausted)
- These shards are candidates for redistribution to healthy GPUs

`record_global_checkpoint(self, checkpoint_id: str, batch_boundaries: Dict[int, int])`:
- Acquire file lock
- Load manifest
- Append to `manifest["global_checkpoints"]`:
  ```python
  {
      "checkpoint_id": checkpoint_id,
      "timestamp": datetime.utcnow().isoformat(),
      "batch_boundaries": {str(rank): batch_idx for rank, batch_idx in batch_boundaries.items()}
  }
  ```
- Save manifest, release file lock
- Called by coordinator when all shards reach compatible batch boundaries (globally consistent sync point)

`get_latest_global_checkpoint(self) -> Optional[Dict]`:
- Load manifest (no lock, read-only)
- Return last element of `manifest["global_checkpoints"]`, or None if empty

`archive_manifest(self, archive_dir: Path)`:
- Load manifest (no lock, read-only)
- Validate all shards have status "complete". If any are not complete, raise `ValueError(f"Cannot archive: shards {incomplete} are not complete")`
- Create `archive_dir` if it doesn't exist (`archive_dir.mkdir(parents=True, exist_ok=True)`)
- Copy manifest to `archive_dir / "manifest_final.json"` using `shutil.copy2`
- Log: `"Manifest archived to {archive_dir}/manifest_final.json"`

`cleanup_checkpoints(self, keep_final_only: bool = True)`:
- Load manifest (no lock, read-only)
- For each shard, iterate through `checkpoints` list
- If `keep_final_only`: remove all per-batch .pt files EXCEPT the last checkpoint per shard. Build path as `self.manifest_path.parent / f"shard_{rank}" / checkpoint["file"]`. Also remove corresponding .done markers.
- If NOT `keep_final_only`: remove ALL per-batch .pt files and .done markers
- Log number of files removed per shard
- Does NOT remove shard directories or the manifest itself

`_load_manifest(self) -> Dict`:
- Try loading JSON from `self.manifest_path`
- On `json.JSONDecodeError`: log error, try loading from `.tmp` file (partial atomic write may have left it)
- On second `json.JSONDecodeError`: try loading from `.backup` file
- If all three fail: raise `ManifestCorruptedError(f"Manifest corrupted: primary, .tmp, and .backup all failed to parse. Path: {self.manifest_path}")` with `manifest_path` attribute set
- Return parsed dict on success

`_save_manifest(self, manifest: Dict)`:
- Create backup BEFORE overwriting: if `self.manifest_path.exists()`, copy to `self.manifest_path.with_suffix('.backup')` using `shutil.copy2`
- Atomic write: write to `self.manifest_path.with_suffix('.tmp')` using `json.dump` with `indent=2`
- Rename: `temp_path.replace(self.manifest_path)`
- Note: .tmp file is left as-is if rename succeeds (serves as secondary recovery source)

`exists(self) -> bool`:
- Return `self.manifest_path.exists()`

**Module-level logger:** `logger = logging.getLogger('virnucpro.pipeline.checkpoint_manifest')`

**Imports:**
- `import json, logging, os, fcntl, shutil, time` from stdlib
- `from pathlib import Path` from stdlib
- `from datetime import datetime` from stdlib
- `from typing import Dict, List, Optional, Tuple` from typing

Do NOT import threading — this module uses file-based locking for cross-process safety, not in-process thread locks.
Do NOT import torch — this module only handles JSON manifest metadata, not tensor data.
  </action>
  <verify>
    python -c "from virnucpro.pipeline.checkpoint_manifest import CheckpointManifest, ManifestCorruptedError; print('Import successful')"
  </verify>
  <done>
    CheckpointManifest importable with ManifestCorruptedError. Manifest schema version 2.0 includes:
    - Global metadata (input_fingerprint, model_config_hash) with run compatibility validation
    - Elastic shard redistribution (assigned_rank vs original_rank, reassign_shard, get_orphaned_shards)
    - Staleness detection (is_shard_stale, stale_shards in get_global_progress)
    - Checkpoint file existence validation before manifest update
    - Global checkpoint barriers (record_global_checkpoint, get_latest_global_checkpoint)
    - Detailed resume info (get_resume_info with batch, sequences, status, retry_count)
    - Corruption recovery (_load_manifest tries primary, .tmp, .backup; raises ManifestCorruptedError)
    - Sequence range tracking in checkpoint entries (informational, for debugging)
    - Cross-process file locking via fcntl.flock (not threading.Lock)
    - Concurrency model documented in module docstring
    - Archive and cleanup methods (archive_manifest, cleanup_checkpoints)
    - Backup creation before every save (_save_manifest creates .backup)
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.checkpoint_manifest import CheckpointManifest, ManifestCorruptedError"`
- CheckpointManifest: verify initialize creates JSON with version 2.0, input_fingerprint, model_config_hash, global_checkpoints, and shard entries with original_rank/assigned_rank/retry_count
- Verify file-based locking pattern (fcntl.flock, not threading.Lock)
- Verify _save_manifest creates .backup before overwriting
- Verify _load_manifest tries primary -> .tmp -> .backup -> ManifestCorruptedError
- Verify update_shard_checkpoint validates checkpoint file exists before updating manifest
- Verify get_global_progress returns stale_shards list
- Verify get_resume_info returns detailed dict with resume_from_batch, assigned_rank, retry_count
- Verify reassign_shard updates assigned_rank but not original_rank
- Verify get_orphaned_shards returns shards where failed AND retry_count >= 3
- Verify record_global_checkpoint/get_latest_global_checkpoint work correctly
- Verify archive_manifest validates all shards complete before archiving
- Verify cleanup_checkpoints removes per-batch .pt files and .done markers
- Verify validate_run_compatibility checks input_fingerprint and model_config_hash
- Verify module docstring includes Concurrency Model section
</verification>

<success_criteria>
CheckpointManifest importable with ManifestCorruptedError exception. Manifest schema version 2.0 supports: global metadata tracking (input fingerprint, model config hash) with run compatibility validation; elastic shard redistribution via assigned_rank/original_rank split with reassign_shard and get_orphaned_shards; staleness detection for zombie shards via configurable threshold; checkpoint file existence validation before manifest updates; global checkpoint barriers for consistent sync points; detailed per-shard resume info (batch index, sequence count, status, retry count); JSON corruption recovery via fallback chain (primary -> .tmp -> .backup -> ManifestCorruptedError); sequence range tracking in checkpoint entries for debugging; cross-process file locking via fcntl.flock; documented concurrency model in module docstring; and archive/cleanup methods for post-completion maintenance. All mutation methods use POSIX file locks; read-only methods operate without locks.
</success_criteria>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-02-SUMMARY.md`
</output>
