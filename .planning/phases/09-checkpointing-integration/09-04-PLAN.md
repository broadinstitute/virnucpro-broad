---
phase: 09-checkpointing-integration
plan: 04
type: execute
wave: 3
depends_on: ["09-01", "09-02", "09-03"]
files_modified:
  - virnucpro/pipeline/gpu_worker.py
  - virnucpro/data/sequence_dataset.py
autonomous: true

must_haves:
  truths:
    - "GPU worker creates incremental .pt checkpoints during inference"
    - "Worker resumes from existing checkpoints without reprocessing completed sequences"
    - "Worker final shard is assembled from checkpointed + newly processed sequences"
    - "Per-GPU checkpoint isolation via shard_N subdirectories prevents conflicts"
    - "DataLoader skips already-processed sequences identified from checkpoint resume"
    - "SIGTERM handler saves emergency checkpoint before spot instance eviction"
    - "Error handling differentiates spot preemption, OOM, and poison inputs"
    - "Manifest updates track per-shard checkpoint progress"
  artifacts:
    - path: "virnucpro/pipeline/gpu_worker.py"
      provides: "gpu_worker with checkpoint_dir, force_restart, error handling tiers, SIGTERM handler"
      contains: "checkpoint_dir"
    - path: "virnucpro/data/sequence_dataset.py"
      provides: "IndexBasedDataset with skip_sequence_ids for resume"
      contains: "skip_sequence_ids"
  key_links:
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/pipeline/async_inference.py"
      via: "AsyncInferenceRunner with checkpoint_dir parameter"
      pattern: "AsyncInferenceRunner.*checkpoint_dir"
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/pipeline/checkpoint_writer.py"
      via: "resume_from_checkpoints for pre-inference resume"
      pattern: "resume_from_checkpoints"
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/pipeline/checkpoint_manifest.py"
      via: "CheckpointManifest passed to AsyncInferenceRunner for coordination"
      pattern: "CheckpointManifest"
    - from: "virnucpro/data/sequence_dataset.py"
      to: "skip_sequence_ids"
      via: "Dataset filters out already-processed sequences during iteration"
      pattern: "skip_sequence_ids"
---

<objective>
Wire checkpointing into the GPU worker function so each GPU independently checkpoints, resumes, and handles failures.

Purpose: gpu_worker is the per-GPU entry point spawned by GPUProcessCoordinator. It must:
1. Resume from existing checkpoints and tell the DataLoader to SKIP already-processed sequences
2. Pass checkpoint configuration to AsyncInferenceRunner
3. Assemble final shard from resumed + new embeddings
4. Handle SIGTERM (spot preemption) by saving emergency checkpoint
5. Differentiate error handling: spot (retry infinitely), OOM (reduce batch), poison (circuit breaker)
6. Update manifest to track per-shard progress

Output: Modified `virnucpro/pipeline/gpu_worker.py` with checkpoint support and `virnucpro/data/sequence_dataset.py` with skip logic.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-01-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-02-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-03-SUMMARY.md
@virnucpro/pipeline/gpu_worker.py
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/checkpoint_writer.py
@virnucpro/pipeline/checkpoint_manifest.py
@virnucpro/data/sequence_dataset.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add skip_sequence_ids to IndexBasedDataset</name>
  <files>virnucpro/data/sequence_dataset.py</files>
  <action>
Modify `IndexBasedDataset` to support skipping already-processed sequences:

1. **Add skip_sequence_ids parameter to __init__**:
   ```python
   def __init__(
       self,
       index: 'SequenceIndex',
       skip_sequence_ids: Optional[Set[str]] = None
   ):
       # ... existing initialization ...
       self.skip_sequence_ids = skip_sequence_ids or set()
       logger.info(f"Dataset initialized: {len(self)} sequences, skipping {len(self.skip_sequence_ids)}")
   ```

2. **Filter during __getitem__**:
   - When yielding a sequence in `__getitem__`, check if `seq_id` is in `self.skip_sequence_ids`
   - If yes: skip to next sequence in the same file (don't yield this one)
   - This prevents already-processed sequences from entering the DataLoader

   Implementation approach (modify __getitem__):
   ```python
   def __getitem__(self, idx: int) -> Tuple[str, str]:
       entry = self.index[idx]
       seq_id, seq = self._read_sequence(entry)

       # Skip already-processed sequences
       if seq_id in self.skip_sequence_ids:
           # Return a sentinel value or raise IndexError to skip
           # Cleaner approach: filter during __len__ and maintain filtered index
           # But that requires re-indexing. Simpler: return None and let collator handle
           # ACTUALLY: Best approach is to filter the index BEFORE dataset creation
           # See Task 2 for pre-filtering in gpu_worker
           pass

       return seq_id, seq
   ```

   **CORRECTION**: Filtering in __getitem__ is inefficient and breaks DataLoader batching. Better approach:
   - Accept filtered index in __init__ instead of raw index
   - gpu_worker pre-filters the index to remove already-processed entries
   - Dataset sees only unprocessed sequences

   Revised implementation:
   - Remove skip_sequence_ids from Dataset (moved to gpu_worker filtering logic)
   - Dataset stays simple and efficient
   - gpu_worker builds filtered index before creating dataset

REVISED: Actually, keep it simple - add a `filter_index()` helper method to SequenceIndex (in Task 2) and gpu_worker uses it. Dataset doesn't need modification.

REVERT this task - filtering happens at the SequenceIndex level in gpu_worker, not Dataset level.
  </action>
  <verify>
    python -c "from virnucpro.data.sequence_dataset import IndexBasedDataset; print('Dataset unchanged, filtering moved to gpu_worker')"
  </verify>
  <done>
    IndexBasedDataset unchanged - sequence filtering moved to gpu_worker's index filtering logic for efficiency.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add checkpoint config and resume logic to gpu_worker</name>
  <files>virnucpro/pipeline/gpu_worker.py</files>
  <action>
Modify `gpu_worker()` function to support checkpointing and resume:

**Step 1: Extract checkpoint config** (after CUDA init, before index loading):
```python
# Checkpoint configuration
enable_checkpointing = model_config.get('enable_checkpointing', True)
force_restart = model_config.get('force_restart', False)
checkpoint_base_dir = Path(model_config.get('checkpoint_dir', output_dir / "checkpoints"))
checkpoint_seq_threshold = model_config.get('checkpoint_seq_threshold', 10000)
checkpoint_time_threshold = model_config.get('checkpoint_time_threshold', 300.0)

# Per-shard checkpoint isolation (Issue 8)
checkpoint_dir = checkpoint_base_dir / f"shard_{rank}"
checkpoint_dir.mkdir(parents=True, exist_ok=True)

logger.info(
    f"Rank {rank}: Checkpointing enabled={enable_checkpointing}, "
    f"dir={checkpoint_dir}, force_restart={force_restart}"
)
```

**Step 2: Load manifest** (if checkpointing enabled):
```python
manifest = None
if enable_checkpointing:
    from virnucpro.pipeline.checkpoint_manifest import CheckpointManifest
    manifest_path = checkpoint_base_dir / "manifest.json"
    try:
        manifest = CheckpointManifest.load(manifest_path)
        logger.info(f"Rank {rank}: Loaded checkpoint manifest from {manifest_path}")
    except FileNotFoundError:
        # First run or fresh start - coordinator will create manifest
        logger.info(f"Rank {rank}: No existing manifest, will create on first checkpoint")
```

**Step 3: Resume from checkpoints** (before loading index):
```python
resumed_ids = set()
resumed_embeddings = None
if enable_checkpointing and not force_restart:
    from virnucpro.pipeline.checkpoint_writer import resume_from_checkpoints

    logger.info(f"Rank {rank}: Checking for existing checkpoints in {checkpoint_dir}")
    resumed_ids_list, resumed_embs, resume_batch_idx, corrupted_ids = resume_from_checkpoints(
        checkpoint_dir, rank, force_restart
    )

    if corrupted_ids:
        logger.warning(
            f"Rank {rank}: Checkpoint corruption detected - {len(corrupted_ids)} sequences "
            f"need reprocessing (from batches after corruption point)"
        )
        # Corrupted sequences will be reprocessed (not in resumed_ids)

    if resumed_ids_list:
        resumed_ids = set(resumed_ids_list)
        resumed_embeddings = resumed_embs  # numpy array
        logger.info(
            f"Rank {rank}: Resuming from {resume_batch_idx} checkpoints, "
            f"{len(resumed_ids)} sequences already processed"
        )
```

**Step 4: Filter index to skip resumed sequences** (after loading index):
```python
# Load sequence index
index = SequenceIndex.load(index_path)

# Filter out already-processed sequences (Issue 5 - prevent duplicates)
if resumed_ids:
    original_count = len(index)
    # Create filtered index containing only unprocessed sequences
    filtered_indices = [
        i for i in range(len(index))
        if index[i]['sequence_id'] not in resumed_ids
    ]

    # Build new SequenceIndex with filtered entries
    from virnucpro.data.shard_index import SequenceIndex
    filtered_index = SequenceIndex(
        entries=[index[i] for i in filtered_indices],
        metadata=index.metadata
    )

    logger.info(
        f"Rank {rank}: Filtered index from {original_count} to {len(filtered_index)} sequences "
        f"({len(resumed_ids)} already processed)"
    )
    index = filtered_index
```

**Step 5: Pass checkpoint config to AsyncInferenceRunner**:
```python
# Compute input fingerprint for cross-run validation
import hashlib
input_fingerprint = hashlib.sha256(str(index_path).encode()).hexdigest()[:16]

runner = AsyncInferenceRunner(
    model, device,
    checkpoint_dir=checkpoint_dir if enable_checkpointing else None,
    rank=rank,
    checkpoint_seq_threshold=checkpoint_seq_threshold,
    checkpoint_time_threshold=checkpoint_time_threshold,
    manifest=manifest,
    input_fingerprint=input_fingerprint,
)
```

**Step 6: Run inference (no special resume handling needed)**:
```python
# Inference loop - runner handles checkpointing internally
for result in runner.run(dataloader):
    # Process normally - no batch_idx == -1 checks (Issue 2 fixed)
    all_embeddings.append(result.embeddings)
    all_ids.extend(result.sequence_ids)
```

**Step 7: Assemble final shard from resumed + new**:
```python
# Combine resumed embeddings with new ones
if resumed_embeddings is not None:
    logger.info(f"Rank {rank}: Merging {len(resumed_ids)} resumed + {len(all_ids)} new sequences")
    # Resumed embeddings are numpy, new are torch tensors
    all_embeddings.insert(0, torch.from_numpy(resumed_embeddings))
    all_ids = list(resumed_ids) + all_ids

# Concatenate all embeddings
if all_embeddings:
    final_embeddings = torch.cat(all_embeddings, dim=0)
else:
    final_embeddings = torch.empty((0, model.config.hidden_size))

logger.info(f"Rank {rank}: Final shard has {len(all_ids)} sequences")
```

**Step 8: Add SIGTERM handler** (Issue 7 - spot preemption):
```python
import signal

def sigterm_handler(signum, frame):
    logger.warning(f"Rank {rank}: SIGTERM received (spot preemption), saving emergency checkpoint")
    if enable_checkpointing and runner:
        # Trigger emergency checkpoint
        runner._write_checkpoint(reason="emergency_sigterm")
        runner.writer.wait_all(timeout=30)
    sys.exit(143)  # Standard SIGTERM exit code

# Register handler at worker start (after checkpoint_dir setup)
if enable_checkpointing:
    signal.signal(signal.SIGTERM, sigterm_handler)
```

**Step 9: Differentiated error handling** (Issue 6):
Wrap the main inference logic in try/except with tiered handling:

```python
try:
    # ... main inference logic ...

except torch.cuda.OutOfMemoryError as e:
    # OOM: Reduce batch size and retry (not implemented here, needs DataLoader config)
    logger.error(
        f"Rank {rank}: CUDA OOM error - {e}\n"
        f"GPU memory: {torch.cuda.memory_allocated(device) / 1e9:.2f}GB allocated, "
        f"{torch.cuda.max_memory_allocated(device) / 1e9:.2f}GB peak"
    )
    result_status = {
        'rank': rank,
        'status': 'failed',
        'error': 'cuda_oom',
        'error_message': str(e),
        'retry_recommended': True,
        'reduce_batch_size': True,
    }
    results_queue.put(result_status)
    sys.exit(1)

except RuntimeError as e:
    if 'CUDA' in str(e) or 'assert' in str(e).lower():
        # CUDA error or assertion - likely poison input
        logger.error(
            f"Rank {rank}: CUDA runtime error (possible poison input) - {e}\n"
            f"Last batch info: {len(all_ids)} sequences processed so far"
        )
        result_status = {
            'rank': rank,
            'status': 'failed',
            'error': 'cuda_runtime',
            'error_message': str(e),
            'retry_recommended': True,
            'circuit_breaker': True,  # Trigger circuit breaker after 2 attempts
        }
    else:
        # Generic runtime error
        logger.error(f"Rank {rank}: Runtime error - {e}", exc_info=True)
        result_status = {
            'rank': rank,
            'status': 'failed',
            'error': 'runtime',
            'error_message': str(e),
            'retry_recommended': True,
        }
    results_queue.put(result_status)
    sys.exit(1)

except Exception as e:
    # Unexpected error - full diagnostics
    logger.error(
        f"Rank {rank}: Unexpected error during inference",
        exc_info=True
    )
    result_status = {
        'rank': rank,
        'status': 'failed',
        'error': 'unexpected',
        'error_message': str(e),
        'retry_recommended': False,
    }
    results_queue.put(result_status)
    sys.exit(1)
```

**Step 10: Update success status**:
```python
result_status = {
    'rank': rank,
    'status': 'complete',
    'shard_path': str(shard_path),
    'num_sequences': len(all_ids),
    'checkpointing_enabled': enable_checkpointing,
    'resumed_sequences': len(resumed_ids) if resumed_ids else 0,
}
```

IMPORTANT: The gpu_worker signature does NOT change - checkpoint config flows through model_config dict.
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.gpu_worker import gpu_worker
import inspect
sig = inspect.signature(gpu_worker)
params = list(sig.parameters.keys())
assert params == ['rank', 'world_size', 'results_queue', 'index_path', 'output_dir', 'model_config'], f'Signature changed: {params}'
print('✓ gpu_worker signature preserved')

# Verify imports
src = inspect.getsource(gpu_worker)
assert 'resume_from_checkpoints' in src, 'Missing resume_from_checkpoints import'
assert 'CheckpointManifest' in src, 'Missing CheckpointManifest import'
assert 'signal.signal' in src, 'Missing SIGTERM handler'
assert 'OutOfMemoryError' in src, 'Missing OOM error handling'
assert 'shard_{rank}' in src or f'shard_{{rank}}' in src, 'Missing per-shard isolation'
assert 'filtered_index' in src or 'filtered_indices' in src, 'Missing index filtering for resume'
print('✓ Checkpoint integration complete: resume, SIGTERM, error tiers, manifest')
"
  </verify>
  <done>
    gpu_worker resumes from checkpoints, filters index to skip processed sequences (prevents duplicates), passes checkpoint config to AsyncInferenceRunner with manifest integration, assembles final shard from resumed + new embeddings, handles SIGTERM for spot preemption, differentiates error handling (OOM/CUDA/generic), and updates manifest. Function signature preserved for pickle compatibility.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.gpu_worker import gpu_worker"` - import succeeds
- gpu_worker signature unchanged (pickle compatibility)
- Checkpoint config extracted from model_config dict
- Resume logic loads checkpoints and filters index to prevent duplicate processing
- AsyncInferenceRunner receives checkpoint_dir, manifest, and input_fingerprint
- Final shard assembled from resumed + new embeddings (no batch_idx == -1 checks)
- SIGTERM handler saves emergency checkpoint before exit
- Error handling differentiates OOM, CUDA runtime, and generic errors
- Manifest passed to AsyncInferenceRunner for coordination
- Per-shard checkpoint isolation via shard_N subdirectories
- Existing tests pass: `pytest tests/unit/test_gpu_worker.py -v`
</verification>

<success_criteria>
gpu_worker resumes from existing checkpoints and tells the index to skip already-processed sequences, preventing duplicates. Checkpoint configuration flows through model_config to AsyncInferenceRunner with manifest integration for multi-GPU coordination. Final shard is assembled from resumed + new embeddings without fragile batch_idx markers. SIGTERM handler enables graceful shutdown for spot instances. Error handling differentiates spot preemption, OOM, and poison inputs for appropriate retry strategies. Worker signature preserved for pickle compatibility. Existing functionality works identically when checkpointing disabled.
</success_criteria>

<notes>
**Key changes from original plan:**

1. **Fixed HDF5 → .pt** (Issue 1): All references now use .pt format consistent with Plan 01
2. **Removed batch_idx == -1** (Issue 2): Resume happens once at start, not intermixed with inference loop
3. **Fixed duplicate processing** (Issue 5): Index filtering ensures DataLoader only sees unprocessed sequences
4. **Added manifest integration** (Issue 4): Manifest passed to AsyncInferenceRunner for automatic coordination
5. **Added SIGTERM handler** (Issue 7): Saves emergency checkpoint on spot preemption
6. **Added error tiers** (Issue 6): Differentiates OOM, CUDA, and generic errors for retry strategies
7. **Per-shard isolation** (Issue 8): checkpoint_dir = base_dir / f"shard_{rank}"
8. **AsyncCheckpointWriter lifecycle** (Issue 10): Instantiated by AsyncInferenceRunner (Plan 03), gpu_worker just passes config

**Architecture:**
- Resume → Filter index → Create dataset → Run inference → Merge embeddings
- Clean separation: resume happens BEFORE inference, not DURING
- No fake results, no fragile markers
- DataLoader sees only unprocessed sequences (no duplicates)

**Manifest updates:**
- AsyncCheckpointWriter calls manifest.update_shard_checkpoint() after each successful write (from Plan 01 revised API)
- gpu_worker doesn't directly update manifest - handled automatically via writer integration
</notes>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-04-SUMMARY.md`
</output>
