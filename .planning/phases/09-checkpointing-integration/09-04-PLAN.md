---
phase: 09-checkpointing-integration
plan: 04
type: execute
wave: 3
depends_on: ["09-01", "09-03"]
files_modified:
  - virnucpro/pipeline/gpu_worker.py
autonomous: true

must_haves:
  truths:
    - "GPU worker creates incremental checkpoints during inference"
    - "Worker resumes from existing checkpoints without reprocessing completed sequences"
    - "Worker final shard HDF5 is assembled from checkpointed + newly processed sequences"
    - "Per-GPU checkpoint isolation via shard_N subdirectories prevents conflicts"
  artifacts:
    - path: "virnucpro/pipeline/gpu_worker.py"
      provides: "gpu_worker with checkpoint_dir, force_restart, checkpoint config passthrough"
      contains: "checkpoint_dir"
  key_links:
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/pipeline/async_inference.py"
      via: "AsyncInferenceRunner with checkpoint_dir parameter"
      pattern: "AsyncInferenceRunner.*checkpoint_dir"
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/pipeline/checkpoint_writer.py"
      via: "resume_from_checkpoints for pre-inference resume"
      pattern: "resume_from_checkpoints"
---

<objective>
Wire checkpointing into the GPU worker function so each GPU independently checkpoints and resumes.

Purpose: gpu_worker is the per-GPU entry point spawned by GPUProcessCoordinator. It must pass checkpoint configuration to AsyncInferenceRunner and handle the resume flow: load existing checkpoints, skip already-processed sequences, and assemble the final shard from checkpointed + new results.

Output: Modified `virnucpro/pipeline/gpu_worker.py` with checkpoint support.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-01-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-03-SUMMARY.md
@virnucpro/pipeline/gpu_worker.py
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/checkpoint_writer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add checkpoint config to gpu_worker</name>
  <files>virnucpro/pipeline/gpu_worker.py</files>
  <action>
Modify `gpu_worker()` function to support checkpointing:

1. **model_config checkpoint fields**: The function already receives `model_config: Dict[str, Any]`. Document that it now accepts these additional keys:
   - `'enable_checkpointing'`: bool (default True) - enables incremental checkpointing
   - `'checkpoint_dir'`: str - path to checkpoint directory (required if checkpointing enabled)
   - `'force_restart'`: bool (default False) - force fresh start ignoring checkpoints
   - `'checkpoint_seq_threshold'`: int (default 10000) - sequence count trigger
   - `'checkpoint_time_threshold'`: float (default 300.0) - time trigger in seconds

2. **Extract checkpoint config** (after Step 2 CUDA init, before Step 3 index loading):
   ```python
   # Checkpoint configuration
   enable_checkpointing = model_config.get('enable_checkpointing', True)
   force_restart = model_config.get('force_restart', False)
   checkpoint_dir = Path(model_config['checkpoint_dir']) if model_config.get('checkpoint_dir') else output_dir / "checkpoints"
   checkpoint_seq_threshold = model_config.get('checkpoint_seq_threshold', 10000)
   checkpoint_time_threshold = model_config.get('checkpoint_time_threshold', 300.0)
   ```
   Log: `f"Checkpointing: enabled={enable_checkpointing}, dir={checkpoint_dir}, force_restart={force_restart}"`

3. **Pass checkpoint params to AsyncInferenceRunner** (Step 6):
   Change the runner instantiation from:
   ```python
   runner = AsyncInferenceRunner(model, device)
   ```
   To:
   ```python
   runner = AsyncInferenceRunner(
       model, device,
       checkpoint_dir=checkpoint_dir if enable_checkpointing else None,
       rank=rank,
       checkpoint_seq_threshold=checkpoint_seq_threshold,
       checkpoint_time_threshold=checkpoint_time_threshold,
   )
   ```

4. **Handle resumed results**: In the inference loop, modify how results are collected:
   ```python
   for result in runner.run(dataloader, force_restart=force_restart):
       if result.batch_idx == -1:
           # Resumed data from checkpoints - already processed
           logger.info(f"Loaded {len(result.sequence_ids)} resumed sequences from checkpoints")
           all_embeddings.append(result.embeddings)
           all_ids.extend(result.sequence_ids)
           continue
       batch_idx += 1
       all_embeddings.append(result.embeddings)
       all_ids.extend(result.sequence_ids)
   ```
   The batch_idx == -1 marker identifies pre-checkpointed data that the runner loaded from disk.

5. **Update status dict**: Add checkpoint info to the success result_status:
   ```python
   result_status = {
       'rank': rank,
       'status': 'complete',
       'shard_path': str(shard_path),
       'num_sequences': len(all_ids),
       'checkpointing_enabled': enable_checkpointing,
   }
   ```

6. **No changes to failure handling**: The existing try/except and numerical instability handling remain unchanged. If a worker crashes, its checkpoints on disk are preserved for resume on retry.

IMPORTANT: The gpu_worker signature does NOT change - it still takes (rank, world_size, results_queue, index_path, output_dir, model_config). Checkpoint config flows through model_config dict, keeping the worker function pickle-compatible.
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.gpu_worker import gpu_worker
import inspect
sig = inspect.signature(gpu_worker)
params = list(sig.parameters.keys())
assert params == ['rank', 'world_size', 'results_queue', 'index_path', 'output_dir', 'model_config'], f'Signature changed: {params}'
print('gpu_worker signature preserved, checkpoint integration ready')
"
  </verify>
  <done>
    gpu_worker extracts checkpoint config from model_config, passes to AsyncInferenceRunner, handles resumed data from checkpoints, and preserves the function signature for pickle compatibility with spawned processes.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.gpu_worker import gpu_worker"` - import succeeds
- gpu_worker signature unchanged (pickle compatibility)
- Checkpoint config extracted from model_config dict
- AsyncInferenceRunner receives checkpoint_dir when enabled
- Resumed results (batch_idx=-1) handled correctly
- Existing tests pass: `pytest tests/unit/test_gpu_worker.py -v`
</verification>

<success_criteria>
gpu_worker passes checkpoint configuration through model_config to AsyncInferenceRunner. Resumed checkpoint data is properly incorporated into the final shard. Worker signature is preserved for pickle compatibility. Existing functionality works identically when checkpointing is not configured.
</success_criteria>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-04-SUMMARY.md`
</output>
