---
phase: 09-checkpointing-integration
plan: 07
type: execute
wave: 5
depends_on: ["09-03", "09-04", "09-05", "09-06"]
files_modified:
  - tests/integration/test_checkpoint_integration.py
autonomous: true

must_haves:
  truths:
    - "Full pipeline creates incremental checkpoints during inference"
    - "Pipeline resumes from checkpoints without reprocessing completed sequences"
    - "Checkpoint validation detects corruption and pipeline continues past it"
    - "Multi-GPU coordinator retries failed workers and tracks manifest"
  artifacts:
    - path: "tests/integration/test_checkpoint_integration.py"
      provides: "Integration tests for end-to-end checkpoint flow"
      min_lines: 150
  key_links:
    - from: "tests/integration/test_checkpoint_integration.py"
      to: "virnucpro/pipeline/async_inference.py"
      via: "AsyncInferenceRunner with checkpoint_dir"
      pattern: "AsyncInferenceRunner.*checkpoint_dir"
    - from: "tests/integration/test_checkpoint_integration.py"
      to: "virnucpro/pipeline/checkpoint_writer.py"
      via: "resume_from_checkpoints verification"
      pattern: "resume_from_checkpoints"
    - from: "tests/integration/test_checkpoint_integration.py"
      to: "virnucpro/pipeline/gpu_coordinator.py"
      via: "wait_for_completion_with_retry testing"
      pattern: "wait_for_completion_with_retry"
---

<objective>
Write integration tests for the complete checkpoint flow: incremental saving, resume, corruption handling, and multi-GPU retry.

Purpose: These tests validate that all checkpoint components work together end-to-end. Unlike unit tests that test components in isolation, integration tests verify the wiring: AsyncInferenceRunner creates checkpoints during inference, resume skips completed work, corruption is handled gracefully, and the coordinator retries failed workers.

Output: `tests/integration/test_checkpoint_integration.py`.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-03-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-04-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-05-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-06-SUMMARY.md
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/gpu_worker.py
@virnucpro/pipeline/gpu_coordinator.py
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/checkpoint_writer.py
@virnucpro/pipeline/checkpoint_manifest.py
@tests/unit/test_async_inference.py
@tests/unit/test_gpu_coordinator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration tests for checkpoint flow</name>
  <files>tests/integration/test_checkpoint_integration.py</files>
  <action>
Create `tests/integration/test_checkpoint_integration.py` with these integration tests:

**Setup helpers:**
- Create a `MockModel` class that returns random embeddings of shape (batch_size, 5120) for testing without real ESM-2. It should have `forward()` returning `{'representations': {36: torch.randn(...)}}` and `forward_packed()` with the same pattern. Has `parameters()` returning a list with a dummy FP32 parameter. Has `eval()` as no-op.
- Create a `mock_dataloader()` fixture that yields batches with 'input_ids' (random tokens), 'sequence_ids' (list of strings), and optionally 'cu_seqlens' for packed format. Should yield a configurable number of batches with configurable sequences per batch.

**AsyncInferenceRunner checkpoint integration tests (mock GPU with CPU tensors):**

1. `test_runner_creates_checkpoints_during_inference` - Create AsyncInferenceRunner with checkpoint_dir, seq_threshold=20 (low for testing). Feed 50 sequences (5 batches of 10). Assert checkpoint files created in shard_0/ directory. Assert .done markers exist.

2. `test_runner_resume_skips_completed_work` - Run inference on 50 sequences (creates checkpoints). Create new runner with same checkpoint_dir. Run again. Assert resumed result has batch_idx=-1. Assert total unique sequence IDs equals 50 (no duplicates from resume + new).

3. `test_runner_force_restart_ignores_checkpoints` - Run inference creating checkpoints. Create new runner, run with force_restart=True. Assert no resumed results (no batch_idx=-1). Assert reprocesses all sequences.

4. `test_runner_final_checkpoint_captures_remaining` - Create runner with seq_threshold=100 (higher than total). Feed 30 sequences. Assert final checkpoint created (reason="final") with all 30 sequences.

5. `test_runner_checkpoint_at_batch_boundary_only` - Verify checkpoints happen between batches, not mid-batch. Create runner with seq_threshold=5. Feed packed batches. Assert checkpoint embeddings shape matches complete batch sizes (never partial).

**Corruption recovery tests:**

6. `test_resume_skips_corrupted_checkpoint` - Create 3 checkpoint files. Corrupt the second one (truncate the file to 10 bytes). Resume from checkpoints. Assert only first checkpoint loaded, third checkpoint NOT loaded (stops at corruption). Assert resume_batch_idx = 1.

7. `test_resume_removes_done_marker_on_corruption` - Same setup as above. After resume, assert the corrupted checkpoint's .done marker has been removed.

**Multi-GPU coordinator retry tests (mock workers):**

8. `test_coordinator_retries_failed_worker` - Create coordinator with world_size=2. Spawn workers where rank=1 fails on first attempt but succeeds on retry (use a module-level function that checks an attempt counter file). Assert wait_for_completion_with_retry returns both ranks successful.

9. `test_coordinator_gives_up_after_max_retries` - Create coordinator where worker always fails. Set max_retries=2. Assert wait_for_completion_with_retry returns the rank as failed.

10. `test_manifest_updated_on_completion` - Run multi-GPU scenario (mocked). After completion, load manifest. Assert completed shards marked "complete", failed shards marked "failed".

**Mark all tests with `@pytest.mark.slow` since they involve file I/O and sleep operations.**

Use `@patch` for mocking CUDA-dependent code (torch.cuda.is_available, etc.) to run on CPU. Use `tmp_path` for all checkpoint directories. Import mock utilities from unittest.mock.

IMPORTANT: These tests must NOT require a GPU. Mock all CUDA operations. The point is testing checkpoint LOGIC, not GPU inference.

Follow project test patterns from existing integration tests (test_multi_gpu_integration.py, test_packed_equivalence.py).
  </action>
  <verify>
    pytest tests/integration/test_checkpoint_integration.py -v -m "not gpu"
  </verify>
  <done>
    All 10 integration tests pass. Tests verify: checkpoint creation during inference, resume from checkpoints, force restart, final checkpoint, batch boundary atomicity, corruption recovery, .done marker cleanup, coordinator retry, max retry exhaustion, and manifest updates.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify existing tests still pass</name>
  <files></files>
  <action>
Run the full test suite to verify no regressions from checkpoint integration:

1. Run all unit tests: `pytest tests/unit/ -v -m "not gpu"`
2. Run all integration tests: `pytest tests/integration/ -v -m "not gpu"`

If any existing tests fail due to the new checkpoint parameters:
- Fix backward compatibility issues in the modified files (async_inference.py, gpu_worker.py, gpu_coordinator.py, multi_gpu_inference.py)
- The fix should make new parameters optional with defaults that preserve existing behavior
- Do NOT modify existing test expectations - fix the code to be backward compatible

Common potential issues:
- test_async_inference.py: If tests create AsyncInferenceRunner, new optional params should not affect them
- test_gpu_worker.py: If tests mock gpu_worker, model_config dict without checkpoint keys should work
- test_gpu_coordinator.py: If tests use wait_for_completion, it should still work (we added wait_for_completion_with_retry, not modified wait_for_completion)
- test_multi_gpu_inference.py: If tests call run_multi_gpu_inference, default enable_checkpointing=True might cause issues if tests don't set up checkpoint_dir - may need to handle gracefully
  </action>
  <verify>
    pytest tests/ -v -m "not gpu and not slow" --tb=short
  </verify>
  <done>
    All existing tests pass with no regressions. New checkpoint parameters are backward compatible. The test suite validates that checkpoint integration does not break existing functionality.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/unit/ -v -m "not gpu"` - all unit tests pass
- `pytest tests/integration/ -v -m "not gpu"` - all integration tests pass
- `pytest tests/integration/test_checkpoint_integration.py -v` - new checkpoint tests pass
- No test pollution warnings from conftest autouse fixture
</verification>

<success_criteria>
All 10 new integration tests pass, covering the full checkpoint lifecycle: creation, resume, force restart, corruption recovery, coordinator retry, and manifest tracking. All existing tests continue to pass with no regressions from checkpoint integration changes.
</success_criteria>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-07-SUMMARY.md`
</output>
