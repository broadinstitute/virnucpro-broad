---
phase: 09-checkpointing-integration
plan: 06
type: execute
wave: 2
depends_on: ["09-01", "09-02", "09-05"]
files_modified:
  - tests/unit/test_checkpoint_writer.py
  - tests/unit/test_checkpoint_manifest.py
  - tests/unit/test_gpu_coordinator.py
autonomous: true

must_haves:
  truths:
    - "CheckpointTrigger fires on sequence, time, emergency thresholds and viral mode"
    - "AsyncCheckpointWriter writes valid .pt checkpoints with .done markers and GPU→CPU safety"
    - "validate_checkpoint_pt detects all corruption types (empty, missing .done, bad torch.load, shape mismatch)"
    - "resume_from_checkpoints loads valid .pt checkpoints and identifies corrupted sequences"
    - "CheckpointManifest handles concurrent multi-process writes safely"
    - "GPUProcessCoordinator differentiated retry policies (spot=infinite, poison=2-strike, transient=3)"
    - "Circuit breaker triggers after 2 same-batch failures"
    - "Async write failures propagate via wait_all()"
  artifacts:
    - path: "tests/unit/test_checkpoint_writer.py"
      provides: "Unit tests for CheckpointTrigger, AsyncCheckpointWriter, validate/resume with .pt format"
      min_lines: 250
    - path: "tests/unit/test_checkpoint_manifest.py"
      provides: "Unit tests for CheckpointManifest with concurrency safety"
      min_lines: 120
    - path: "tests/unit/test_gpu_coordinator.py"
      provides: "Unit tests for differentiated retry policies and circuit breaker"
      min_lines: 150
  key_links:
    - from: "tests/unit/test_checkpoint_writer.py"
      to: "virnucpro/pipeline/checkpoint_writer.py"
      via: "tests .pt format, GPU→CPU transfer, async failure propagation"
      pattern: "torch\\.save|torch\\.load"
    - from: "tests/unit/test_checkpoint_manifest.py"
      to: "virnucpro/pipeline/checkpoint_manifest.py"
      via: "tests multi-process concurrent access"
      pattern: "multiprocessing|fcntl"
    - from: "tests/unit/test_gpu_coordinator.py"
      to: "virnucpro/pipeline/gpu_coordinator.py"
      via: "tests differentiated retry policies and circuit breaker"
      pattern: "_classify_error|_should_retry_worker"
---

<objective>
Write comprehensive unit tests for checkpoint foundation with .pt format, concurrency safety, and fault tolerance.

Purpose: These tests validate:
1. Checkpoint components use .pt format (not HDF5)
2. GPU→CPU transfer safety prevents CUDA context issues
3. Concurrent manifest writes don't corrupt JSON
4. Differentiated retry policies (spot/poison/transient)
5. Circuit breaker for poison inputs
6. Async write failures propagate correctly
7. Resume data integrity (shapes align, no corruption)

Testing these components in isolation catches bugs before integration.

Output: Three test files with comprehensive coverage.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-01-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-02-SUMMARY.md
@.planning/phases/09-checkpointing-integration/09-05-SUMMARY.md
@virnucpro/pipeline/checkpoint_writer.py
@virnucpro/pipeline/checkpoint_manifest.py
@virnucpro/pipeline/gpu_coordinator.py
@tests/unit/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Unit tests for checkpoint_writer.py (.pt format)</name>
  <files>tests/unit/test_checkpoint_writer.py</files>
  <action>
Create `tests/unit/test_checkpoint_writer.py` with these test cases:

**CheckpointTrigger tests:**

1. `test_trigger_sequence_threshold_fires` - Create trigger with seq_threshold=100. Call should_checkpoint with batch_size=50 twice. First call: (False, None). Second call: (True, "sequence_threshold").

2. `test_trigger_time_threshold_fires` - Create trigger with time_threshold_sec=0.1. Sleep 0.15s. Call should_checkpoint with batch_size=1. Assert (True, "time_threshold").

3. `test_trigger_emergency_override` - Create trigger with emergency_override_sec=0.1. Sleep 0.15s. Call should_checkpoint. Assert (True, "emergency_time_override"). Verify this fires even if sequences_since_checkpoint < threshold (emergency always triggers).

4. `test_trigger_reset_clears_counters` - Trigger fires on sequence threshold, call reset(), then verify trigger doesn't fire on next small batch.

5. `test_trigger_viral_mode_overrides_defaults` (Issue 7) - Set VIRNUCPRO_VIRAL_CHECKPOINT_MODE=true env var (use monkeypatch). Create trigger with DEFAULT args (no explicit thresholds). Assert internal seq_threshold=5000 and time_threshold_sec=180.0. Unset env var. Create trigger with EXPLICIT args seq_threshold=999. Assert seq_threshold=999 (env var doesn't override explicit). Clean up.

6. `test_trigger_viral_mode_disabled_uses_defaults` - Unset env var. Create trigger. Assert seq_threshold=10000, time_threshold_sec=300.0.

**AsyncCheckpointWriter tests with .pt format (Issue 1):**

7. `test_writer_creates_pt_with_done_marker` - Create writer, call write_checkpoint_async with dummy embeddings (np.random.randn(10, 128)), sequence_ids (list of 10 strings), metadata dict. Call wait_all(). Assert checkpoint .pt file exists, .pt.done marker exists.

8. `test_writer_pt_contains_correct_data` - Write checkpoint, read back with torch.load. Assert checkpoint dict has 'embeddings', 'sequence_ids', 'metadata' keys. Assert embeddings shape matches, sequence_ids match, metadata values present.

9. `test_writer_atomic_write_no_temp_file_remains` - After write_checkpoint_async + wait_all, assert no .tmp files in directory.

10. `test_writer_copies_numpy_data_safely` (Issue 9 race condition fix) - Create numpy array, start async write with threading.Barrier to pause before actual torch.save. Modify original array while write is paused. Release barrier. Wait for completion. Read checkpoint and verify ORIGINAL data (not modified). Use mock to inject barrier into _write_checkpoint_sync.

11. `test_writer_gpu_to_cpu_transfer` (Issue 5) - Mock torch.Tensor.cpu() and .numpy() methods. Pass GPU tensor (mock with .cpu() returning CPU tensor mock). Call write_checkpoint_async. Verify .cpu() was called before .numpy(), verifying GPU→CPU transfer happened.

12. `test_writer_shutdown_waits_for_pending` - Queue 3 writes, call shutdown(). Assert all 3 .pt files exist.

13. `test_writer_async_failure_propagates` (Issue 10) - Mock torch.save to raise IOError on second call. Queue 2 writes. Call wait_all(). Assert wait_all() raises RuntimeError containing aggregated error messages from both writes (one succeeds, one fails).

14. `test_writer_has_pending_returns_true_before_completion` - Queue write, immediately check has_pending() returns True. Call wait_all(), check has_pending() returns False.

**validate_checkpoint_pt tests (Issue 1 - .pt not HDF5):**

15. `test_validate_empty_file_fails` - Create 0-byte .pt file. Assert returns (False, "file is 0 bytes").

16. `test_validate_no_done_marker_fails` - Create valid .pt checkpoint (via torch.save) without .done marker. Assert returns (False, "missing .done marker").

17. `test_validate_corrupt_torch_file_fails` - Write garbage bytes to .pt file (not valid pickle). Add .done marker. Assert returns (False, "failed to load") when torch.load fails.

18. `test_validate_missing_keys_fails` - Save checkpoint dict with only 'embeddings' key (no sequence_ids). Add .done marker. Assert returns (False, "missing required keys").

19. `test_validate_shape_mismatch_fails` - Save checkpoint where embeddings has 10 rows but sequence_ids has 5 entries. Add .done marker. Assert returns (False, "shape mismatch").

20. `test_validate_valid_checkpoint_passes` - Create complete valid checkpoint via AsyncCheckpointWriter. Assert returns (True, "").

**resume_from_checkpoints tests:**

21. `test_resume_no_checkpoints_returns_empty_4tuple` - Empty checkpoint dir. Assert returns ([], None, 0, []) (4-tuple per Plan 01 API).

22. `test_resume_force_restart_ignores_checkpoints` - Write checkpoints, call with force_restart=True. Assert returns ([], None, 0, []).

23. `test_resume_loads_valid_pt_checkpoints_in_order` - Write batch_00000.pt and batch_00001.pt via AsyncCheckpointWriter. Call resume_from_checkpoints. Assert returns concatenated IDs, concatenated embeddings (numpy), resume_batch_idx=2, empty corrupted list.

24. `test_resume_stops_at_corrupted_checkpoint_and_returns_corrupted_ids` - Write 3 checkpoints. Corrupt middle one (remove its .done marker). Call resume. Assert: loads first checkpoint only, resume_batch_idx=1, corrupted_sequence_ids contains IDs from batch 1 and 2 (batches after corruption point).

25. `test_resume_data_integrity_shapes_align` (Issue 8) - Write 2 checkpoints with embeddings (5, 128) and (3, 128). Resume. Assert returned embeddings shape is (8, 128) - correct concatenation. Assert len(sequence_ids) = 8.

26. `test_resume_data_integrity_no_corruption` (Issue 8) - Write checkpoint with known embeddings values (np.arange(30).reshape(10,3)). Resume. Assert returned embeddings exactly match original (no data corruption during save/load cycle).

Use `import pytest`, `import numpy as np`, `import torch`, `from pathlib import Path`, `from unittest.mock import patch, Mock, MagicMock`.

Use `tmp_path` fixture for all file operations (pytest built-in, auto-cleaned).

IMPORTANT: All tests use .pt format with torch.save/torch.load, NOT h5py.
  </action>
  <verify>
    pytest tests/unit/test_checkpoint_writer.py -v
  </verify>
  <done>
    All 26 unit tests for CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_pt, and resume_from_checkpoints pass. Tests use .pt format, verify GPU→CPU transfer safety, test async failure propagation, verify resume data integrity, fix race condition with barrier pattern, and cover all corruption detection scenarios.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for checkpoint_manifest.py (concurrency safety)</name>
  <files>tests/unit/test_checkpoint_manifest.py</files>
  <action>
Create `tests/unit/test_checkpoint_manifest.py` with these test cases:

**Basic manifest operations:**

1. `test_manifest_initialize_creates_json` - Initialize manifest with world_size=4. Assert JSON file exists, contains version "1.0", has 4 shard entries all "in_progress".

2. `test_manifest_update_shard_checkpoint` - Initialize, call update_shard_checkpoint(rank=0, batch_idx=5, num_sequences=100, checkpoint_file="batch_00005.pt"). Load JSON, verify shard 0 has last_checkpoint_batch=5, total_sequences=100, checkpoint in list.

3. `test_manifest_mark_shard_complete` - Initialize, mark shard 0 complete. Verify status="complete" and completed_at timestamp set.

4. `test_manifest_mark_shard_failed` - Initialize, mark shard 1 failed with error message. Verify status="failed", error stored.

5. `test_manifest_get_resumable_shards` - Initialize with 4 shards. Mark 0 complete, leave 1 in_progress, mark 2 failed, leave 3 in_progress. Assert get_resumable_shards returns [1, 2, 3] (in_progress and failed).

6. `test_manifest_get_completed_shards` - Same setup as above. Assert get_completed_shards returns [0].

7. `test_manifest_get_global_progress` - Setup with mixed states. Assert progress dict has correct counts.

8. `test_manifest_atomic_write_no_temp_remains` - Initialize and update. Assert no .tmp files in directory.

9. `test_manifest_exists_false_before_init` - Create manifest object without initializing. Assert exists() returns False.

10. `test_manifest_cumulative_sequence_count` - Call update_shard_checkpoint for same shard twice. Assert total_sequences is cumulative (sum of both updates).

**Concurrency safety tests (Issue 4):**

11. `test_manifest_concurrent_updates_no_corruption` - Spawn 4 processes via multiprocessing.Process. Each process calls update_shard_checkpoint for different ranks 50 times. Join all processes. Load final manifest and verify: all 200 updates recorded (4 ranks × 50 updates each), JSON is valid (no corruption), no data loss.

12. `test_manifest_concurrent_same_shard_updates` - Spawn 2 processes both updating rank=0. Each does 25 updates. Verify final total_sequences for rank 0 is 50× (batch size), not less (proves no lost updates from race).

13. `test_manifest_file_lock_prevents_corruption` - Mock fcntl.flock to track lock calls. Call update_shard_checkpoint. Assert flock was called with LOCK_EX (exclusive lock) before write, LOCK_UN (unlock) after.

**Manifest redistribution (from Plan 05):**

14. `test_manifest_record_redistribution` - Initialize manifest. Call record_redistribution(failed_rank=2, new_rank=0). Load manifest. Verify redistribution history recorded with timestamp.

15. `test_manifest_get_redistributed_shards` - Record 2 redistributions. Call get_redistributed_shards(). Assert returns dict mapping failed_rank → new_rank.

Use `tmp_path` fixture for all file operations. Import `multiprocessing` for concurrency tests.

IMPORTANT: Concurrency tests use actual multiprocessing.Process (not threads) to simulate multi-GPU worker scenario.
  </action>
  <verify>
    pytest tests/unit/test_checkpoint_manifest.py -v
  </verify>
  <done>
    All 15 unit tests for CheckpointManifest pass. Tests cover initialization, shard tracking, completion/failure marking, queries, atomic writes, CONCURRENT multi-process updates (no corruption), file locking, and redistribution tracking.
  </done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for gpu_coordinator.py (fault tolerance)</name>
  <files>tests/unit/test_gpu_coordinator.py</files>
  <action>
Create `tests/unit/test_gpu_coordinator.py` (if it doesn't exist) or add to existing file:

**Error classification tests (Issue 9):**

1. `test_classify_error_spot_preemption_exitcode` - Create status dict with exitcode=143. Assert _classify_error returns "spot_preemption".

2. `test_classify_error_spot_preemption_message` - Create status with error_message="Received SIGTERM...". Assert returns "spot_preemption".

3. `test_classify_error_oom` - Create status with error="cuda_oom" and error_message="CUDA out of memory". Assert returns "oom".

4. `test_classify_error_poison_input` - Create status with error="cuda_runtime" or circuit_breaker=True. Assert returns "poison_input".

5. `test_classify_error_transient_fallback` - Create status with generic error. Assert returns "transient".

**Differentiated retry policy tests (Issue 3):**

6. `test_should_retry_spot_preemption_infinite` - Mock coordinator with retry_count=10 for rank 0. Call _should_retry_worker with error_type="spot_preemption". Assert returns (True, "spot preemption #11, capacity will return") - proves infinite retry.

7. `test_should_retry_poison_input_circuit_breaker` (Issue 2) - Mock coordinator. Simulate same batch failing twice: call _should_retry_worker with batch_idx=42, error_type="poison_input" twice. First call returns (True, ...). Second call returns (False, "circuit breaker: batch 42 failed 2 times..."). Verify (42, rank) added to self.failed_batches.

8. `test_should_retry_oom_exponential_backoff` - Mock coordinator with retry_count=2. Call _should_retry_worker with error_type="oom", runtime_config.max_retries_transient=3. Assert returns (True, "attempt 3/3"). Increment retry_count to 3, call again, assert returns (False, "exhausted 3 retries").

9. `test_should_retry_transient_max_retries` - Similar to OOM test but with error_type="transient".

**Checkpoint validation tests (Issue 10):**

10. `test_validate_checkpoint_dir_removes_orphaned_tmp_files` - Create checkpoint dir with 2 .pt files and 3 .tmp files. Call _validate_checkpoint_dir. Assert .tmp files deleted, .pt files remain.

11. `test_validate_checkpoint_dir_done_marker_mismatch_warning` - Create 3 .pt files but only 2 .pt.done markers. Call _validate_checkpoint_dir. Assert returns True (allows resume) but logs warning.

12. `test_validate_checkpoint_dir_fresh_start` - Call _validate_checkpoint_dir for non-existent shard dir. Assert returns True.

**Retry delay tests:**

13. `test_retry_worker_spot_preemption_delay` - Mock time.sleep and runtime_config.spot_retry_poll_interval=60. Call _retry_worker with error_type="spot_preemption". Assert sleep called with 60.0.

14. `test_retry_worker_oom_exponential_backoff` - Mock time.sleep. Set retry_count=0, call _retry_worker with error_type="oom". Assert sleep(1.0). Set retry_count=2, call again, assert sleep(4.0). Set retry_count=10, assert sleep(60.0) - capped.

15. `test_retry_worker_validates_checkpoint_before_respawn` - Mock _validate_checkpoint_dir to return False. Call _retry_worker. Verify worker NOT respawned (validation failed).

**SIGTERM handler test (Issue 6):**

16. `test_sigterm_handler_sets_shutdown_flag` - Create coordinator. Call _sigterm_handler(signal.SIGTERM, None). Assert self.shutdown_requested = True.

17. `test_sigterm_handler_waits_for_checkpoints` - Mock time.sleep and terminate_all. Call _sigterm_handler. Verify sleep(30) called (wait for worker checkpoints). Verify terminate_all() called.

Use `from unittest.mock import Mock, patch, MagicMock`. Mock RuntimeConfig where needed.

IMPORTANT: These tests use mocks/stubs, not actual worker processes. Focus on logic correctness.
  </action>
  <verify>
    pytest tests/unit/test_gpu_coordinator.py -v -k "checkpoint or retry or classify"
  </verify>
  <done>
    All 17 unit tests for GPUProcessCoordinator fault tolerance pass. Tests cover error classification (spot/OOM/poison/transient), differentiated retry policies (spot=infinite, poison=2-strike circuit breaker, OOM/transient=3-attempt), checkpoint validation, retry delays, and SIGTERM handler.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/unit/test_checkpoint_writer.py tests/unit/test_checkpoint_manifest.py tests/unit/test_gpu_coordinator.py -v` - all tests pass
- No test pollution (torch mock isolation verified by conftest autouse fixture)
- All checkpoint files created in tmp_path (no filesystem side effects)
- Concurrency tests use multiprocessing.Process (realistic multi-GPU simulation)
- All tests use .pt format with torch.save/torch.load (no h5py)
</verification>

<success_criteria>
All 58 unit tests pass (26 writer + 15 manifest + 17 coordinator). Tests cover:
- .pt format (not HDF5) for all checkpoint operations
- GPU→CPU transfer safety (mock verification)
- Async write failure propagation via wait_all()
- Resume data integrity (shape alignment, no corruption)
- Concurrent manifest updates (multi-process, no corruption)
- Differentiated retry policies (spot=infinite, poison=2-strike, transient=3)
- Circuit breaker for poison inputs (same batch fails twice)
- Emergency checkpoint trigger (>10min override)
- Viral mode environment variable (threshold override verification)
- Checkpoint validation before respawn
- SIGTERM handler graceful shutdown
</success_criteria>

<notes>
**Key changes from original plan:**

1. **HDF5 → .pt format** (Issue 1):
   - All test names changed: test_writer_creates_pt_with_done_marker
   - Use torch.save/torch.load instead of h5py
   - Tests 7-20, 23-26 rewritten for .pt format

2. **Circuit breaker tests** (Issue 2):
   - Test 7 (coordinator): Same batch fails twice → circuit breaker triggers
   - Verifies (rank, batch_idx) added to failed_batches set

3. **Differentiated retry tests** (Issue 3):
   - Tests 6-9 (coordinator): Spot=infinite, poison=2, OOM/transient=3
   - Verifies retry count increments and limits enforced

4. **Manifest concurrency tests** (Issue 4):
   - Tests 11-13: Multi-process concurrent updates
   - Use actual multiprocessing.Process
   - Verify no data loss, no JSON corruption

5. **GPU→CPU transfer test** (Issue 5):
   - Test 11 (writer): Mock torch.Tensor.cpu() and .numpy()
   - Verify .cpu() called before .numpy()

6. **Emergency checkpoint test** (Issue 6):
   - Test 3 already exists, verified it tests >threshold firing

7. **Viral mode test** (Issue 7):
   - Test 5 rewritten to verify 5000/180s thresholds when env var set
   - Test 6 added to verify defaults when env var unset
   - Tests env var doesn't override explicit args

8. **Resume data integrity tests** (Issue 8):
   - Tests 25-26: Shape alignment, no corruption
   - Verify concatenation correctness

9. **Race condition fix** (Issue 9):
   - Test 10: Use threading.Barrier to pause write, modify data, verify original saved
   - Proves .copy() happens before async write

10. **Async failure propagation** (Issue 10):
    - Test 13: Mock torch.save to fail, verify wait_all() raises aggregated errors

11. **Checkpoint cleanup** (Issue 11):
    - Test 10 (coordinator): Verify orphaned .tmp files removed
    - Implicit in validate tests (cleanup on resume)

**Test count breakdown:**
- CheckpointTrigger: 6 tests
- AsyncCheckpointWriter: 8 tests
- validate_checkpoint_pt: 6 tests
- resume_from_checkpoints: 6 tests
- CheckpointManifest: 15 tests (5 new for concurrency)
- GPUProcessCoordinator: 17 tests (all new)
- Total: 58 tests
</notes>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-06-SUMMARY.md`
</output>
