---
phase: 09-checkpointing-integration
plan: 03
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - virnucpro/pipeline/async_inference.py
autonomous: true

must_haves:
  truths:
    - "AsyncInferenceRunner checkpoints at batch boundaries without breaking packed attention"
    - "Checkpoint writes happen asynchronously (GPU continues inference while I/O runs)"
    - "Runner can resume from existing checkpoints and skip already-processed batches"
    - "Progress bar initializes at resume position for continuity"
    - "Checkpoint metadata includes GPU memory usage snapshot for crash debugging"
  artifacts:
    - path: "virnucpro/pipeline/async_inference.py"
      provides: "AsyncInferenceRunner with checkpoint hooks in run() method"
      contains: "CheckpointTrigger"
  key_links:
    - from: "virnucpro/pipeline/async_inference.py"
      to: "virnucpro/pipeline/checkpoint_writer.py"
      via: "import CheckpointTrigger, AsyncCheckpointWriter, resume_from_checkpoints"
      pattern: "from virnucpro\\.pipeline\\.checkpoint_writer import"
---

<objective>
Wire checkpointing into AsyncInferenceRunner so inference produces incremental checkpoints.

Purpose: AsyncInferenceRunner is the core inference loop for both single-GPU and multi-GPU modes. Adding checkpoint hooks here means ALL inference paths automatically get checkpointing. The runner must checkpoint at batch boundaries (respecting packed attention atomicity) and support resuming from prior checkpoints.

Output: Modified `virnucpro/pipeline/async_inference.py` with checkpoint integration.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-01-SUMMARY.md
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/checkpoint_writer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add checkpoint parameters to AsyncInferenceRunner.__init__</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Modify `AsyncInferenceRunner.__init__` to accept optional checkpoint parameters:

Add new parameters (all optional, defaults to disabled):
- `checkpoint_dir: Optional[Path] = None` - directory for checkpoint files (None = checkpointing disabled)
- `rank: int = 0` - shard rank for per-GPU isolation
- `checkpoint_seq_threshold: int = 10000` - sequence count trigger
- `checkpoint_time_threshold: float = 300.0` - time trigger in seconds

When checkpoint_dir is not None:
- Create `self.shard_checkpoint_dir = checkpoint_dir / f"shard_{rank}"`
- `self.shard_checkpoint_dir.mkdir(parents=True, exist_ok=True)`
- Initialize `self.trigger = CheckpointTrigger(seq_threshold=checkpoint_seq_threshold, time_threshold_sec=checkpoint_time_threshold)`
- Initialize `self.writer = AsyncCheckpointWriter(max_workers=1)`
- Initialize accumulator state: `self._ckpt_embeddings: List[np.ndarray] = []`, `self._ckpt_ids: List[str] = []`, `self._ckpt_batch_idx: int = 0`
- Store `self.checkpoint_dir = checkpoint_dir` and `self.rank = rank`

When checkpoint_dir is None:
- Set `self.trigger = None`, `self.writer = None`
- No checkpoint state initialized

Add `self._checkpointing_enabled` property: `return self.checkpoint_dir is not None`

Add imports at top of file:
- `from virnucpro.pipeline.checkpoint_writer import CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_hdf5, resume_from_checkpoints`
- `import numpy as np`
- `import os`

IMPORTANT: Do NOT change existing __init__ parameter signatures or behavior. Only ADD new optional parameters. Existing callers must work unchanged.
  </action>
  <verify>
    python -c "from virnucpro.pipeline.async_inference import AsyncInferenceRunner; print('Import successful')"
  </verify>
  <done>
    AsyncInferenceRunner accepts optional checkpoint_dir parameter. When provided, initializes trigger, writer, and accumulator state. When not provided, behaves identically to before.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add checkpoint hooks to run() method and _write_checkpoint helper</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Modify `AsyncInferenceRunner.run()` method to integrate checkpointing:

1. **Add resume parameters**: Change `run()` signature to add `force_restart: bool = False` parameter (default False, backward compatible).

2. **Resume logic at start of run()**: Before the inference loop, if checkpointing enabled and not force_restart:
   - Call `resume_from_checkpoints(self.checkpoint_dir, self.rank, force_restart)`
   - If resumed data exists (ids not empty):
     - Store in `self._ckpt_embeddings` and `self._ckpt_ids`
     - Set `self._ckpt_batch_idx` to resume_batch_idx
     - Log: `f"Resuming shard {self.rank}: {len(resumed_ids)} sequences from {resume_batch_idx} checkpoints"`
     - Yield an `InferenceResult` with the resumed data (batch_idx=-1 as marker for "resumed data")
       - Convert resumed embeddings np.ndarray to torch.Tensor for InferenceResult compatibility

3. **Checkpoint trigger after each batch**: In the main inference loop, after `yield result` and before `batch_idx += 1`:
   - If checkpointing enabled:
     - Accumulate: `self._ckpt_embeddings.append(result.embeddings.numpy())` and `self._ckpt_ids.extend(result.sequence_ids)`
     - Call `self.trigger.should_checkpoint(len(result.sequence_ids))`
     - If should checkpoint: call `self._write_checkpoint(reason)` then `self.trigger.reset()`

4. **Final checkpoint after loop**: After the flush logic (after `if hasattr(dataloader.collate_fn, 'flush')` block), if checkpointing enabled and `self._ckpt_embeddings` is not empty:
   - Call `self._write_checkpoint("final")`

5. **Wait for writes in finally block**: In the existing finally block, if checkpointing enabled:
   - Call `self.writer.wait_all(timeout=300)` to ensure all async writes complete before returning
   - Call `self.writer.shutdown()`
   - Log: "All checkpoint writes completed"

6. **Add `_write_checkpoint(self, reason: str)` method**:
   - If no accumulated data (`self._ckpt_embeddings` is empty): return early
   - Concatenate: `embeddings = np.concatenate(self._ckpt_embeddings, axis=0)`
   - Build checkpoint path: `self.shard_checkpoint_dir / f"batch_{self._ckpt_batch_idx:05d}.h5"`
   - Determine GPU device from self.model (e.g., `device = next(self.model.parameters()).device`)
   - Build metadata dict:
     - batch_idx, num_sequences (len of ids), timestamp (datetime.utcnow().isoformat()), trigger_reason
     - model_dtype (str of model param dtype), packing_enabled (check VIRNUCPRO_DISABLE_PACKING env var)
     - `gpu_memory_allocated_bytes`: `torch.cuda.memory_allocated(device)` if device.type == 'cuda' else 0
     - `gpu_memory_peak_bytes`: `torch.cuda.max_memory_allocated(device)` if device.type == 'cuda' else 0
   - Call `self.writer.write_checkpoint_async(checkpoint_path, embeddings, self._ckpt_ids, metadata)`
   - Log: `f"Checkpoint {self._ckpt_batch_idx} queued: {len(self._ckpt_ids)} sequences, reason={reason}"`
   - Increment `self._ckpt_batch_idx`
   - Reset accumulators: `self._ckpt_embeddings = []`, `self._ckpt_ids = []`

Add `from datetime import datetime` import. Import `torch` is already available (existing dependency).

CRITICAL: The checkpoint trigger fires AFTER yielding the batch result, at batch boundaries only. This respects packed attention atomicity - never mid-batch. The accumulate-then-check pattern ensures batch completeness.

IMPORTANT: Do NOT modify process_batch(), _run_inference(), _extract_embeddings(), or any other existing methods. Only modify run() and add _write_checkpoint().
  </action>
  <verify>
    python -c "
from virnucpro.pipeline.async_inference import AsyncInferenceRunner
import inspect
sig = inspect.signature(AsyncInferenceRunner.run)
assert 'force_restart' in sig.parameters, 'Missing force_restart param'
assert hasattr(AsyncInferenceRunner, '_write_checkpoint'), 'Missing _write_checkpoint method'
# Verify _write_checkpoint includes GPU memory in its implementation
src = inspect.getsource(AsyncInferenceRunner._write_checkpoint)
assert 'gpu_memory_allocated_bytes' in src, 'Missing gpu_memory_allocated_bytes in metadata'
assert 'gpu_memory_peak_bytes' in src, 'Missing gpu_memory_peak_bytes in metadata'
assert 'memory_allocated' in src, 'Missing torch.cuda.memory_allocated call'
print('Checkpoint integration with GPU memory metadata verified')
"
  </verify>
  <done>
    AsyncInferenceRunner.run() resumes from prior checkpoints, accumulates embeddings, triggers checkpoint writes at batch boundaries, writes final checkpoint, and waits for all async writes to complete. Checkpoint trigger respects packed attention atomicity by only firing between batches. Checkpoint metadata includes GPU memory usage snapshot (current allocated + peak) for crash debugging and consistency verification on resume.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.async_inference import AsyncInferenceRunner"` - import succeeds
- `AsyncInferenceRunner()` without checkpoint_dir works identically to before (backward compatible)
- `AsyncInferenceRunner(checkpoint_dir=...)` enables checkpointing with trigger + writer
- `run()` accepts force_restart parameter
- `_write_checkpoint()` method exists and uses async writer
- `_write_checkpoint()` metadata includes gpu_memory_allocated_bytes and gpu_memory_peak_bytes
- Existing tests still pass: `pytest tests/unit/test_async_inference.py -v`
</verification>

<success_criteria>
AsyncInferenceRunner integrates checkpointing at batch boundaries. Resume logic loads prior checkpoints. Async writes run in background thread. Final checkpoint ensures no data loss. Checkpoint metadata includes GPU memory snapshot (allocated + peak bytes) for crash debugging. Backward compatible - existing callers work without checkpoint_dir. All existing tests pass.
</success_criteria>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-03-SUMMARY.md`
</output>
