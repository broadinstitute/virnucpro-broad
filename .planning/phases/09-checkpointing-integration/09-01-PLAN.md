---
phase: 09-checkpointing-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/checkpoint_writer.py
autonomous: true

must_haves:
  truths:
    - "Checkpoint trigger fires at sequence count OR time threshold (whichever first)"
    - "Async checkpoint writes do not block GPU inference loop"
    - "Checkpoint files use atomic temp-then-rename with .done markers"
    - "HDF5 checkpoint validation detects corruption (size, structure, shape)"
  artifacts:
    - path: "virnucpro/pipeline/checkpoint_writer.py"
      provides: "CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_hdf5, resume_from_checkpoints"
      min_lines: 200
  key_links:
    - from: "virnucpro/pipeline/checkpoint_writer.py"
      to: "h5py"
      via: "HDF5 reads and writes for checkpoint data"
      pattern: "h5py\\.File"
    - from: "virnucpro/pipeline/checkpoint_writer.py"
      to: "concurrent.futures.ThreadPoolExecutor"
      via: "Background thread for async I/O"
      pattern: "ThreadPoolExecutor"
---

<objective>
Create the checkpoint foundation: adaptive trigger, async writer, HDF5 validation, and resume logic.

Purpose: These are the core building blocks that all other checkpoint plans depend on. CheckpointTrigger determines WHEN to checkpoint, AsyncCheckpointWriter handles HOW to write without blocking the GPU, validate_checkpoint_hdf5 validates checkpoint integrity, and resume_from_checkpoints loads prior progress.

Output: `virnucpro/pipeline/checkpoint_writer.py` with all four components.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-RESEARCH.md
@virnucpro/core/checkpoint.py
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/gpu_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: CheckpointTrigger and AsyncCheckpointWriter</name>
  <files>virnucpro/pipeline/checkpoint_writer.py</files>
  <action>
Create `virnucpro/pipeline/checkpoint_writer.py` with these components:

1. **CheckpointTrigger** class:
   - `__init__(self, seq_threshold: int = 10000, time_threshold_sec: float = 300.0, emergency_override_sec: float = 600.0)` - configurable thresholds
   - `should_checkpoint(self, batch_size: int) -> Tuple[bool, Optional[str]]` - returns (should_checkpoint, reason). Reasons: "sequence_threshold", "time_threshold", "emergency_time_override"
   - `reset(self)` - reset counters after checkpoint write. Resets both `sequences_since_checkpoint` and `last_checkpoint_time`
   - Uses `time.perf_counter()` for high-resolution timing
   - Emergency override (>emergency_override_sec without checkpoint) always triggers, even mid-batch
   - Support VIRNUCPRO_VIRAL_CHECKPOINT_MODE env var: when set to "true", use 5000 seq / 180s thresholds instead of defaults

2. **AsyncCheckpointWriter** class:
   - `__init__(self, max_workers: int = 1)` - uses `concurrent.futures.ThreadPoolExecutor`
   - `write_checkpoint_async(self, checkpoint_path: Path, embeddings: np.ndarray, sequence_ids: List[str], metadata: Dict[str, Any]) -> Future` - submits async write. CRITICAL: must `.copy()` embeddings and sequence_ids before submitting to background thread (prevents race conditions when GPU reuses tensor memory)
   - `_write_checkpoint_sync(self, ...)` - internal sync write: creates temp file, writes h5py datasets (embeddings, sequence_ids with vlen str dtype), stores metadata as HDF5 attrs, atomic rename via `temp_path.replace(checkpoint_path)`, then creates `.done` marker via `checkpoint_path.with_suffix('.h5.done').touch()`
   - `wait_all(self, timeout: Optional[float] = None)` - blocks until all pending writes complete, raises if any failed
   - `has_pending(self) -> bool` - check if writes are in-flight
   - `shutdown(self)` - shutdown executor, waits for pending writes
   - Track pending futures in a list protected by `threading.Lock`
   - On write failure: unlink temp file, log error, re-raise

3. **validate_checkpoint_hdf5** function:
   - `validate_checkpoint_hdf5(checkpoint_path: Path) -> Tuple[bool, str]` - multi-level validation
   - Level 1: File size > 0
   - Level 2: `.done` marker exists (use `has_done_marker` from `virnucpro.core.checkpoint`)
   - Level 3: HDF5 readable, contains 'embeddings' and 'sequence_ids' datasets
   - Level 4: Shape consistency (embeddings.shape[0] == sequence_ids.shape[0])
   - Returns (True, "") on success, (False, error_description) on failure

4. **resume_from_checkpoints** function:
   - `resume_from_checkpoints(checkpoint_dir: Path, rank: int, force_restart: bool = False) -> Tuple[List[str], Optional[np.ndarray], int]`
   - If force_restart or shard dir doesn't exist: return ([], None, 0)
   - Find all `batch_*.h5` files in `checkpoint_dir / f"shard_{rank}"`
   - Sort by batch number (extracted from filename: `batch_00042.h5` -> 42)
   - For each checkpoint in order: validate with `validate_checkpoint_hdf5`, load embeddings and sequence_ids
   - Stop at first corrupted checkpoint (log warning, don't load remaining)
   - On corruption: invalidate corrupted checkpoint by removing its .done marker, log which batch
   - Concatenate all valid embeddings with `np.concatenate`
   - Return (all_ids, concatenated_embeddings, resume_batch_idx) where resume_batch_idx = last_valid_batch + 1
   - Log: "Resuming from N checkpoints: M sequences, last_batch=K"

Module-level logger: `logger = logging.getLogger('virnucpro.pipeline.checkpoint_writer')`
Module docstring explaining this is the checkpoint foundation for Phase 9.

Use these imports:
- `import time, os, json, logging, threading` from stdlib
- `from concurrent.futures import ThreadPoolExecutor, Future` from stdlib
- `from pathlib import Path` from stdlib
- `from typing import ...` for type hints
- `import numpy as np`
- `import h5py` (import at use site inside functions, not module level - matches project pattern)
- `from virnucpro.core.checkpoint import has_done_marker` for .done marker checking
  </action>
  <verify>
    python -c "from virnucpro.pipeline.checkpoint_writer import CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_hdf5, resume_from_checkpoints; print('All imports successful')"
  </verify>
  <done>
    CheckpointTrigger fires on sequence count, time, and emergency thresholds. AsyncCheckpointWriter writes HDF5 checkpoints in background thread with atomic rename and .done markers. validate_checkpoint_hdf5 performs 4-level validation. resume_from_checkpoints loads valid checkpoints and returns resume state.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.checkpoint_writer import CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_hdf5, resume_from_checkpoints"`
- CheckpointTrigger: instantiate, call should_checkpoint with batch_size, verify trigger logic
- AsyncCheckpointWriter: verify ThreadPoolExecutor setup, data copying in write_checkpoint_async
- validate_checkpoint_hdf5: verify 4-level validation chain
- resume_from_checkpoints: verify sorted checkpoint loading with corruption handling
</verification>

<success_criteria>
All four components importable and structurally correct. CheckpointTrigger respects sequence/time/emergency thresholds. AsyncCheckpointWriter copies data before async submission. validate_checkpoint_hdf5 checks file size, .done marker, HDF5 structure, and shape consistency. resume_from_checkpoints loads valid checkpoints in order and stops at corruption.
</success_criteria>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-01-SUMMARY.md`
</output>
