---
phase: 09-checkpointing-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/checkpoint_writer.py
autonomous: true

must_haves:
  truths:
    - "Checkpoint trigger fires at sequence count OR time threshold (whichever first)"
    - "Async checkpoint writes do not block GPU inference loop"
    - "Checkpoint files use atomic temp-then-rename with .done markers"
    - "Checkpoint validation detects corruption (size, .done marker, loadability, shape)"
    - "GPU tensors are transferred to CPU before async write to prevent CUDA context issues"
    - "Resume returns corrupted sequence IDs for requeue by caller"
    - "Async write failures propagate via wait_all() instead of being silently swallowed"
  artifacts:
    - path: "virnucpro/pipeline/checkpoint_writer.py"
      provides: "CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_pt, validate_checkpoint_metadata, resume_from_checkpoints"
      min_lines: 250
  key_links:
    - from: "virnucpro/pipeline/checkpoint_writer.py"
      to: "torch"
      via: "torch.save/torch.load for .pt checkpoint format (consistent with Phase 3)"
      pattern: "torch\\.(save|load)"
    - from: "virnucpro/pipeline/checkpoint_writer.py"
      to: "concurrent.futures.ThreadPoolExecutor"
      via: "Background thread for async I/O"
      pattern: "ThreadPoolExecutor"
    - from: "virnucpro/pipeline/checkpoint_writer.py"
      to: "virnucpro.core.checkpoint"
      via: "has_done_marker and remove_done_marker for .done marker management"
      pattern: "from virnucpro\\.core\\.checkpoint import"
---

<objective>
Create the checkpoint foundation: adaptive trigger, async writer, .pt validation, metadata validation, and resume logic.

Purpose: These are the core building blocks that all other checkpoint plans depend on. CheckpointTrigger determines WHEN to checkpoint, AsyncCheckpointWriter handles HOW to write without blocking the GPU (including GPU-to-CPU transfer), validate_checkpoint_pt validates checkpoint integrity, validate_checkpoint_metadata verifies compatibility, and resume_from_checkpoints loads prior progress and identifies corrupted sequences for requeue.

Output: `virnucpro/pipeline/checkpoint_writer.py` with all five components.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-checkpointing-integration/09-CONTEXT.md
@.planning/phases/09-checkpointing-integration/09-RESEARCH.md
@virnucpro/core/checkpoint.py
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/gpu_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_pt, validate_checkpoint_metadata, and resume_from_checkpoints</name>
  <files>virnucpro/pipeline/checkpoint_writer.py</files>
  <action>
Create `virnucpro/pipeline/checkpoint_writer.py` with these components:

1. **CheckpointTrigger** class:
   - `__init__(self, seq_threshold: int = 10000, time_threshold_sec: float = 300.0, emergency_override_sec: float = 600.0)` - configurable thresholds
   - `should_checkpoint(self, batch_size: int) -> Tuple[bool, Optional[str]]` - returns (should_checkpoint, reason). Reasons: "sequence_threshold", "time_threshold", "emergency_time_override"
   - `reset(self)` - reset counters after checkpoint write. Resets both `sequences_since_checkpoint` and `last_checkpoint_time`
   - Uses `time.perf_counter()` for high-resolution timing
   - Emergency override (>emergency_override_sec without checkpoint) always triggers, even mid-batch
   - Support VIRNUCPRO_VIRAL_CHECKPOINT_MODE env var: when set to "true" AND constructor args are at their default values, use 5000 seq / 180s thresholds. If constructor args are explicitly non-default, honor the explicit args (env var only overrides defaults). Document this precedence in docstring: "Env var overrides default thresholds but not explicit arguments."

2. **AsyncCheckpointWriter** class:
   - `__init__(self, max_workers: int = 1, manifest: Optional['CheckpointManifest'] = None, rank: Optional[int] = None)` - uses `concurrent.futures.ThreadPoolExecutor`. Optional manifest and rank for coordinator integration (Issue 6).
   - `write_checkpoint_async(self, checkpoint_path: Path, embeddings, sequence_ids: List[str], metadata: Dict[str, Any]) -> Future` - submits async write. CRITICAL data safety before submitting to background thread:
     - If embeddings is a `torch.Tensor`: call `.cpu().numpy()` to transfer from GPU to CPU numpy array (Issue 2 - prevents CUDA context issues in background thread)
     - If embeddings is already `np.ndarray`: call `.copy()` to prevent race conditions when buffer reused
     - Copy sequence_ids: `list(sequence_ids)` (shallow copy sufficient for strings)
     - Copy metadata: `metadata.copy()` (shallow copy sufficient for simple types)
   - `_write_checkpoint_sync(self, checkpoint_path: Path, embeddings: np.ndarray, sequence_ids: List[str], metadata: Dict[str, Any])` - internal sync write:
     - Creates temp file with `.tmp` suffix
     - Builds checkpoint dict: `{'embeddings': embeddings, 'sequence_ids': sequence_ids, 'metadata': metadata}`
     - Uses `torch.save(checkpoint_dict, temp_path)` to write (NOT h5py - consistent with Phase 3 atomic_save pattern)
     - Atomic rename: `temp_path.replace(checkpoint_path)`
     - Creates `.done` marker: `checkpoint_path.with_suffix(checkpoint_path.suffix + '.done').touch()` (e.g., `batch_00042.pt.done`)
     - If manifest and rank are set: call `self.manifest.update_shard_checkpoint(self.rank, metadata.get('batch_idx', 0), len(sequence_ids), checkpoint_path.name)` after successful write (Issue 6)
   - `wait_all(self, timeout: Optional[float] = None)` - blocks until all pending writes complete. MUST iterate through all futures and call `.result()` to re-raise any exceptions (Issue 7). Collect all errors into a list; if any writes failed, raise `RuntimeError` with aggregated error messages. Clear pending_futures after checking all.
   - `has_pending(self) -> bool` - returns `len(self.pending_futures) > 0` with lock protection (Issue 12)
   - `shutdown(self)` - shutdown executor, waits for pending writes
   - Track pending futures in a list protected by `threading.Lock`
   - On write failure in _write_checkpoint_sync: unlink temp file if exists, log error, re-raise

3. **validate_checkpoint_pt** function (renamed from validate_checkpoint_hdf5 - Issue 1):
   - `validate_checkpoint_pt(checkpoint_path: Path) -> Tuple[bool, str]` - multi-level validation for .pt checkpoint files
   - Level 1: File exists and size > 0
   - Level 2: `.done` marker exists (use `has_done_marker` from `virnucpro.core.checkpoint`)
   - Level 3: File loadable via `torch.load(checkpoint_path, map_location='cpu', weights_only=False)`, contains 'embeddings' and 'sequence_ids' keys
   - Level 4: Shape consistency - `len(checkpoint['embeddings'])` == `len(checkpoint['sequence_ids'])`
   - Returns (True, "") on success, (False, error_description) on failure
   - Wrap Level 3-4 in try/except to catch corrupted .pt files gracefully

4. **validate_checkpoint_metadata** function (Issue 5):
   - `validate_checkpoint_metadata(metadata: Dict[str, Any], current_model_config_hash: Optional[str] = None, current_packing_enabled: Optional[bool] = None) -> Tuple[bool, List[str]]`
   - Returns (is_valid, list_of_warnings). Note: returns warnings not errors - metadata mismatches are logged but don't prevent loading (caller decides)
   - Check 1: If current_model_config_hash provided AND metadata contains 'model_config_hash', compare them. Mismatch produces warning: "Model config hash mismatch: checkpoint={X}, current={Y}"
   - Check 2: If current_packing_enabled provided AND metadata contains 'packing_enabled', compare them. Mismatch produces warning: "Packing config mismatch: checkpoint={X}, current={Y}"
   - Check 3: Verify required metadata keys present: ['batch_idx', 'num_sequences', 'timestamp']. Missing keys produce warning per key.
   - Returns (True, []) if no warnings, (False, [warning1, warning2, ...]) if any warnings

5. **resume_from_checkpoints** function:
   - `resume_from_checkpoints(checkpoint_dir: Path, rank: int, force_restart: bool = False, manifest: Optional['CheckpointManifest'] = None) -> Tuple[List[str], Optional[np.ndarray], int, List[str]]`
   - Return type is now a 4-tuple (Issue 3): (all_ids, concatenated_embeddings, resume_batch_idx, corrupted_sequence_ids)
     - corrupted_sequence_ids: list of sequence IDs from checkpoints AFTER the first corruption point (these need reprocessing)
   - If force_restart or shard dir doesn't exist: return ([], None, 0, [])
   - Find all `batch_*.pt` files in `checkpoint_dir / f"shard_{rank}"` (using .pt extension, not .h5)
   - Sort by batch number using regex `batch_(\d+)\.pt` for robust extraction (Issue 8). If regex doesn't match a filename, log warning and skip that file.
   - For each checkpoint in order: validate with `validate_checkpoint_pt`, load embeddings and sequence_ids
   - Stop at first corrupted checkpoint (log warning, don't load remaining)
   - On corruption: call `remove_done_marker` from `virnucpro.core.checkpoint` to invalidate corrupted checkpoint. Log which batch is corrupted.
   - For remaining checkpoints AFTER corruption: load only their sequence_ids (if loadable) to build the corrupted_sequence_ids list for caller requeue. If file isn't loadable, log that sequences from that file are also lost.
   - Concatenate all valid embeddings with `np.concatenate`
   - Optional manifest validation (Issue 4): If manifest parameter provided, validate:
     - (a) Shard exists in manifest (warn if not)
     - (b) manifest's last_checkpoint_batch matches resume_batch_idx - 1 (warn on mismatch)
     - (c) manifest's total_sequences matches loaded count (warn on mismatch)
     - Log warnings for mismatches but do not fail - filesystem checkpoints are source of truth
   - Return (all_ids, concatenated_embeddings, resume_batch_idx, corrupted_sequence_ids) where resume_batch_idx = last_valid_batch + 1
   - Log: "Resuming from N checkpoints: M sequences, last_batch=K" on success
   - Log: "Corruption detected at batch X: Y sequences need reprocessing" if corruption found

Module-level logger: `logger = logging.getLogger('virnucpro.pipeline.checkpoint_writer')`
Module docstring explaining this is the checkpoint foundation for Phase 9, using PyTorch .pt format consistent with Phase 3's atomic_save pattern.

Use these imports:
- `import time, os, re, logging, threading` from stdlib
- `from concurrent.futures import ThreadPoolExecutor, Future` from stdlib
- `from pathlib import Path` from stdlib
- `from typing import ...` for type hints
- `import numpy as np`
- `import torch` (for torch.save/torch.load - consistent with Phase 3 checkpoint format)
- `from virnucpro.core.checkpoint import has_done_marker, remove_done_marker` for .done marker management

NOTE: Do NOT import h5py anywhere. Checkpoints use torch.save() with .pt format, consistent with Phase 3's atomic_save() pattern. This avoids HDF5 append corruption risks and uses the proven serialization format.
  </action>
  <verify>
    python -c "from virnucpro.pipeline.checkpoint_writer import CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_pt, validate_checkpoint_metadata, resume_from_checkpoints; print('All imports successful')"
  </verify>
  <done>
    CheckpointTrigger fires on sequence count, time, and emergency thresholds with env var override respecting explicit args. AsyncCheckpointWriter writes .pt checkpoints in background thread with GPU->CPU transfer, atomic rename, .done markers, optional manifest integration, and proper error propagation in wait_all(). validate_checkpoint_pt performs 4-level validation for .pt files. validate_checkpoint_metadata checks compatibility of model config and packing settings. resume_from_checkpoints loads valid checkpoints, stops at corruption, returns corrupted sequence IDs for requeue, and optionally validates against manifest.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.pipeline.checkpoint_writer import CheckpointTrigger, AsyncCheckpointWriter, validate_checkpoint_pt, validate_checkpoint_metadata, resume_from_checkpoints"`
- CheckpointTrigger: instantiate, call should_checkpoint with batch_size, verify trigger logic. Verify VIRNUCPRO_VIRAL_CHECKPOINT_MODE env var overrides defaults but not explicit args.
- AsyncCheckpointWriter: verify ThreadPoolExecutor setup, GPU->CPU transfer + numpy copy in write_checkpoint_async, torch.save in _write_checkpoint_sync, error propagation in wait_all(), has_pending() with lock, optional manifest integration
- validate_checkpoint_pt: verify 4-level validation chain for .pt files (size, .done, loadable, shape)
- validate_checkpoint_metadata: verify model_config_hash and packing_enabled comparison, required key checks
- resume_from_checkpoints: verify sorted checkpoint loading with regex parsing, corruption handling with corrupted_sequence_ids, optional manifest validation, remove_done_marker on corruption
</verification>

<success_criteria>
All five components importable and structurally correct. CheckpointTrigger respects sequence/time/emergency thresholds with env var precedence. AsyncCheckpointWriter transfers GPU tensors to CPU before async submission, uses torch.save for .pt format, propagates errors in wait_all(), integrates with optional manifest. validate_checkpoint_pt checks file size, .done marker, torch.load, and shape consistency. validate_checkpoint_metadata checks config hash and packing compatibility. resume_from_checkpoints loads valid checkpoints in order, stops at corruption, returns corrupted sequence IDs for requeue, uses regex for robust filename parsing, and optionally validates against manifest.
</success_criteria>

<output>
After completion, create `.planning/phases/09-checkpointing-integration/09-01-SUMMARY.md`
</output>
