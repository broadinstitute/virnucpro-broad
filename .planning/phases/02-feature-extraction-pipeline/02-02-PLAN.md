---
phase: 02-feature-extraction-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - scripts/test_extraction.py
autonomous: false

must_haves:
  truths:
    - "extract_fast_esm() produces 1280-dim embeddings from real protein sequences"
    - "Output .pt file contains {'proteins': [labels], 'data': [tensors]} with correct counts"
    - "Embeddings are valid: no NaN, no Inf, correct dimension"
    - "Output file is loadable by merge_data() consumption pattern"
    - "Batch processing works correctly with multiple sequences"
    - "Resume capability works: re-running skips existing output"
  artifacts:
    - path: "scripts/test_extraction.py"
      provides: "End-to-end extraction test with sample FASTA and validation"
      contains: "extract_fast_esm"
  key_links:
    - from: "scripts/test_extraction.py"
      to: "units.py:extract_fast_esm()"
      via: "function call with loaded model"
      pattern: "extract_fast_esm"
    - from: "extract_fast_esm() output .pt"
      to: "merge_data() consumption pattern"
      via: "torch.load + zip(proteins, data)"
      pattern: "protein_data_dict\\[protein\\].*=.*data"
---

<objective>
Create a test script that validates extract_fast_esm() end-to-end on sample protein sequences inside the Docker environment, confirming correct embedding dimensions, output format, and compatibility with merge_data().

Purpose: This validates that the implementation from Plan 01 actually works on real GPU with real protein sequences, producing output that the downstream pipeline (merge_data, training) can consume. This is the final gate before Phase 3.

Output: Test script that can be run in Docker to validate the complete extraction pipeline.
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-feature-extraction-pipeline/02-RESEARCH.md
@.planning/phases/02-feature-extraction-pipeline/02-CONTEXT.md
@.planning/phases/02-feature-extraction-pipeline/02-01-SUMMARY.md
@units.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end extraction test script</name>
  <files>scripts/test_extraction.py</files>
  <action>
Create scripts/test_extraction.py that validates the entire extract_fast_esm() pipeline with real GPU inference.

The script should:

1. **Create sample FASTA data** -- write a temporary FASTA file with 5-10 protein sequences of varying lengths (short: 20aa, medium: 100aa, long: 500aa, very long: 1200aa to test truncation). Use realistic amino acid sequences (e.g., MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKLVERG for short ones; for longer ones, repeat/extend realistic patterns). Include at least one sequence longer than truncation_seq_length (1024) to test truncation behavior.

2. **Load model** -- Load FastESM2_650 from HuggingFace using the same pattern as features_extract.py:
   ```python
   model = AutoModel.from_pretrained("Synthyra/FastESM2_650", trust_remote_code=True, torch_dtype=torch.float16).eval().cuda()
   tokenizer = model.tokenizer
   ```

3. **Run extract_fast_esm()** with the sample FASTA, saving output to a temp .pt file:
   - Call with truncation_seq_length=1024, toks_per_batch=2048

4. **Validate output format:**
   - Load the .pt file with torch.load()
   - Assert keys are exactly 'proteins' and 'data'
   - Assert len(proteins) == number of input sequences
   - Assert len(data) == len(proteins)

5. **Validate embedding properties:**
   - Each embedding is a 1D tensor of shape (1280,)
   - Each embedding dtype is torch.float32 (converted from fp16)
   - No NaN values in any embedding
   - No Inf values in any embedding
   - Embeddings are not all zeros (sanity check)
   - Different sequences produce different embeddings (not identical)

6. **Validate merge_data() compatibility:**
   - Simulate how merge_data() reads the output:
     ```python
     ESM_outfile = torch.load(output_file)
     protein_data_dict = {}
     for protein, data in zip(ESM_outfile['proteins'], ESM_outfile['data']):
         protein_data_dict[protein] = data
     # Each value must be a tensor usable with torch.cat(..., dim=-1)
     for key, val in protein_data_dict.items():
         assert isinstance(val, torch.Tensor), f"{key}: data is {type(val)}, expected Tensor"
         assert val.shape == (1280,), f"{key}: shape is {val.shape}, expected (1280,)"
     ```

7. **Validate resume capability:**
   - Call extract_fast_esm() again with the same output file
   - It should return immediately (skip extraction) and return valid data

8. **Report results** with clear pass/fail for each check:
   ```
   Test Results:
   [PASS] Output file created
   [PASS] Correct keys in .pt file
   [PASS] Correct sequence count (N)
   [PASS] All embeddings are 1280-dim
   [PASS] All embeddings are float32
   [PASS] No NaN values
   [PASS] No Inf values
   [PASS] Embeddings are non-zero
   [PASS] Different sequences produce different embeddings
   [PASS] merge_data() compatibility verified
   [PASS] Resume capability works
   ```

9. **Clean up** temp files at the end (or use Python tempfile module for automatic cleanup).

10. **Exit with code 0 on all pass, code 1 on any failure.**

Print timing information: how long the extraction took for N sequences.
  </action>
  <verify>
Run inside Docker container:
```
docker-compose run --rm virnucpro python scripts/test_extraction.py
```
All tests should pass with exit code 0. Expect output showing 1280-dim embeddings for all test sequences.
  </verify>
  <done>
scripts/test_extraction.py runs successfully in Docker, all validation checks pass. Embeddings are 1280-dim float32 tensors. Output format is compatible with merge_data(). Resume capability works. Script exits with code 0.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete FastESM2_650 feature extraction pipeline:
1. extract_fast_esm() function in units.py (from Plan 01)
2. Updated features_extract.py with FastESM2_650 model loading (from Plan 01)
3. End-to-end test script validating extraction, format, and merge compatibility

This covers all Phase 2 requirements: FEAT-01 through FEAT-06.
  </what-built>
  <how-to-verify>
1. Run the extraction test in Docker:
   ```
   docker-compose run --rm virnucpro python scripts/test_extraction.py
   ```
2. Verify all test checks show [PASS]
3. Verify extraction timing is reported (should complete in under 1 minute for test sequences)
4. Confirm output shows 1280-dim embeddings (not 2560-dim from old ESM2 3B)
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
After all tasks:

1. scripts/test_extraction.py exists and is executable
2. Running `docker-compose run --rm virnucpro python scripts/test_extraction.py` passes all checks
3. Output .pt files have correct format for merge_data() consumption
4. Embeddings are 1280-dim, float32, no NaN/Inf
5. Phase 2 requirements FEAT-01 through FEAT-06 are all satisfied:
   - FEAT-01: extract_fast_esm() uses AutoModel.from_pretrained() API
   - FEAT-02: Tokenizer correctly processes protein sequences with truncation
   - FEAT-03: Mean-pooled embeddings produce 1280-dim output
   - FEAT-04: Batch processing with configurable batch size
   - FEAT-05: GPU acceleration active (model on CUDA)
   - FEAT-06: Output saved to .pt files in same format as ESM2 pipeline
</verification>

<success_criteria>
- Test script runs to completion in Docker with all checks passing
- 1280-dim embeddings confirmed for all test sequences
- Output format confirmed compatible with merge_data()
- Resume capability demonstrated (second run skips extraction)
- Extraction timing reported (confirms GPU acceleration working)
- User approves at checkpoint
</success_criteria>

<output>
After completion, create `.planning/phases/02-feature-extraction-pipeline/02-02-SUMMARY.md`
</output>
