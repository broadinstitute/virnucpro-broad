---
phase: 02-feature-extraction-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - units.py
  - features_extract.py
autonomous: true

must_haves:
  truths:
    - "extract_fast_esm() accepts a FASTA file and produces 1280-dim embeddings"
    - "Embeddings are mean-pooled over residue positions excluding BOS and EOS tokens"
    - "Output format exactly matches ESM2 structure: {'proteins': [labels], 'data': [tensors]}"
    - "Batch processing uses dynamic batching with toks_per_batch=2048"
    - "features_extract.py loads FastESM2_650 once and processes protein files sequentially"
    - "Existing files are skipped for resume capability"
    - "Failures are logged to extraction_failures.log and processing continues"
  artifacts:
    - path: "units.py"
      provides: "extract_fast_esm() function with get_batch_indices() and validate_embeddings() helpers"
      contains: "def extract_fast_esm"
    - path: "features_extract.py"
      provides: "FastESM2_650 model loading and sequential protein file processing"
      contains: "FastESM_model"
  key_links:
    - from: "units.py:extract_fast_esm()"
      to: "model.forward()"
      via: "tokenizer + forward pass + mean pooling"
      pattern: "outputs\\.last_hidden_state"
    - from: "units.py:extract_fast_esm()"
      to: "torch.save"
      via: "save {'proteins': proteins, 'data': data}"
      pattern: "torch\\.save.*proteins.*data"
    - from: "features_extract.py:process_file_pro()"
      to: "units.py:extract_fast_esm()"
      via: "function call with pre-loaded model"
      pattern: "extract_fast_esm"
    - from: "units.py:extract_fast_esm() output"
      to: "units.py:merge_data() input"
      via: "{'proteins': [...], 'data': [...]} format"
      pattern: "ESM_outfile\\['proteins'\\]"
---

<objective>
Implement the extract_fast_esm() function in units.py and update features_extract.py to use FastESM2_650 for protein embedding extraction.

Purpose: This is the core implementation of Phase 2 -- replacing the deprecated ESM2 3B extraction with FastESM2_650. The function must produce identically-formatted output files so that merge_data() and all downstream code work without modification.

Output: Working extract_fast_esm() function in units.py, updated features_extract.py with FastESM2_650 model loading and sequential processing.
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-feature-extraction-pipeline/02-RESEARCH.md
@.planning/phases/02-feature-extraction-pipeline/02-CONTEXT.md
@.planning/phases/01-environment-setup/01-02-SUMMARY.md
@units.py
@features_extract.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement extract_fast_esm() with helpers in units.py</name>
  <files>units.py</files>
  <action>
Add three new functions to units.py: get_batch_indices(), validate_embeddings(), and extract_fast_esm(). Place them after the existing extract_esm() function (which remains as a deprecated stub).

Add `import logging` and `import os` to the top of units.py (os may already be imported, verify). Add `logger = logging.getLogger(__name__)` near the top.

**get_batch_indices(sequence_lengths, toks_per_batch=2048):**
- Takes list of sequence lengths and max tokens per batch
- Sorts sequences by length descending for efficient packing
- Groups into batches where total tokens (each seq_len + 2 for BOS/EOS) does not exceed toks_per_batch
- Returns list of lists of indices (each inner list is one batch)
- If a single sequence exceeds toks_per_batch, it gets its own batch (never skip sequences)

**validate_embeddings(proteins, data, expected_dim=1280):**
- Validates dimension check: each tensor must be shape (expected_dim,)
- Validates NaN detection: no NaN values in any tensor
- Validates Inf detection: no Inf values in any tensor
- Returns list of error strings (empty list if all valid)

**extract_fast_esm(fasta_file, out_file=None, model=None, tokenizer=None, truncation_seq_length=1024, toks_per_batch=2048):**
- Per user decision: skip if out_file exists (resume capability) -- load and return existing data
- Read all sequences from FASTA using SeqIO.parse(fasta_file, 'fasta')
- Compute sequence lengths (capped at truncation_seq_length)
- Create batches using get_batch_indices()
- Process batches inside torch.no_grad() context:
  - Tokenize with model's tokenizer: tokenizer(batch_seqs, return_tensors='pt', padding='longest', truncation=True, max_length=truncation_seq_length + 2)
  - Move input_ids and attention_mask to model.device
  - Forward pass: outputs = model(input_ids=input_ids, attention_mask=attention_mask)
  - Mean pool excluding BOS/EOS: for each sequence i, compute last_hidden_state[i, 1:seq_len+1].mean(0) where seq_len = min(len(original_seq), truncation_seq_length). This exactly matches original ESM2 approach: t[i, 1:truncate_len+1].mean(0)
  - Convert to float32 on CPU: embedding.float().cpu()
  - Append label and embedding to proteins/data lists
  - Wrap batch loop in try/except: on torch.cuda.OutOfMemoryError, call torch.cuda.empty_cache() and if batch has >1 sequence, split batch in half and retry each half recursively. If single sequence OOMs, log failure and continue (per user decision: skip and log failures)
  - On any other exception for a batch, log the error and continue processing remaining batches
- After all batches: validate with validate_embeddings()
- If validation errors, log each error as warning. If ALL embeddings failed, raise RuntimeError.
- Per user decision: write failures to extraction_failures.log (append mode) with format: "timestamp | file | sequence_id | error_type | error_message"
- Save with torch.save({'proteins': proteins, 'data': data}, out_file) if out_file provided
- Use tqdm for batch-level progress bar (per user decision, matching DNABERT_S style)
- Log start/finish messages via logger.info()
- Return (proteins, data) tuple

IMPORTANT: The output data list must contain 1D tensors of shape (1280,), NOT dicts. The merge_data() function does `protein_data_dict[protein] = data` where data is accessed as a raw tensor. See merge_data() line 250 in units.py.

Do NOT modify extract_esm() (keep the NotImplementedError stub), extract_DNABERT_S(), merge_data(), or any other existing functions.
  </action>
  <verify>
Run inside Docker container:
```
docker-compose run --rm virnucpro python -c "
from units import extract_fast_esm, get_batch_indices, validate_embeddings
import torch

# Test get_batch_indices
lengths = [100, 200, 300, 50, 150]
batches = get_batch_indices(lengths, toks_per_batch=2048)
print(f'Batches: {batches}')
assert all(isinstance(b, list) for b in batches), 'Batches must be lists'
# All indices present
flat = [i for b in batches for i in b]
assert sorted(flat) == list(range(5)), f'Missing indices: {flat}'

# Test validate_embeddings
good = [torch.randn(1280) for _ in range(3)]
errors = validate_embeddings(['a','b','c'], good)
assert errors == [], f'Valid embeddings had errors: {errors}'

bad = [torch.tensor([float('nan')] * 1280)]
errors = validate_embeddings(['x'], bad)
assert len(errors) > 0, 'NaN not detected'

print('All unit tests passed')
"
```
  </verify>
  <done>
extract_fast_esm(), get_batch_indices(), and validate_embeddings() importable from units.py. get_batch_indices correctly groups sequences by token count. validate_embeddings catches NaN/Inf/dimension issues. extract_fast_esm has correct function signature accepting model, tokenizer, truncation_seq_length, and toks_per_batch parameters.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update features_extract.py for FastESM2_650 model loading and sequential processing</name>
  <files>features_extract.py</files>
  <action>
Update features_extract.py to load FastESM2_650 and use sequential processing for protein files:

1. **Replace ESM model loading** (lines 13-14):
   Remove the `ESM_model = None` and `ESM_alphabet = None` placeholder lines.
   Add FastESM2_650 loading:
   ```python
   import torch as torch_module  # avoid shadowing if needed
   from transformers import AutoModel

   # Load FastESM2_650 once at module level (per user decision: load once, keep in GPU memory)
   FastESM_model = AutoModel.from_pretrained(
       "Synthyra/FastESM2_650",
       trust_remote_code=True,
       torch_dtype=torch.float16
   ).eval().cuda()
   FastESM_tokenizer = FastESM_model.tokenizer
   ```
   Note: torch is already imported via `from units import *`. Use that existing torch. AutoModel is also imported via units. Only add what's not already available.

2. **Update process_file_pro()** (lines 27-35):
   Replace the call to extract_esm() with extract_fast_esm():
   ```python
   def process_file_pro(file):
       output_file = f'{file.split(".fa")[0]}_ESM.pt'
       merged_file_path = output_file.replace('./data/', './data/data_merge/').replace('.identified_protein', '_merged').replace('ESM', 'merged')

       if os.path.exists(output_file) or os.path.exists(merged_file_path):
           return output_file
       extract_fast_esm(
           fasta_file=file,
           out_file=output_file,
           model=FastESM_model,
           tokenizer=FastESM_tokenizer
       )
       print(f'saved to: {output_file}')
       return output_file
   ```
   Key change: passes model and tokenizer explicitly. No model_loaded, model, alphabet params.

3. **Replace multiprocessing.Pool for protein files with sequential processing:**
   - Find all instances of `multiprocessing.Pool(processes=2)` followed by `pool.map(process_file_pro, ...)` (lines 71-72, 227-229)
   - Replace each with a sequential for-loop using tqdm:
   ```python
   # Was: with multiprocessing.Pool(processes=2) as pool:
   #          results = pool.map(process_file_pro, viral_protein_files)
   # Now: Sequential processing (CUDA contexts are not fork-safe)
   results = []
   for f in tqdm(viral_protein_files, desc="Extracting protein embeddings"):
       results.append(process_file_pro(f))
   ```
   Do this for BOTH occurrences of protein multiprocessing.Pool in the file.

   IMPORTANT: Keep all `multiprocessing.Pool(processes=8)` calls for DNA/DNABERT_S processing UNCHANGED. Only replace the protein (processes=2) pools.

4. **Keep all other code unchanged.** Do not modify DNA processing, file listing logic, merge logic, or the random sampling code. The file is long but only 3 specific areas need changes.

5. **Add logging setup** at the top of the file (after imports):
   ```python
   import logging
   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
   ```
   This enables the logging from extract_fast_esm() to display during features_extract.py runs.
  </action>
  <verify>
Run inside Docker container:
```
docker-compose run --rm virnucpro python -c "
# Verify features_extract.py can be imported without errors
# (It runs module-level code, but we just need to check imports work)
# Instead, verify the key components are accessible
from units import extract_fast_esm
from transformers import AutoModel
import torch

# Verify model can load (this is the critical check)
model = AutoModel.from_pretrained(
    'Synthyra/FastESM2_650',
    trust_remote_code=True,
    torch_dtype=torch.float16
).eval().cuda()
tokenizer = model.tokenizer

print(f'Model loaded on: {next(model.parameters()).device}')
print(f'Model dtype: {next(model.parameters()).dtype}')
print(f'Tokenizer type: {type(tokenizer).__name__}')
print(f'Hidden size: {model.config.hidden_size}')
assert model.config.hidden_size == 1280, 'Expected 1280 hidden size'

# Quick tokenizer test
tokens = tokenizer(['MPRTEIN'], return_tensors='pt', padding='longest', truncation=True, max_length=1026)
print(f'Token shape: {tokens[\"input_ids\"].shape}')
assert tokens['input_ids'].shape[1] <= 1026, 'Tokenized length exceeds max'

print('Features extract setup verified')
"
```
  </verify>
  <done>
features_extract.py loads FastESM2_650 model at module level, process_file_pro() calls extract_fast_esm() with model and tokenizer, protein file processing uses sequential loops instead of multiprocessing.Pool, DNA processing with multiprocessing.Pool(processes=8) remains unchanged.
  </done>
</task>

</tasks>

<verification>
After both tasks:

1. units.py contains extract_fast_esm(), get_batch_indices(), validate_embeddings()
2. features_extract.py loads FastESM2_650 and uses sequential protein processing
3. No existing functions (extract_DNABERT_S, merge_data, split_fasta_file) are modified
4. extract_esm() still raises NotImplementedError
5. Output format {'proteins': [...], 'data': [...]} is compatible with merge_data()
6. All multiprocessing.Pool(processes=8) for DNA processing preserved
7. Code runs without import errors in Docker container
</verification>

<success_criteria>
- extract_fast_esm() function exists in units.py with correct signature
- Function implements mean pooling excluding BOS/EOS (positions 1:seq_len+1)
- Function outputs {'proteins': [labels], 'data': [1280-dim tensors]} matching ESM2 format
- Dynamic batching groups sequences by total token count (toks_per_batch=2048)
- Embedding validation checks dimensions, NaN, and Inf
- features_extract.py loads FastESM2_650 once at module level
- Protein processing is sequential (no multiprocessing.Pool for GPU work)
- Resume capability: existing output files are skipped
- Failure logging: errors written to extraction_failures.log
</success_criteria>

<output>
After completion, create `.planning/phases/02-feature-extraction-pipeline/02-01-SUMMARY.md`
</output>
