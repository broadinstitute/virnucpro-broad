---
phase: 08-fp16-precision-validation
plan: 04
type: execute
wave: 2
depends_on: ["08-01", "08-02"]
files_modified:
  - tests/benchmarks/test_fp16_throughput.py
autonomous: true

must_haves:
  truths:
    - "FP16 throughput measured using forward_packed() production code path"
    - "FP32 baseline (Phase 7 with FlashAttention) throughput established in same run"
    - "FlashAttention kernel invocation verified before benchmarking"
    - "Stratified length batches prevent padding skew in measurements"
    - "Throughput improvement ratio (FP16/FP32) calculated and printed"
    - "Total runtime comparison printed (primary metric per CONTEXT.md)"
    - "Peak GPU memory reported for both FP16 and FP32"
    - "Compute time isolated from data transfer time for accurate measurement"
  artifacts:
    - path: "tests/benchmarks/test_fp16_throughput.py"
      provides: "FP16 vs FP32+FlashAttention throughput benchmarking using forward_packed()"
      min_lines: 200
      exports: ["TestFP16Throughput"]
  key_links:
    - from: "tests/benchmarks/test_fp16_throughput.py"
      to: "virnucpro/models/esm2_flash.py"
      via: "load_esm2_model with enable_fp16=True and enable_fp16=False"
      pattern: "load_esm2_model.*enable_fp16"
---

<objective>
Benchmark FP16 throughput using forward_packed() production code path against the FP32+FlashAttention baseline (Phase 7) to quantify runtime reduction from FP16 precision alone.

Purpose: Phase 7 runs with FP32+FlashAttention+packing (current production baseline). Phase 8 switches to FP16 while keeping FlashAttention. This benchmark establishes the FP32+FlashAttention baseline and FP16+FlashAttention performance within the same controlled run using forward_packed() (production code path), then computes the improvement ratio. The speedup isolates FP16 tensor core acceleration + larger batch sizes from memory savings. Expected improvement: 1.5-1.8x (FP16 tensor cores ~1.3-1.5x + larger batches ~1.2x). Per CONTEXT.md, total runtime reduction is the primary metric. This is the one-time Phase 8 validation benchmark, not the on-demand diagnostic (which uses --fp32-compare flag at runtime).

Output: Benchmark test that loads the same model in FP32+FlashAttention (Phase 7 baseline) and FP16+FlashAttention (Phase 8), runs forward_packed() with stratified length batches, verifies FlashAttention is active, and prints a comparison table showing runtime reduction and throughput improvement.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@tests/benchmarks/test_end_to_end.py
@virnucpro/models/esm2_flash.py
@virnucpro/pipeline/async_inference.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create FP16 vs FP32 throughput benchmark</name>
  <files>tests/benchmarks/test_fp16_throughput.py</files>
  <action>
  Create `tests/benchmarks/test_fp16_throughput.py` following existing benchmark patterns.

  **Module header:**
  ```python
  """FP16 vs FP32+FlashAttention throughput benchmarking using forward_packed().

  Benchmarks the production code path (forward_packed with FlashAttention varlen)
  to measure FP16 precision speedup isolated from other optimizations.

  Baseline: Phase 7 FP32 with FlashAttention enabled (current production)
  Target: Phase 8 FP16 with FlashAttention (FP16 tensor cores + larger batches)
  Expected: 1.5-1.8x speedup (FP16 tensor cores ~1.3-1.5x + larger batches ~1.2x)

  The benchmark:
  - Uses forward_packed() production code path (not standard forward)
  - Verifies FlashAttention kernels are active (fails if fallback detected)
  - Uses stratified length batches (short/medium/long) to prevent padding skew
  - Isolates compute time from data transfer (batches pre-transferred to GPU)
  - Measures total runtime reduction (primary metric) and tokens/second

  This is the one-time Phase 8 validation benchmark. For production diagnostics,
  use --fp32-compare flag instead.

  Run with: pytest tests/benchmarks/test_fp16_throughput.py -v -s
  """
  ```

  Mark as slow and GPU-required:
  ```python
  pytestmark = [
      pytest.mark.slow,
      pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA required"),
  ]
  ```

  **Helper function for generating stratified test sequences:**
  ```python
  def generate_stratified_sequences(num_per_length=50):
      """Generate sequences with homogeneous lengths per batch.

      Stratified by length to avoid padding skew. In standard attention, a batch
      with one 400aa and seven 50aa sequences pads to 400aa (7Ã— waste). Homogeneous
      lengths give accurate per-length throughput measurements.

      Returns:
          Dict[str, List[Tuple[str, str]]]: {length_class: [(id, seq), ...]}
      """
      import random
      amino_acids = "ACDEFGHIKLMNPQRSTVWY"

      # Three length buckets (short/medium/long)
      short_seqs = [(f"short_{i}", "".join(random.choices(amino_acids, k=50)))
                    for i in range(num_per_length)]
      medium_seqs = [(f"med_{i}", "".join(random.choices(amino_acids, k=150)))
                     for i in range(num_per_length)]
      long_seqs = [(f"long_{i}", "".join(random.choices(amino_acids, k=300)))
                   for i in range(num_per_length)]

      return {
          "short": short_seqs,
          "medium": medium_seqs,
          "long": long_seqs
      }
  ```

  **Helper function for FlashAttention verification:**
  ```python
  def verify_flashattention_active(model):
      """Verify FlashAttention kernels are loaded and will be invoked.

      Raises:
          RuntimeError: If FlashAttention not available or model not integrated
      """
      try:
          from flash_attn import flash_attn_varlen_func
      except ImportError:
          raise RuntimeError(
              "FlashAttention not available. Install: pip install flash-attn>=2.6.0"
          )

      # Check model has forward_packed method
      if not hasattr(model, 'forward_packed'):
          raise RuntimeError(
              "Model missing forward_packed - FlashAttention not integrated. "
              "Ensure Phase 6 forward_packed integration is complete."
          )

      # Run small test to verify no fallback warnings
      import warnings
      test_ids = torch.tensor([[1, 2, 3, 4, 5]], device=model.device)
      cu_seqlens = torch.tensor([0, 5], dtype=torch.int32, device=model.device)

      with warnings.catch_warnings(record=True) as w:
          warnings.simplefilter("always")
          with torch.no_grad():
              _ = model.forward_packed(input_ids=test_ids, cu_seqlens=cu_seqlens, max_seqlen=5)

          # Check for fallback warnings
          fallback_msgs = [str(warning.message) for warning in w
                         if "fallback" in str(warning.message).lower()]
          if fallback_msgs:
              raise RuntimeError(
                  f"FlashAttention fallback detected: {fallback_msgs}. "
                  f"Benchmark would not measure FlashAttention performance."
              )

      return True

  def benchmark_model_packed(model, sequences, device, num_warmup=10, num_iterations=50):
      """Benchmark forward_packed() with production code path.

      Uses VarlenCollator + GreedyPacker to create packed batches (production path).
      Isolates GPU compute time from data transfer by pre-batching on CPU.

      Args:
          model: ESM2WithFlashAttention model
          sequences: List of (id, seq) tuples (homogeneous lengths recommended)
          device: CUDA device
          num_warmup: Warmup iterations (increased for CUDA cache + FlashAttention compilation)
          num_iterations: Benchmark iterations

      Returns:
          Dict with elapsed_sec, tokens_per_sec, sequences_per_sec, peak_memory_gb
      """
      from virnucpro.data.collators import VarlenCollator
      from virnucpro.data.packing import GreedyPacker

      # Create packed batches on CPU (isolate from compute timing)
      packer = GreedyPacker(token_budget=8192)  # Realistic production budget
      collator = VarlenCollator(packer=packer, tokenizer=model.alphabet)

      batches = []
      batch_size = 32  # Pack 32 sequences per batch
      for i in range(0, len(sequences), batch_size):
          batch_seqs = sequences[i:i+batch_size]
          batch = collator(batch_seqs)
          # Pre-transfer to GPU (exclude from timing)
          batch_gpu = {
              "input_ids": batch["input_ids"].to(device),
              "cu_seqlens": batch["cu_seqlens"].to(device),
              "max_seqlen": batch["max_seqlen"],
              "sequence_ids": batch["sequence_ids"]
          }
          batches.append(batch_gpu)

      # Warmup (increased for 3B model + FlashAttention kernel compilation)
      model.eval()
      with torch.no_grad():
          for i in range(num_warmup):
              batch = batches[i % len(batches)]
              _ = model.forward_packed(
                  input_ids=batch["input_ids"],
                  cu_seqlens=batch["cu_seqlens"],
                  max_seqlen=batch["max_seqlen"]
              )

      # Reset memory stats and sync
      torch.cuda.reset_peak_memory_stats(device)
      torch.cuda.synchronize(device)

      # Benchmark (compute-only timing - data already on GPU)
      start = time.perf_counter()
      total_tokens = 0
      total_sequences = 0

      with torch.no_grad():
          for i in range(num_iterations):
              batch = batches[i % len(batches)]
              total_tokens += batch["input_ids"].numel()
              total_sequences += len(batch["sequence_ids"])
              _ = model.forward_packed(
                  input_ids=batch["input_ids"],
                  cu_seqlens=batch["cu_seqlens"],
                  max_seqlen=batch["max_seqlen"]
              )

      torch.cuda.synchronize(device)
      elapsed = time.perf_counter() - start

      peak_memory_gb = torch.cuda.max_memory_allocated(device) / (1024**3)

      return {
          "elapsed_sec": elapsed,
          "tokens_per_sec": total_tokens / elapsed,
          "sequences_per_sec": total_sequences / elapsed,
          "peak_memory_gb": peak_memory_gb,
          "iterations": num_iterations,
          "total_tokens": total_tokens,
          "total_sequences": total_sequences,
      }
  ```

  **class TestFP16Throughput:**

  - `test_fp16_vs_fp32_throughput` - The main benchmark test using forward_packed():
    1. Generate stratified sequences (50 each of short/medium/long)
    2. Load model in FP32 (Phase 7 baseline: FP32+FlashAttention)
    3. Verify FlashAttention is active (fail if fallback detected)
    4. Benchmark FP32 on each length class using forward_packed()
    5. Delete FP32 model, clear GPU cache
    6. Load model in FP16 (Phase 8: FP16+FlashAttention)
    7. Verify FlashAttention is active (fail if fallback detected)
    8. Benchmark FP16 on each length class using forward_packed()
    9. Calculate speedup ratio per length class: `fp16_tokens_per_sec / fp32_tokens_per_sec`
    10. Calculate overall weighted speedup
    11. Calculate runtime reduction: `1 - (fp16_elapsed / fp32_elapsed)`
    12. Calculate memory savings: `1 - (fp16_memory / fp32_memory)`
    13. Print comprehensive summary table:
        ```
        Phase 8 FP16 vs Phase 7 FP32+FlashAttention Baseline
        =====================================================
        Metric           | FP32 (P7) | FP16 (P8) | Improvement
        ---------------------------------------------------------
        Total runtime     | ...s      | ...s      | X.Xx faster (primary)
        Tokens/sec       | ...       | ...       | X.Xx
        Sequences/sec    | ...       | ...       | X.Xx
        Peak memory      | ...GB     | ...GB     | ...% reduction
        ---------------------------------------------------------
        Per length class:
          Short (50aa):    X.Xx speedup
          Medium (150aa):  X.Xx speedup
          Long (300aa):    X.Xx speedup
        ---------------------------------------------------------
        Expected: 1.5-1.8x throughput improvement
          (FP16 tensor cores ~1.3-1.5x + larger batches ~1.2x)
        Baseline: Phase 7 FP32 with FlashAttention enabled
        Code path: forward_packed() (production)
        ```
    14. Assert throughput improvement >1.0x (FP16 should be at least as fast)
    15. Assert peak memory FP16 <= FP32 (should not use more memory)

  - `test_fp16_memory_reduction` - Load model in FP16, check peak memory.
    1. Load model in FP32 (Phase 7 baseline: FP32+FlashAttention), record `torch.cuda.max_memory_allocated`
    2. Delete model, clear cache
    3. Load model in FP16 (Phase 8: FP16+FlashAttention), record peak memory
    4. Print memory comparison: "FP32+FlashAttention (Phase 7 baseline): X.XGB, FP16+FlashAttention (Phase 8): Y.YGB"
    5. Assert FP16 memory < FP32 memory

  Note: Do NOT assert specific speedup ratios (e.g., >1.5x) since benchmark environment
  may differ from production. Just verify FP16 is not slower and report the numbers.
  The printed table gives the user clear data to evaluate against the 1.5-1.8x target.
  </action>
  <verify>
  Run: `pytest tests/benchmarks/test_fp16_throughput.py -v -s` (on GPU server) -- passes and prints benchmark results
  Run: `grep -n 'forward_packed' tests/benchmarks/test_fp16_throughput.py` -- verify production code path used
  Run: `grep -n 'verify_flashattention_active' tests/benchmarks/test_fp16_throughput.py` -- FlashAttention verification present
  Run: `grep -n 'generate_stratified_sequences' tests/benchmarks/test_fp16_throughput.py` -- stratified sequences used
  Run: `grep -n '1.5-1.8x' tests/benchmarks/test_fp16_throughput.py` -- expected speedup documented
  Run: `grep -n 'num_warmup=10' tests/benchmarks/test_fp16_throughput.py` -- warmup increased from 5 to 10
  Verify: `wc -l tests/benchmarks/test_fp16_throughput.py` -- at least 150 lines (added FlashAttention verification + stratified sequences)
  Run: `pytest tests/benchmarks/test_fp16_throughput.py -v -s -k "memory"` -- quick memory test
  </verify>
  <done>
  FP16 vs FP32+FlashAttention throughput benchmark created using forward_packed() production code path. Establishes FP32+FlashAttention baseline (Phase 7) and FP16+FlashAttention (Phase 8) in same run with FlashAttention verification. Uses stratified length batches to prevent padding skew. Isolates compute time from data transfer. Reports total runtime reduction (primary metric), per-length-class speedups, tokens/sec, and memory savings. Results printed as comparison table with 1.5-1.8x expected speedup (FP16 tensor cores + larger batches, baseline includes FlashAttention).
  </done>
</task>

</tasks>

<verification>
- `pytest tests/benchmarks/test_fp16_throughput.py -v -s` -- benchmark runs and reports results
- FlashAttention verification passes before benchmark (no fallback detected)
- forward_packed() code path used (production path, not standard forward)
- Stratified length batches used (short/medium/long separately)
- Total runtime comparison printed (Phase 7 FP32+FlashAttention vs Phase 8 FP16+FlashAttention)
- Per-length-class speedups printed (short/medium/long)
- Throughput and memory numbers printed as comparison table
- Expected speedup documented as 1.5-1.8x (not 1.8-2x) with baseline clarification
- FP16 not slower than FP32 (improvement >= 1.0x)
- FP16 peak memory <= FP32 peak memory
</verification>

<success_criteria>
- Benchmark uses forward_packed() production code path (not standard forward)
- FlashAttention kernel invocation verified before benchmarking (fails if fallback detected)
- Stratified length batches prevent padding skew in measurements
- Compute time isolated from data transfer time (batches pre-transferred to GPU)
- Warmup increased to 10 iterations (was 5 - sufficient for 3B model + FlashAttention compilation)
- Benchmark establishes FP32+FlashAttention baseline (Phase 7) within the same run
- Expected speedup adjusted to 1.5-1.8x (FP16 tensor cores ~1.3-1.5x + larger batches ~1.2x)
- Baseline explicitly clarified as "Phase 7 FP32 with FlashAttention enabled"
- Total runtime reduction reported as primary metric (per CONTEXT.md)
- Both tokens/sec and end-to-end runtime measured (per CONTEXT.md: isolate GPU from I/O)
- Per-length-class speedups reported (short/medium/long) to show variance
- Memory usage compared between FP16 and FP32
- Results printed as clear comparison table with Phase 7/8 labels
- No assertion on specific speedup ratio (environment-dependent) -- just verify FP16 is not worse
</success_criteria>

<notes>
**Baseline clarification:**
Phase 7 completed with FP32+FlashAttention+packing. The FP16 speedup (1.5-1.8x) comes from:
1. FP16 tensor cores: ~1.3-1.5x faster than FP32 (hardware acceleration)
2. Larger batch sizes: ~1.2x from memory savings allowing more sequences per batch

The baseline is NOT Phase 6 FP32+standard-attention (~3-4x slower), which would conflate multiple optimizations.

**Why forward_packed() is critical:**
- Standard forward(): Uses padded batches, doesn't invoke FlashAttention varlen kernels
- forward_packed(): Production code path with cu_seqlens, invokes FlashAttention varlen
- If FlashAttention fails to load, forward_packed() might fall back to standard attention
- Verification ensures we're measuring FlashAttention+FP16 performance, not just FP16

**Stratified sequences rationale:**
Mixed lengths (20-400aa) in one batch pad to max (400aa), making 20aa sequences run as slow as 400aa.
Stratified batches (all 50aa, or all 150aa, or all 300aa) eliminate padding skew and show per-length variance.
</notes>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-04-SUMMARY.md`
</output>
