---
phase: 08-fp16-precision-validation
plan: 04
type: execute
wave: 2
depends_on: ["08-01", "08-02"]
files_modified:
  - tests/benchmarks/test_fp16_throughput.py
autonomous: true

must_haves:
  truths:
    - "FP16 throughput measured in tokens/second and sequences/second"
    - "FP32 throughput measured for baseline comparison"
    - "Throughput improvement ratio calculated and logged"
    - "Peak GPU memory reported for both FP16 and FP32"
  artifacts:
    - path: "tests/benchmarks/test_fp16_throughput.py"
      provides: "FP16 vs FP32 throughput benchmarking"
      min_lines: 100
      exports: ["TestFP16Throughput"]
  key_links:
    - from: "tests/benchmarks/test_fp16_throughput.py"
      to: "virnucpro/models/esm2_flash.py"
      via: "load_esm2_model with enable_fp16=True and enable_fp16=False"
      pattern: "load_esm2_model.*enable_fp16"
    - from: "tests/benchmarks/test_fp16_throughput.py"
      to: "virnucpro/pipeline/async_inference.py"
      via: "AsyncInferenceRunner for inference loop"
      pattern: "AsyncInferenceRunner"
---

<objective>
Benchmark FP16 vs FP32 throughput to quantify the performance improvement.

Purpose: CONTEXT.md specifies total runtime reduction as the primary metric, with 1.8-2x throughput target. This plan creates a benchmark that measures tokens/second, end-to-end runtime, and peak memory for both FP16 and FP32, producing a clear comparison.

Output: Benchmark test file that measures and compares FP16 vs FP32 throughput.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@tests/benchmarks/test_end_to_end.py
@virnucpro/models/esm2_flash.py
@virnucpro/pipeline/async_inference.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create FP16 vs FP32 throughput benchmark</name>
  <files>tests/benchmarks/test_fp16_throughput.py</files>
  <action>
  Create `tests/benchmarks/test_fp16_throughput.py` following existing benchmark patterns.

  **Module header:**
  ```python
  """FP16 vs FP32 throughput benchmarking for ESM-2.

  Measures tokens/second, sequences/second, total runtime, and peak GPU memory
  for both FP16 and FP32 precision to quantify the performance improvement.

  Run with: pytest tests/benchmarks/test_fp16_throughput.py -v -s
  """
  ```

  Mark as slow and GPU-required:
  ```python
  pytestmark = [
      pytest.mark.slow,
      pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA required"),
  ]
  ```

  **Helper function for generating test sequences:**
  ```python
  def generate_test_sequences(num_sequences=100, min_len=20, max_len=400):
      """Generate random protein sequences for benchmarking."""
      import random
      amino_acids = "ACDEFGHIKLMNPQRSTVWY"
      sequences = []
      for i in range(num_sequences):
          length = random.randint(min_len, max_len)
          seq = "".join(random.choices(amino_acids, k=length))
          sequences.append((f"bench_{i}", seq))
      return sequences
  ```

  **Helper function for running benchmark:**
  ```python
  def benchmark_model(model, batch_converter, sequences, device, num_warmup=5, num_batches=50):
      """Run inference benchmark and return timing/memory stats."""
      # Prepare batches (small batch size for consistent comparison)
      batch_size = 8
      batches = []
      for i in range(0, len(sequences), batch_size):
          batch_seqs = sequences[i:i+batch_size]
          labels, strs, tokens = batch_converter(batch_seqs)
          batches.append(tokens.to(device))

      # Warmup
      model.eval()
      with torch.no_grad():
          for i in range(min(num_warmup, len(batches))):
              _ = model(batches[i % len(batches)], repr_layers=[36])

      # Reset memory stats
      torch.cuda.reset_peak_memory_stats(device)
      torch.cuda.synchronize(device)

      # Benchmark
      start = time.perf_counter()
      total_tokens = 0
      total_sequences = 0
      batches_run = 0

      with torch.no_grad():
          for i in range(num_batches):
              batch = batches[i % len(batches)]
              total_tokens += batch.numel()
              total_sequences += batch.shape[0]
              _ = model(batch, repr_layers=[36])
              batches_run += 1

      torch.cuda.synchronize(device)
      elapsed = time.perf_counter() - start

      peak_memory_gb = torch.cuda.max_memory_allocated(device) / (1024**3)

      return {
          "elapsed_sec": elapsed,
          "tokens_per_sec": total_tokens / elapsed,
          "sequences_per_sec": total_sequences / elapsed,
          "peak_memory_gb": peak_memory_gb,
          "batches_run": batches_run,
          "total_tokens": total_tokens,
          "total_sequences": total_sequences,
      }
  ```

  **class TestFP16Throughput:**

  - `test_fp16_vs_fp32_throughput` - The main benchmark test:
    1. Generate 200 sequences (varied lengths 20-400 aa)
    2. Load model in FP32, benchmark it
    3. Load model in FP16, benchmark it (reuse batch_converter)
    4. Calculate speedup ratio: `fp16_tokens_per_sec / fp32_tokens_per_sec`
    5. Calculate memory savings: `1 - (fp16_memory / fp32_memory)`
    6. Print comprehensive summary table:
       ```
       FP16 vs FP32 Benchmark Results
       ================================
       Metric           | FP32      | FP16      | Improvement
       Tokens/sec       | ...       | ...       | X.Xx
       Sequences/sec    | ...       | ...       | X.Xx
       Total runtime    | ...s      | ...s      | X.Xx faster
       Peak memory      | ...GB     | ...GB     | ...% reduction
       ```
    7. Assert throughput improvement >1.0x (FP16 should be at least as fast)
    8. Assert peak memory FP16 <= FP32 (should not use more memory)

  - `test_fp16_memory_reduction` - Load model in FP16, check peak memory.
    1. Load model in FP32, record `torch.cuda.max_memory_allocated`
    2. Delete model, clear cache
    3. Load model in FP16, record peak memory
    4. Print memory comparison
    5. Assert FP16 memory < FP32 memory

  Note: Do NOT assert specific speedup ratios (e.g., >1.8x) since benchmark environment
  may differ from production. Just verify FP16 is not slower and report the numbers.
  Log all results clearly so the user can evaluate.
  </action>
  <verify>
  Run: `pytest tests/benchmarks/test_fp16_throughput.py -v -s` (on GPU server) -- passes and prints benchmark results
  Verify: `wc -l tests/benchmarks/test_fp16_throughput.py` -- at least 100 lines
  Run: `pytest tests/benchmarks/test_fp16_throughput.py -v -s -k "memory"` -- quick memory test
  </verify>
  <done>
  FP16 vs FP32 throughput benchmark created. Measures tokens/sec, sequences/sec, total runtime, and peak GPU memory. Results printed as comparison table. Throughput improvement ratio and memory savings quantified.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/benchmarks/test_fp16_throughput.py -v -s` -- benchmark runs and reports results
- Throughput and memory numbers printed to stdout for evaluation
- FP16 not slower than FP32 (improvement >= 1.0x)
- FP16 peak memory <= FP32 peak memory
</verification>

<success_criteria>
- Benchmark measures FP16 and FP32 tokens/sec with same workload
- Memory usage compared between FP16 and FP32
- Results printed as clear comparison table
- No assertion on specific speedup ratio (environment-dependent) -- just verify FP16 is not worse
- Benchmark follows existing tests/benchmarks/ patterns
</success_criteria>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-04-SUMMARY.md`
</output>
