---
phase: 08-fp16-precision-validation
plan: 05
type: execute
wave: 3
depends_on: ["08-03"]
files_modified:
  - virnucpro/models/esm2_flash.py
  - virnucpro/models/dnabert_flash.py
  - tests/unit/test_fp16_conversion.py
autonomous: true
conditional: true

must_haves:
  truths:
    - "MixedPrecisionLayerNorm wrapper maintains FP16 pipeline while computing in FP32"
    - "MixedPrecisionSoftmax wrapper handles attention softmax in FP32 (fallback path safety)"
    - "Selective FP32 triggered only when 08-03 fails: similarity <0.99 or NaN/Inf detected"
    - "ESM-2 and DNABERT-S both support selective FP32 via wrapper replacement"
    - "Memory overhead <1MB (LayerNorm params ~368KB for ESM-2 3B)"
    - "End-to-end mixed precision test verifies FP16 input → FP32 computation → FP16 output"
  artifacts:
    - path: "virnucpro/models/esm2_flash.py"
      provides: "MixedPrecisionLayerNorm and MixedPrecisionSoftmax wrappers"
      contains: "MixedPrecisionLayerNorm"
    - path: "virnucpro/models/dnabert_flash.py"
      provides: "Selective FP32 conversion using wrappers"
      contains: "apply_selective_fp32"
  key_links:
    - from: "virnucpro/models/esm2_flash.py"
      to: "LayerNorm and Softmax modules"
      via: "apply_selective_fp32 replaces modules with MixedPrecision wrappers"
      pattern: "MixedPrecision"
---

<objective>
Implement selective FP32 conversion for numerically unstable layers (PREC-03 fallback).

Purpose: CONTEXT.md decision says "move operations back to FP32 only if tests fail (data-driven decisions)." This plan is CONDITIONAL -- execute ONLY if Plan 08-03 validation fails with:
- Cosine similarity <0.99 between FP16 and FP32 embeddings, OR
- NaN/Inf detected in FP16 outputs

**Trigger logic:**
```
IF 08-03 passes (similarity ≥0.99 AND no NaN/Inf):
  SKIP this plan entirely (full FP16 works)
ELSE:
  Execute 08-05 → re-run 08-03 tests with selective_fp32=True
```

**Fallback workflow:** Plan 08-03 fails → Execute Plan 08-05 (selective FP32) → Re-run Plan 08-03 tests to validate.

Output: Wrapper classes (MixedPrecisionLayerNorm, MixedPrecisionSoftmax) that compute in FP32 while maintaining FP16 pipeline continuity. The apply_selective_fp32() method replaces modules with wrappers.

**Precision strategy:**
- **LayerNorm → FP32** (primary): Variance reduction across 2560 dims loses precision in FP16, compounds over 36 layers
- **Attention Softmax → FP32** (defensive): FlashAttention handles this internally, but convert for fallback path safety (unpack/repack from 06-03)
- **Attention/FFN → FP16** (unchanged): Bulk of compute stays in FP16 for throughput

**Memory overhead:** LayerNorm params for ESM-2 3B: ~2 × 2560 × 36 × 2 (weight+bias) ≈ 368KB. Total overhead <1MB.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-01-SUMMARY.md
@virnucpro/models/esm2_flash.py
@virnucpro/models/dnabert_flash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement MixedPrecision wrapper classes</name>
  <files>virnucpro/models/esm2_flash.py, virnucpro/models/dnabert_flash.py</files>
  <action>
  **In esm2_flash.py, add wrapper classes at module level (before ESM2WithFlashAttention class):**

  ```python
  class MixedPrecisionLayerNorm(nn.Module):
      """LayerNorm computed in FP32 while maintaining FP16 pipeline.

      Wrapper pattern ensures clean dtype transitions without modifying forward pass call sites.

      Why needed:
      - LayerNorm variance reduction across 2560 dims loses precision in FP16
      - Direct .float() breaks pipeline (FP32 output → next layer expects FP16)

      Pattern:
      - Input cast: FP16 → FP32
      - Computation in FP32
      - Output cast: FP32 → FP16 (maintains pipeline continuity)

      Memory overhead: ~2 * hidden_dim * 2 (weight+bias) ≈ 10KB per layer
      """

      def __init__(self, layernorm_fp32: nn.LayerNorm):
          super().__init__()
          self.norm = layernorm_fp32.float()  # Ensure FP32
          # Preserve original attributes for introspection
          self.normalized_shape = self.norm.normalized_shape
          self.eps = self.norm.eps
          self.elementwise_affine = self.norm.elementwise_affine

      def forward(self, x: torch.Tensor) -> torch.Tensor:
          # Input cast: FP16 → FP32 (explicit, not auto-promotion)
          input_dtype = x.dtype
          x_fp32 = x.float()

          # Computation in FP32
          out_fp32 = self.norm(x_fp32)

          # Output cast: FP32 → input dtype (maintains pipeline continuity)
          # If input was FP16, output is FP16. If FP32, stays FP32.
          return out_fp32.to(input_dtype)


  class MixedPrecisionSoftmax(nn.Module):
      """Softmax computed in FP32 for attention scores.

      Why needed:
      - Attention scores (Q@K.T) can reach seq_len * sqrt(dim)
      - For 1024-length sequences with dim=2560: ~51k, near FP16 max (65504)
      - FlashAttention handles this internally, but needed for fallback path

      Note: With FlashAttention enabled, this wrapper is defensive (kernel already
      uses FP32 for softmax normalization). Critical for unpack/repack fallback.
      """

      def __init__(self, dim: int = -1):
          super().__init__()
          self.dim = dim

      def forward(self, x: torch.Tensor) -> torch.Tensor:
          # Input cast: FP16 → FP32
          input_dtype = x.dtype
          x_fp32 = x.float()

          # Softmax in FP32
          out_fp32 = torch.nn.functional.softmax(x_fp32, dim=self.dim)

          # Output cast: FP32 → input dtype
          return out_fp32.to(input_dtype)
  ```

  **In ESM2WithFlashAttention class, add apply_selective_fp32 method:**

  ```python
  def apply_selective_fp32(self) -> None:
      """Replace LayerNorm and Softmax with FP32 wrappers while keeping rest in FP16.

      Called as fallback when full FP16 causes NaN/Inf or similarity <0.99.

      Targets:
      1. LayerNorm (primary): Variance reduction loses precision over 36 layers
      2. Attention Softmax (defensive): Overflow safety for fallback path

      Non-invasive: Uses wrapper pattern, forward pass code unchanged.

      Memory overhead: ~368KB LayerNorm params (ESM-2 3B) + negligible softmax
      """
      layernorm_count = 0
      softmax_count = 0

      for layer_idx, layer in enumerate(self.model.layers):
          # Replace LayerNorm modules with wrappers
          if hasattr(layer, 'self_attn_layer_norm'):
              layer.self_attn_layer_norm = MixedPrecisionLayerNorm(
                  layer.self_attn_layer_norm
              )
              layernorm_count += 1

          if hasattr(layer, 'final_layer_norm'):
              layer.final_layer_norm = MixedPrecisionLayerNorm(
                  layer.final_layer_norm
              )
              layernorm_count += 1

          # Replace attention softmax (if accessible)
          # Note: ESM-2 may inline softmax in attention - this is defensive
          if hasattr(layer, 'self_attn') and hasattr(layer.self_attn, 'softmax'):
              layer.self_attn.softmax = MixedPrecisionSoftmax(dim=-1)
              softmax_count += 1

      # Also convert final layer norm if present
      if hasattr(self.model, 'emb_layer_norm_after'):
          self.model.emb_layer_norm_after = MixedPrecisionLayerNorm(
              self.model.emb_layer_norm_after
          )
          layernorm_count += 1

      logger.info(
          f"Selective FP32 applied: {layernorm_count} LayerNorm, "
          f"{softmax_count} Softmax converted to FP32 wrappers. "
          f"Attention and FFN layers remain in FP16. "
          f"Memory overhead: <1MB."
      )
  ```

  **Modify ESM2WithFlashAttention.__init__:**
  - Add `selective_fp32: bool = False` parameter
  - After `self.model.half()` if `enable_fp16=True`:
    ```python
    if enable_fp16:
        self.model = self.model.half()
        if selective_fp32:
            self.apply_selective_fp32()
    ```
  - Store `self.selective_fp32 = selective_fp32`
  - Update `__repr__` to show: `'mixed (FP16 + selective FP32)' if self.selective_fp32 else ('float16' if self.use_fp16 else 'float32')`

  **Update load_esm2_model:**
  - Add `selective_fp32: bool = False` parameter
  - Pass to `ESM2WithFlashAttention.__init__`

  **In dnabert_flash.py:**

  Copy MixedPrecisionLayerNorm and MixedPrecisionSoftmax classes (same implementation).

  **In DNABERTWithFlashAttention class, add apply_selective_fp32:**

  ```python
  def apply_selective_fp32(self) -> None:
      """Replace LayerNorm with FP32 wrappers while keeping rest in FP16.

      DNABERT-S (MosaicBERT variant): Use generic LayerNorm detection.
      Collects modules first to avoid in-place iteration issues.
      """
      # Step 1: Collect LayerNorm modules (avoid in-place iteration)
      layernorm_modules = []
      for name, module in self.model.named_modules():
          if isinstance(module, torch.nn.LayerNorm):
              layernorm_modules.append((name, module))

      # Step 2: Replace with wrappers
      layernorm_count = 0
      for name, module in layernorm_modules:
          # Navigate to parent module and replace
          parent_name, attr_name = name.rsplit('.', 1) if '.' in name else ('', name)

          if parent_name:
              parent = self.model.get_submodule(parent_name)
          else:
              parent = self.model

          setattr(parent, attr_name, MixedPrecisionLayerNorm(module))
          layernorm_count += 1

      logger.info(
          f"Selective FP32 applied: {layernorm_count} LayerNorm converted to FP32 wrappers. "
          f"Memory overhead: <1MB."
      )
  ```

  Also add `selective_fp32` parameter to `__init__` and `load_dnabert_model` (same pattern as ESM-2).
  </action>
  <verify>
  Run: `grep -n 'class MixedPrecisionLayerNorm' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- wrapper class in both files
  Run: `grep -n 'class MixedPrecisionSoftmax' virnucpro/models/esm2_flash.py` -- softmax wrapper present
  Run: `grep -n 'apply_selective_fp32' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- method exists in both
  Run: `grep -n 'selective_fp32' virnucpro/models/esm2_flash.py` -- parameter in __init__ and load_esm2_model
  Run: `grep -n 'rsplit' virnucpro/models/dnabert_flash.py` -- DNABERT avoids in-place iteration (collects first)
  Run: `pytest tests/unit/ -v -k "esm2 or dnabert"` -- existing tests pass (selective_fp32 defaults to False)
  </verify>
  <done>
  MixedPrecisionLayerNorm and MixedPrecisionSoftmax wrappers implemented. Wrappers maintain FP16 pipeline continuity (input FP16 → compute FP32 → output FP16). apply_selective_fp32() replaces modules with wrappers non-invasively. DNABERT collects modules before replacement to avoid in-place iteration issues. Memory overhead <1MB documented.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for selective FP32 conversion with wrappers</name>
  <files>tests/unit/test_fp16_conversion.py</files>
  <action>
  Add tests to existing `tests/unit/test_fp16_conversion.py`:

  **Test 11: test_mixed_precision_layernorm_dtype_flow**
  ```python
  def test_mixed_precision_layernorm_dtype_flow():
      """Verify MixedPrecisionLayerNorm maintains FP16 pipeline."""
      from virnucpro.models.esm2_flash import MixedPrecisionLayerNorm

      # Create FP32 LayerNorm, wrap it
      ln = torch.nn.LayerNorm(128)
      wrapped = MixedPrecisionLayerNorm(ln)

      # FP16 input
      x_fp16 = torch.randn(10, 128, dtype=torch.float16)

      # Output should be FP16 (pipeline continuity)
      out = wrapped(x_fp16)
      assert out.dtype == torch.float16, f"Expected FP16 output, got {out.dtype}"

      # Verify computation actually happened (output differs from input)
      assert not torch.allclose(x_fp16, out, atol=1e-2), "LayerNorm should transform input"
  ```

  **Test 12: test_selective_fp32_converts_layernorm_to_wrapper**
  ```python
  def test_selective_fp32_converts_layernorm_to_wrapper():
      """Verify apply_selective_fp32 replaces LayerNorm with wrappers."""
      from virnucpro.models.esm2_flash import ESM2WithFlashAttention, MixedPrecisionLayerNorm
      from unittest.mock import Mock, MagicMock

      # Create mock ESM-2 model with known layer structure
      mock_model = Mock()
      mock_layer = Mock()
      mock_layer.self_attn_layer_norm = torch.nn.LayerNorm(128)
      mock_layer.final_layer_norm = torch.nn.LayerNorm(128)
      mock_model.layers = [mock_layer]

      wrapper = ESM2WithFlashAttention.__new__(ESM2WithFlashAttention)
      wrapper.model = mock_model
      wrapper.apply_selective_fp32()

      # Verify replacement with wrappers (not .float())
      assert isinstance(mock_layer.self_attn_layer_norm, MixedPrecisionLayerNorm)
      assert isinstance(mock_layer.final_layer_norm, MixedPrecisionLayerNorm)
  ```

  **Test 13: test_selective_fp32_end_to_end_mixed_precision**
  ```python
  def test_selective_fp32_end_to_end_mixed_precision():
      """End-to-end test: FP16 model with selective FP32 runs inference without errors."""
      from virnucpro.models.esm2_flash import MixedPrecisionLayerNorm
      from unittest.mock import Mock

      # Create minimal transformer layer with mixed precision
      class MixedPrecisionLayer(torch.nn.Module):
          def __init__(self):
              super().__init__()
              self.attention = torch.nn.Linear(64, 64).half()  # FP16
              self.layernorm = MixedPrecisionLayerNorm(torch.nn.LayerNorm(64))  # Wrapper
              self.ffn = torch.nn.Linear(64, 64).half()  # FP16

          def forward(self, x):
              # Attention in FP16
              attn_out = self.attention(x)
              # LayerNorm wrapper: FP16 → FP32 compute → FP16
              normed = self.layernorm(attn_out)
              # FFN in FP16
              return self.ffn(normed)

      layer = MixedPrecisionLayer()

      # FP16 input
      x = torch.randn(2, 10, 64, dtype=torch.float16)

      # Should run without dtype errors
      out = layer(x)

      # Output should be FP16 (pipeline maintained)
      assert out.dtype == torch.float16

      # Verify no NaN/Inf (mixed precision working)
      assert not torch.isnan(out).any()
      assert not torch.isinf(out).any()
  ```

  **Test 14: test_selective_fp32_keeps_attention_fp16**
  ```python
  def test_selective_fp32_keeps_attention_fp16():
      """Verify attention layers stay FP16 after selective FP32."""
      from virnucpro.models.esm2_flash import ESM2WithFlashAttention
      from unittest.mock import Mock

      mock_model = Mock()
      mock_layer = Mock()

      # Attention layer in FP16
      attn_layer = torch.nn.Linear(128, 128).half()
      mock_layer.self_attn = Mock()
      mock_layer.self_attn.in_proj_weight = attn_layer.weight

      # LayerNorm to be converted
      mock_layer.self_attn_layer_norm = torch.nn.LayerNorm(128)
      mock_layer.final_layer_norm = torch.nn.LayerNorm(128)

      mock_model.layers = [mock_layer]

      wrapper = ESM2WithFlashAttention.__new__(ESM2WithFlashAttention)
      wrapper.model = mock_model
      wrapper.apply_selective_fp32()

      # Attention should still be FP16
      assert mock_layer.self_attn.in_proj_weight.dtype == torch.float16
  ```

  **Test 15: test_selective_fp32_log_message**
  ```python
  def test_selective_fp32_log_message(caplog):
      """Verify apply_selective_fp32 logs conversion count and memory overhead."""
      from virnucpro.models.esm2_flash import ESM2WithFlashAttention
      from unittest.mock import Mock

      mock_model = Mock()
      mock_layer = Mock()
      mock_layer.self_attn_layer_norm = torch.nn.LayerNorm(128)
      mock_layer.final_layer_norm = torch.nn.LayerNorm(128)
      mock_model.layers = [mock_layer]

      wrapper = ESM2WithFlashAttention.__new__(ESM2WithFlashAttention)
      wrapper.model = mock_model

      with caplog.at_level('INFO'):
          wrapper.apply_selective_fp32()

      # Verify log mentions counts and memory overhead
      log_text = caplog.text
      assert '2 LayerNorm' in log_text or 'LayerNorm' in log_text
      assert 'Memory overhead' in log_text or '<1MB' in log_text
  ```

  Use `unittest.mock` to create models with known layer structure.
  Check parameter dtypes and wrapper types (isinstance checks).
  </action>
  <verify>
  Run: `pytest tests/unit/test_fp16_conversion.py -v` -- all 15 tests pass (10 from 08-02 + 5 new)
  Run: `pytest tests/unit/test_fp16_conversion.py -v -k "mixed_precision"` -- wrapper tests pass
  Run: `pytest tests/unit/test_fp16_conversion.py -v -k "end_to_end"` -- Test 13 validates full mixed precision flow
  Run: `pytest tests/unit/ -v --tb=short` -- no regressions
  </verify>
  <done>
  5 new unit tests verify: MixedPrecisionLayerNorm maintains FP16 pipeline (Test 11), apply_selective_fp32 uses wrappers not .float() (Test 12), end-to-end mixed precision works without dtype errors (Test 13), attention stays FP16 (Test 14), logging includes memory overhead (Test 15).
  </done>
</task>

</tasks>

<verification>
- `pytest tests/unit/test_fp16_conversion.py -v` -- all 15 tests pass
- `grep -n 'class MixedPrecision' virnucpro/models/esm2_flash.py` -- wrapper classes present
- `grep -n 'apply_selective_fp32' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- method in both
- `grep -n 'to(input_dtype)' virnucpro/models/esm2_flash.py` -- wrappers maintain pipeline (output cast present)
- `grep -n 'rsplit' virnucpro/models/dnabert_flash.py` -- DNABERT avoids in-place iteration
- If this plan was triggered by 08-03 failure: re-run `pytest tests/integration/test_fp16_validation.py -v` with `selective_fp32=True` to confirm fix
</verification>

<success_criteria>
- MixedPrecisionLayerNorm wrapper: FP16 input → FP32 compute → FP16 output (pipeline continuity)
- MixedPrecisionSoftmax wrapper: FP32 softmax for attention scores (fallback path safety)
- apply_selective_fp32() replaces modules with wrappers (non-invasive, forward pass unchanged)
- DNABERT collects modules before replacement (avoids in-place iteration)
- selective_fp32 parameter defaults to False (no behavior change unless explicitly requested)
- Memory overhead <1MB documented (LayerNorm params ~368KB for ESM-2 3B)
- Unit Test 13 validates end-to-end: FP16 model with selective FP32 runs without dtype errors
- If used as fallback: re-running 08-03 validation tests passes with selective_fp32=True
</success_criteria>

<notes>
**Why LayerNorm is primary target:**
- Variance reduction: `var = mean((x - mean(x))^2)` across 2560 dims
- Sum of 2560 squared values loses precision in FP16 (~10 bits mantissa)
- Compounds over 36 transformer layers → accumulating drift

**Why softmax is defensive:**
- With FlashAttention enabled: kernel uses FP32 internally for softmax normalization
- Without FlashAttention (unpack/repack fallback from Phase 6): softmax conversion becomes critical
- Attention scores Q@K.T can reach `seq_len * sqrt(dim)` ≈ 51k for long sequences (near FP16 max 65504)

**Wrapper pattern benefits:**
- Clean manual casting (no autocast overhead)
- Non-invasive (forward pass code unchanged)
- Encapsulated dtype logic (one place, not scattered)
- Testable in isolation (Test 11)

**DNABERT in-place iteration fix:**
- Collect modules first: `layernorm_modules = [(name, module) for ...]`
- Then replace: `setattr(parent, attr_name, wrapper)`
- Avoids modifying dict during iteration

**Trigger logic:**
Execute this plan ONLY if 08-03 reports:
- Cosine similarity <0.99 between FP16 and FP32, OR
- NaN/Inf in FP16 outputs

If 08-03 passes, skip this plan (full FP16 works).
</notes>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-05-SUMMARY.md`
</output>
