---
phase: 08-fp16-precision-validation
plan: 05
type: execute
wave: 3
depends_on: ["08-03"]
files_modified:
  - virnucpro/models/esm2_flash.py
  - virnucpro/models/dnabert_flash.py
  - tests/unit/test_fp16_conversion.py
autonomous: true
conditional: true

must_haves:
  truths:
    - "Selective FP32 conversion moves LayerNorm and softmax to FP32 while keeping rest in FP16"
    - "Selective FP32 is triggered only when full FP16 fails validation (similarity <0.99 or NaN/Inf)"
    - "ESM-2 and DNABERT-S both support selective FP32 LayerNorm conversion"
    - "Unit tests verify selective FP32 converts correct layers"
  artifacts:
    - path: "virnucpro/models/esm2_flash.py"
      provides: "Selective FP32 conversion for numerically unstable layers"
      contains: "apply_selective_fp32"
    - path: "virnucpro/models/dnabert_flash.py"
      provides: "Selective FP32 conversion for DNABERT-S LayerNorm"
      contains: "apply_selective_fp32"
  key_links:
    - from: "virnucpro/models/esm2_flash.py"
      to: "LayerNorm modules"
      via: "apply_selective_fp32 iterates model layers and converts LayerNorm to float()"
      pattern: "layer_norm.*float\\(\\)"
---

<objective>
Implement selective FP32 conversion for numerically unstable layers (PREC-03 fallback).

Purpose: CONTEXT.md decision says "move operations back to FP32 only if tests fail (data-driven decisions)." This plan is CONDITIONAL -- it should only be executed if Plan 08-03 validation tests fail with similarity <0.99 or NaN/Inf detection. If Plan 08-03 passes (full FP16 works), skip this plan entirely.

Fallback workflow: Plan 08-03 fails --> Execute Plan 08-05 (selective FP32) --> Re-run Plan 08-03 tests to validate.

Output: apply_selective_fp32() method on model wrappers that converts LayerNorm and softmax to FP32 while keeping attention and feed-forward in FP16.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-01-SUMMARY.md
@virnucpro/models/esm2_flash.py
@virnucpro/models/dnabert_flash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add selective FP32 conversion for LayerNorm layers</name>
  <files>virnucpro/models/esm2_flash.py, virnucpro/models/dnabert_flash.py</files>
  <action>
  **In ESM2WithFlashAttention (esm2_flash.py):**

  Add a method `apply_selective_fp32()` that converts numerically unstable layers to FP32:

  ```python
  def apply_selective_fp32(self) -> None:
      """Convert LayerNorm layers to FP32 while keeping rest in FP16.

      Called as fallback when full FP16 causes NaN/Inf or similarity <0.99.
      LayerNorm accumulates large values that can overflow FP16 (max 65504).
      Keeping LayerNorm in FP32 fixes this while preserving FP16 speedup
      for attention and feed-forward layers.

      Pattern from 08-RESEARCH.md Pitfall 2.
      """
      converted_count = 0
      for layer in self.model.layers:
          # ESM-2 has two LayerNorm per transformer layer
          if hasattr(layer, 'self_attn_layer_norm'):
              layer.self_attn_layer_norm = layer.self_attn_layer_norm.float()
              converted_count += 1
          if hasattr(layer, 'final_layer_norm'):
              layer.final_layer_norm = layer.final_layer_norm.float()
              converted_count += 1

      # Also convert final layer norm if present
      if hasattr(self.model, 'emb_layer_norm_after'):
          self.model.emb_layer_norm_after = self.model.emb_layer_norm_after.float()
          converted_count += 1

      logger.info(
          f"Selective FP32 applied: {converted_count} LayerNorm layers converted to FP32. "
          f"Attention and FFN layers remain in FP16."
      )
  ```

  Also modify `__init__` to accept `selective_fp32` parameter (default: False):
  - If `enable_fp16=True` AND `selective_fp32=True`: call `self.model.half()` then `self.apply_selective_fp32()`
  - Store `self.selective_fp32 = selective_fp32`
  - Update `__repr__` to include selective_fp32 status

  Update `load_esm2_model` to accept and pass `selective_fp32` parameter.

  **In DNABERTWithFlashAttention (dnabert_flash.py):**

  Same pattern -- add `apply_selective_fp32()`:

  ```python
  def apply_selective_fp32(self) -> None:
      """Convert LayerNorm layers to FP32 while keeping rest in FP16."""
      converted_count = 0
      # DNABERT-S (MosaicBERT variant) layer structure
      for name, module in self.model.named_modules():
          if isinstance(module, torch.nn.LayerNorm):
              module.float()
              converted_count += 1

      logger.info(
          f"Selective FP32 applied: {converted_count} LayerNorm layers converted to FP32."
      )
  ```

  For DNABERT-S, use generic `isinstance(module, torch.nn.LayerNorm)` approach since MosaicBERT
  layer names may differ from ESM-2. The `named_modules()` approach catches all LayerNorm
  instances regardless of nesting.

  Also add `selective_fp32` parameter to `__init__` and `load_dnabert_model`.
  </action>
  <verify>
  Run: `grep -n 'apply_selective_fp32' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- method exists in both
  Run: `grep -n 'selective_fp32' virnucpro/models/esm2_flash.py` -- parameter in __init__ and load_esm2_model
  Run: `pytest tests/unit/ -v -k "esm2 or dnabert"` -- existing tests pass (selective_fp32 defaults to False)
  </verify>
  <done>
  Selective FP32 conversion implemented for both ESM-2 and DNABERT-S. LayerNorm layers move to FP32 while attention/FFN stay in FP16. Method callable after model.half() to fix NaN/Inf from LayerNorm overflow.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for selective FP32 conversion</name>
  <files>tests/unit/test_fp16_conversion.py</files>
  <action>
  Add tests to existing `tests/unit/test_fp16_conversion.py`:

  10. **test_selective_fp32_converts_layernorm** - Create a mock ESM-2 model with FP16 layers. Call `apply_selective_fp32()`. Verify LayerNorm layers are FP32 while other layers remain FP16.
  11. **test_selective_fp32_keeps_attention_fp16** - After selective FP32, verify self-attention weight parameters are still FP16.
  12. **test_selective_fp32_log_message** - Verify apply_selective_fp32 logs the count of converted layers.

  Use `unittest.mock` to create model with known layer structure.
  Check parameter dtypes with `param.dtype == torch.float32` for LayerNorm and `param.dtype == torch.float16` for attention.
  </action>
  <verify>
  Run: `pytest tests/unit/test_fp16_conversion.py -v` -- all tests pass (including new ones)
  Run: `pytest tests/unit/ -v --tb=short` -- no regressions
  </verify>
  <done>
  Unit tests verify selective FP32 converts LayerNorm to FP32 while keeping attention in FP16, and logs the conversion count.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/unit/test_fp16_conversion.py -v` -- all tests pass
- `grep -n 'apply_selective_fp32' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- method in both
- If this plan was triggered by 08-03 failure: re-run `pytest tests/integration/test_fp16_validation.py -v` with `selective_fp32=True` to confirm fix
</verification>

<success_criteria>
- apply_selective_fp32() converts LayerNorm to FP32, keeps attention/FFN in FP16
- Method works for both ESM-2 (known layer names) and DNABERT-S (generic LayerNorm detection)
- selective_fp32 parameter defaults to False (no behavior change unless explicitly requested)
- Unit tests verify layer dtype changes
- If used as fallback: re-running 08-03 validation tests passes with selective_fp32=True
</success_criteria>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-05-SUMMARY.md`
</output>
