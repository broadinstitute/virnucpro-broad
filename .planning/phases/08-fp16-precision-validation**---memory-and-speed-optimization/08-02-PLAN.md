---
phase: 08-fp16-precision-validation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/async_inference.py
  - virnucpro/pipeline/gpu_worker.py
  - tests/unit/test_fp16_conversion.py
autonomous: true

must_haves:
  truths:
    - "NaN/Inf in embeddings raises RuntimeError with diagnostic message"
    - "NaN/Inf detection uses single CUDA sync (optimized for <1ms overhead)"
    - "NaN/Inf detection mentions VIRNUCPRO_DISABLE_FP16 in error message"
    - "gpu_worker passes enable_fp16 to ESM-2 model loader with env var precedence"
    - "gpu_worker passes enable_fp16 to DNABERT-S model loader with env var precedence"
    - "VIRNUCPRO_DISABLE_FP16 env var overrides model_config enable_fp16 (safety-critical)"
    - "gpu_worker catches numerical instability RuntimeError and exits gracefully"
    - "Unit tests verify NaN/Inf detection and FP16 feature flag"
    - "End-to-end test verifies VIRNUCPRO_DISABLE_FP16=1 produces FP32 embeddings (not FP16)"
  artifacts:
    - path: "virnucpro/pipeline/async_inference.py"
      provides: "NaN/Inf detection in _run_inference"
      contains: "check_numerical_stability"
    - path: "virnucpro/pipeline/gpu_worker.py"
      provides: "FP16 passthrough in model loading for both ESM-2 and DNABERT-S"
      contains: "enable_fp16"
    - path: "tests/unit/test_fp16_conversion.py"
      provides: "Unit tests for FP16 conversion, stability, and end-to-end verification"
      min_lines: 100
  key_links:
    - from: "virnucpro/pipeline/async_inference.py"
      to: "NaN/Inf detection"
      via: "check_numerical_stability after model forward"
      pattern: "check_numerical_stability"
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/models/esm2_flash.py"
      via: "enable_fp16 kwarg in ESM-2 loading"
      pattern: "enable_fp16"
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/models/dnabert_flash.py"
      via: "enable_fp16 kwarg in DNABERT-S loading"
      pattern: "enable_fp16"
---

<objective>
Add NaN/Inf numerical stability detection to the inference pipeline and wire FP16 flag through gpu_worker for both ESM-2 and DNABERT-S with env var precedence and error handling.

Purpose: FP16 can cause overflow in LayerNorm or attention. NaN/Inf detection catches this early with actionable error messages. The gpu_worker must pass the FP16 flag to model loading for both model types in multi-GPU inference, with env var taking precedence over config for safety-critical rollback. Error handling ensures graceful worker exit with partial result salvaging.

Output: Optimized NaN/Inf detection in async_inference.py (<1ms overhead), FP16 passthrough in gpu_worker.py with env var precedence (both ESM-2 and DNABERT-S), error handling for numerical instability, unit tests for all features.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/gpu_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add NaN/Inf detection to inference pipeline and wire FP16 in gpu_worker for both model types</name>
  <files>virnucpro/pipeline/async_inference.py, virnucpro/pipeline/gpu_worker.py</files>
  <action>
  **In async_inference.py:**

  Add a module-level function for numerical stability checking (optimized for single CUDA sync):
  ```python
  def check_numerical_stability(embeddings: torch.Tensor, context: str = "embeddings") -> None:
      """Detect NaN/Inf in tensors with minimal CUDA synchronization.

      Optimized to perform single sync point - batches all GPU operations before
      calling .item(). Reduces latency from ~5-10ms to <1ms per batch.

      Args:
          embeddings: Tensor to check (any shape)
          context: Description for error message (e.g., "batch_42")

      Raises:
          RuntimeError: If NaN or Inf detected, with diagnostic stats
      """
      # Batch all GPU operations (no sync yet)
      nan_mask = torch.isnan(embeddings)
      inf_mask = torch.isinf(embeddings)
      has_nan = nan_mask.any()
      has_inf = inf_mask.any()

      # Single sync point - only sync if error detected
      if has_nan.item() or has_inf.item():
          # Now collect diagnostics (already computed masks above)
          valid_mask = ~(nan_mask | inf_mask)
          valid_vals = embeddings[valid_mask]

          # Batch remaining .item() calls
          stats = {
              "nan_count": nan_mask.sum().item(),
              "inf_count": inf_mask.sum().item(),
              "valid_min": valid_vals.min().item() if valid_vals.numel() > 0 else float('nan'),
              "valid_max": valid_vals.max().item() if valid_vals.numel() > 0 else float('nan'),
          }

          raise RuntimeError(
              f"Numerical instability in {context}: "
              f"NaN={stats['nan_count']}, Inf={stats['inf_count']}, "
              f"valid range=[{stats['valid_min']:.2e}, {stats['valid_max']:.2e}]. "
              f"This may indicate FP16 overflow. Try VIRNUCPRO_DISABLE_FP16=1"
          )
  ```

  **In _run_inference method (after getting representations):**

  Add stability check after BOTH packed and unpacked paths, BEFORE returning:
  ```python
  # Check numerical stability (catches FP16 overflow)
  check_numerical_stability(representations, context=f"batch_{self._batch_count}")
  return representations
  ```

  Add the check at the TWO return points in _run_inference:
  1. After packed path (line ~164, before `return representations`)
  2. After unpacked path (line ~180, before `return representations`)

  **In gpu_worker.py:**

  **At top of file:** Add import for env var check:
  ```python
  from virnucpro.utils.precision import should_use_fp16
  ```

  **ESM-2 loading (around line 116-124):** Pass enable_fp16 with env var precedence:
  ```python
  if model_type == 'esm2':
      from virnucpro.models.esm2_flash import load_esm2_model

      # Environment variable takes precedence for safety-critical rollback
      # If VIRNUCPRO_DISABLE_FP16=1 is set, it overrides config
      if not should_use_fp16():
          enable_fp16 = False
      else:
          enable_fp16 = model_config.get('enable_fp16', True)

      model, batch_converter = load_esm2_model(
          model_name=model_config.get('model_name', 'esm2_t36_3B_UR50D'),
          device=str(device),
          enable_fp16=enable_fp16
      )
  ```

  **DNABERT-S loading (add elif branch after ESM-2):** Add new model type support with enable_fp16 and env var precedence:
  ```python
  elif model_type == 'dnabert':
      from virnucpro.models.dnabert_flash import load_dnabert_model

      # Environment variable takes precedence (same as ESM-2)
      if not should_use_fp16():
          enable_fp16 = False
      else:
          enable_fp16 = model_config.get('enable_fp16', True)

      model, tokenizer = load_dnabert_model(
          device=str(device),
          enable_fp16=enable_fp16
      )
      # DNABERT uses tokenizer directly, not batch_converter
      batch_converter = tokenizer
      logger.info("DNABERT-S model loaded")
  else:
      raise ValueError(f"Unsupported model type: {model_type}")
  ```

  **In inference loop (around line where runner._run_inference is called):** Add error handling for numerical instability:
  ```python
  try:
      embeddings = runner._run_inference(batch)
      # ... existing HDF5 write logic ...
  except RuntimeError as e:
      if "Numerical instability" in str(e):
          # Log which sequences failed for debugging
          logger.error(
              f"Rank {rank}: NaN/Inf detected in batch {batch_idx}. "
              f"Sequences: {batch.get('sequence_ids', 'unknown')}. "
              f"Error: {e}"
          )
          # Report failure to orchestrator
          results_queue.put({
              "rank": rank,
              "status": "numerical_instability",
              "failed_batch": batch_idx,
              "error": str(e)
          })
          # Fail this worker - orchestrator handles partial results (Phase 7 pattern)
          sys.exit(1)
      else:
          # Re-raise unexpected errors
          raise
  ```

  Update the model_config docstring to include `'model_type': 'esm2' or 'dnabert'` and note that `VIRNUCPRO_DISABLE_FP16` env var overrides `enable_fp16` config.
  </action>
  <verify>
  Run: `grep -n 'check_numerical_stability' virnucpro/pipeline/async_inference.py` -- should appear as function def and 2 call sites
  Run: `grep -n 'Optimized to perform single sync' virnucpro/pipeline/async_inference.py` -- docstring confirms CUDA sync optimization
  Run: `grep -n 'should_use_fp16' virnucpro/pipeline/gpu_worker.py` -- import present for env var precedence check
  Run: `grep -n 'Environment variable takes precedence' virnucpro/pipeline/gpu_worker.py` -- comment present in BOTH esm2 and dnabert loading
  Run: `grep -n 'Numerical instability' virnucpro/pipeline/gpu_worker.py` -- error handling for NaN/Inf in inference loop
  Run: `grep -n 'enable_fp16' virnucpro/pipeline/gpu_worker.py` -- should appear in BOTH esm2 and dnabert model loading branches
  Run: `grep -n 'dnabert' virnucpro/pipeline/gpu_worker.py` -- should appear in model loading elif branch
  Run: `grep -n 'VIRNUCPRO_DISABLE_FP16' virnucpro/pipeline/async_inference.py` -- should appear in error message
  Run: `pytest tests/unit/test_async_inference.py -v` -- existing tests pass
  Run: `pytest tests/unit/test_gpu_worker.py -v` -- existing tests pass
  </verify>
  <done>
  NaN/Inf detection added after every forward pass with single CUDA sync optimization (<1ms overhead vs 5-10ms). Error messages include VIRNUCPRO_DISABLE_FP16 hint. gpu_worker passes enable_fp16 with env var precedence (safety-critical rollback overrides config). RuntimeError handling logs failed batches and exits worker gracefully for partial result salvaging.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for FP16 conversion and NaN/Inf detection</name>
  <files>tests/unit/test_fp16_conversion.py</files>
  <action>
  Create `tests/unit/test_fp16_conversion.py` with unit tests for:

  1. **test_should_use_fp16_default** - Verify `should_use_fp16()` returns True by default
  2. **test_should_use_fp16_disabled** - Set `VIRNUCPRO_DISABLE_FP16=1`, verify returns False
  3. **test_should_use_fp16_disabled_true** - Set `VIRNUCPRO_DISABLE_FP16=true`, verify returns False
  4. **test_check_numerical_stability_clean** - Pass normal tensor, verify no exception
  5. **test_check_numerical_stability_nan** - Pass tensor with NaN, verify RuntimeError raised with "NaN" and "VIRNUCPRO_DISABLE_FP16" in message
  6. **test_check_numerical_stability_inf** - Pass tensor with Inf, verify RuntimeError raised
  7. **test_check_numerical_stability_mixed** - Pass tensor with both NaN and Inf, verify error reports both counts
  8. **test_esm2_model_init_fp16_flag** - Mock ESM-2 model, verify __init__ calls model.half() when enable_fp16=True
  9. **test_esm2_model_init_fp32_flag** - Mock ESM-2 model, verify __init__ does NOT call model.half() when enable_fp16=False
  10. **test_disable_fp16_produces_fp32_embeddings** - **END-TO-END VERIFICATION**: Mock load_esm2_model with VIRNUCPRO_DISABLE_FP16=1, verify model dtype is FP32, run forward pass on mock model, verify output embeddings are FP32 (not FP16). This validates the entire flag → model loading → inference path produces FP32 output as expected.

  Use `@patch.dict(os.environ, ...)` for env var tests.
  Use `unittest.mock.patch` for model tests (don't load real models).
  Import `check_numerical_stability` from `virnucpro.pipeline.async_inference`.
  Import `should_use_fp16` from `virnucpro.utils.precision`.

  Follow existing test naming: `test_{what}_{condition}_{expected_outcome}`.
  Use `pytest.raises(RuntimeError, match=...)` for exception tests.

  **For test #10 (end-to-end):** Mock the model's forward pass to return a tensor in the same dtype as the model. Assert that when VIRNUCPRO_DISABLE_FP16=1, the model is FP32 and outputs are FP32 (not FP16). This catches the bug where flag is ignored after first forward_packed call.
  </action>
  <verify>
  Run: `pytest tests/unit/test_fp16_conversion.py -v` -- all 10 tests pass
  Run: `pytest tests/unit/test_fp16_conversion.py -v -k "disable_fp16_produces"` -- end-to-end test passes
  Run: `pytest tests/unit/ -v --tb=short` -- no regressions
  </verify>
  <done>
  Unit tests verify: should_use_fp16() respects env var, check_numerical_stability catches NaN/Inf with diagnostic messages, ESM-2 model wrapper calls model.half() based on enable_fp16 flag. **END-TO-END TEST:** VIRNUCPRO_DISABLE_FP16=1 actually produces FP32 embeddings throughout inference (not just at model loading).
  </done>
</task>

</tasks>

<verification>
- `pytest tests/unit/test_fp16_conversion.py -v` -- all 10 tests pass
- `pytest tests/unit/ -v` -- no regressions in existing tests
- `grep -n 'check_numerical_stability' virnucpro/pipeline/async_inference.py` -- function defined and called
- `grep -n 'single sync' virnucpro/pipeline/async_inference.py` -- CUDA sync optimization documented
- `grep -n 'should_use_fp16' virnucpro/pipeline/gpu_worker.py` -- env var precedence check present
- `grep -n 'Environment variable takes precedence' virnucpro/pipeline/gpu_worker.py` -- comment in both model loading paths
- `grep -n 'Numerical instability' virnucpro/pipeline/gpu_worker.py` -- error handling for NaN/Inf present
- `grep -n 'enable_fp16' virnucpro/pipeline/gpu_worker.py` -- wired through for both esm2 and dnabert with precedence logic
</verification>

<success_criteria>
- NaN/Inf detection raises RuntimeError with actionable VIRNUCPRO_DISABLE_FP16 hint
- NaN/Inf detection optimized for single CUDA sync (<1ms overhead vs 5-10ms unoptimized)
- Detection runs after every forward pass (both packed and unpacked paths)
- Environment variable VIRNUCPRO_DISABLE_FP16 takes precedence over model_config.enable_fp16 (safety-critical rollback)
- gpu_worker catches numerical instability RuntimeError, logs failed sequences, exits gracefully for partial result salvaging
- gpu_worker passes enable_fp16 with env var precedence to both ESM-2 and DNABERT-S model loaders
- 10 unit tests pass covering feature flag, stability detection, model init, and end-to-end FP32 verification
- End-to-end test verifies VIRNUCPRO_DISABLE_FP16=1 produces FP32 embeddings (catches flag-ignored-after-first-call bug)
- No regressions in existing test suite
</success_criteria>

<notes>
**Deferred to execution or Phase 10:**
- Realistic FP16 overflow tests (e.g., LayerNorm on large values) - current mocked NaN tests catch basic bugs, integration tests in Plan 08-03 will test real model overflow
- Operational monitoring metrics (e.g., FP16 failure rate tracking) - Phase 10 telemetry can add structured logging for production monitoring
</notes>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-02-SUMMARY.md`
</output>
