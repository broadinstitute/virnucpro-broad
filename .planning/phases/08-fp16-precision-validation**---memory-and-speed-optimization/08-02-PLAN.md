---
phase: 08-fp16-precision-validation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/async_inference.py
  - virnucpro/pipeline/gpu_worker.py
  - tests/unit/test_fp16_conversion.py
autonomous: true

must_haves:
  truths:
    - "NaN/Inf in embeddings raises RuntimeError with diagnostic message"
    - "NaN/Inf detection mentions VIRNUCPRO_DISABLE_FP16 in error message"
    - "gpu_worker passes enable_fp16 from model_config to model loader"
    - "Unit tests verify NaN/Inf detection and FP16 feature flag"
  artifacts:
    - path: "virnucpro/pipeline/async_inference.py"
      provides: "NaN/Inf detection in _run_inference"
      contains: "check_numerical_stability"
    - path: "virnucpro/pipeline/gpu_worker.py"
      provides: "FP16 passthrough in model loading"
      contains: "enable_fp16"
    - path: "tests/unit/test_fp16_conversion.py"
      provides: "Unit tests for FP16 conversion and stability"
      min_lines: 80
  key_links:
    - from: "virnucpro/pipeline/async_inference.py"
      to: "NaN/Inf detection"
      via: "check_numerical_stability after model forward"
      pattern: "check_numerical_stability"
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/models/esm2_flash.py"
      via: "enable_fp16 kwarg"
      pattern: "enable_fp16"
---

<objective>
Add NaN/Inf numerical stability detection to the inference pipeline and wire FP16 flag through gpu_worker.

Purpose: FP16 can cause overflow in LayerNorm or attention. NaN/Inf detection catches this early with actionable error messages. The gpu_worker must also pass the FP16 flag to model loading for multi-GPU inference.

Output: NaN/Inf detection in async_inference.py, FP16 passthrough in gpu_worker.py, unit tests for both.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@virnucpro/pipeline/async_inference.py
@virnucpro/pipeline/gpu_worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add NaN/Inf detection to inference pipeline and wire FP16 in gpu_worker</name>
  <files>virnucpro/pipeline/async_inference.py, virnucpro/pipeline/gpu_worker.py</files>
  <action>
  **In async_inference.py:**

  Add a module-level function for numerical stability checking:
  ```python
  def check_numerical_stability(embeddings: torch.Tensor, context: str = "embeddings") -> None:
      """Detect NaN/Inf in tensors. Raises RuntimeError if found."""
      has_nan = torch.isnan(embeddings).any().item()
      has_inf = torch.isinf(embeddings).any().item()

      if has_nan or has_inf:
          # Compute stats on valid values for diagnostics
          valid_mask = ~torch.isnan(embeddings) & ~torch.isinf(embeddings)
          valid_vals = embeddings[valid_mask]
          stats = {
              "nan_count": torch.isnan(embeddings).sum().item(),
              "inf_count": torch.isinf(embeddings).sum().item(),
              "valid_min": valid_vals.min().item() if valid_vals.numel() > 0 else float('nan'),
              "valid_max": valid_vals.max().item() if valid_vals.numel() > 0 else float('nan'),
          }
          raise RuntimeError(
              f"Numerical instability in {context}: "
              f"NaN={stats['nan_count']}, Inf={stats['inf_count']}, "
              f"valid range=[{stats['valid_min']:.2e}, {stats['valid_max']:.2e}]. "
              f"This may indicate FP16 overflow. Try VIRNUCPRO_DISABLE_FP16=1"
          )
  ```

  **In _run_inference method (after getting representations):**

  Add stability check after BOTH packed and unpacked paths, BEFORE returning:
  ```python
  # Check numerical stability (catches FP16 overflow)
  check_numerical_stability(representations, context=f"batch_{self._batch_count}")
  return representations
  ```

  Add the check at the TWO return points in _run_inference:
  1. After packed path (line ~164, before `return representations`)
  2. After unpacked path (line ~180, before `return representations`)

  **In gpu_worker.py:**

  In the ESM-2 model loading section (around line 118-124), pass enable_fp16:
  ```python
  if model_type == 'esm2':
      from virnucpro.models.esm2_flash import load_esm2_model
      model, batch_converter = load_esm2_model(
          model_name=model_config.get('model_name', 'esm2_t36_3B_UR50D'),
          device=str(device),
          enable_fp16=model_config.get('enable_fp16', None)  # None = check env var
      )
  ```
  </action>
  <verify>
  Run: `grep -n 'check_numerical_stability' virnucpro/pipeline/async_inference.py` -- should appear as function def and 2 call sites
  Run: `grep -n 'enable_fp16' virnucpro/pipeline/gpu_worker.py` -- should appear in model loading
  Run: `grep -n 'VIRNUCPRO_DISABLE_FP16' virnucpro/pipeline/async_inference.py` -- should appear in error message
  Run: `pytest tests/unit/test_async_inference.py -v` -- existing tests pass
  Run: `pytest tests/unit/test_gpu_worker.py -v` -- existing tests pass
  </verify>
  <done>
  NaN/Inf detection added after every forward pass in _run_inference (both packed and unpacked paths). Error messages include VIRNUCPRO_DISABLE_FP16 hint. gpu_worker passes enable_fp16 from model_config to load_esm2_model.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for FP16 conversion and NaN/Inf detection</name>
  <files>tests/unit/test_fp16_conversion.py</files>
  <action>
  Create `tests/unit/test_fp16_conversion.py` with unit tests for:

  1. **test_should_use_fp16_default** - Verify `should_use_fp16()` returns True by default
  2. **test_should_use_fp16_disabled** - Set `VIRNUCPRO_DISABLE_FP16=1`, verify returns False
  3. **test_should_use_fp16_disabled_true** - Set `VIRNUCPRO_DISABLE_FP16=true`, verify returns False
  4. **test_check_numerical_stability_clean** - Pass normal tensor, verify no exception
  5. **test_check_numerical_stability_nan** - Pass tensor with NaN, verify RuntimeError raised with "NaN" and "VIRNUCPRO_DISABLE_FP16" in message
  6. **test_check_numerical_stability_inf** - Pass tensor with Inf, verify RuntimeError raised
  7. **test_check_numerical_stability_mixed** - Pass tensor with both NaN and Inf, verify error reports both counts
  8. **test_esm2_model_init_fp16_flag** - Mock ESM-2 model, verify __init__ calls model.half() when enable_fp16=True
  9. **test_esm2_model_init_fp32_flag** - Mock ESM-2 model, verify __init__ does NOT call model.half() when enable_fp16=False

  Use `@patch.dict(os.environ, ...)` for env var tests.
  Use `unittest.mock.patch` for model tests (don't load real models).
  Import `check_numerical_stability` from `virnucpro.pipeline.async_inference`.
  Import `should_use_fp16` from `virnucpro.models.esm2_flash`.

  Follow existing test naming: `test_{what}_{condition}_{expected_outcome}`.
  Use `pytest.raises(RuntimeError, match=...)` for exception tests.
  </action>
  <verify>
  Run: `pytest tests/unit/test_fp16_conversion.py -v` -- all tests pass
  Run: `pytest tests/unit/ -v --tb=short` -- no regressions
  </verify>
  <done>
  Unit tests verify: should_use_fp16() respects env var, check_numerical_stability catches NaN/Inf with diagnostic messages, ESM-2 model wrapper calls model.half() based on enable_fp16 flag.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/unit/test_fp16_conversion.py -v` -- all tests pass
- `pytest tests/unit/ -v` -- no regressions in existing tests
- `grep -n 'check_numerical_stability' virnucpro/pipeline/async_inference.py` -- function defined and called
- `grep -n 'enable_fp16' virnucpro/pipeline/gpu_worker.py` -- wired through
</verification>

<success_criteria>
- NaN/Inf detection raises RuntimeError with actionable VIRNUCPRO_DISABLE_FP16 hint
- Detection runs after every forward pass (both packed and unpacked paths)
- gpu_worker passes enable_fp16 from model_config to model loader
- 9+ unit tests pass covering feature flag, stability detection, and model init
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-02-SUMMARY.md`
</output>
