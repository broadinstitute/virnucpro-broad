---
phase: 08-fp16-precision-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/models/esm2_flash.py
  - virnucpro/models/dnabert_flash.py
  - virnucpro/models/packed_attention.py
autonomous: true

must_haves:
  truths:
    - "ESM-2 model loads in FP16 by default (model.half())"
    - "DNABERT-S model loads in FP16 by default (model.half())"
    - "VIRNUCPRO_DISABLE_FP16 env var reverts to FP32"
    - "FlashAttention receives FP16 inputs (not BF16 auto-convert)"
    - "Embeddings stored in FP32 regardless of model precision"
  artifacts:
    - path: "virnucpro/models/esm2_flash.py"
      provides: "FP16 model loading with feature flag"
      contains: "model.half()"
    - path: "virnucpro/models/dnabert_flash.py"
      provides: "FP16 model loading with feature flag"
      contains: "model.half()"
    - path: "virnucpro/models/packed_attention.py"
      provides: "FlashAttention FP16 alignment"
  key_links:
    - from: "virnucpro/models/esm2_flash.py"
      to: "VIRNUCPRO_DISABLE_FP16"
      via: "os.getenv check in should_use_fp16()"
      pattern: "VIRNUCPRO_DISABLE_FP16"
    - from: "virnucpro/models/esm2_flash.py"
      to: "virnucpro/models/packed_attention.py"
      via: "forward_packed uses FP16 inputs natively"
      pattern: "forward_packed"
---

<objective>
Convert ESM-2 and DNABERT-S model loading to FP16 precision with feature flag rollback.

Purpose: FP16 is the foundation for throughput improvement. Models must load in FP16 by default, FlashAttention must receive FP16 inputs (not BF16 auto-convert), and a feature flag must allow emergency rollback to FP32.

Output: Modified model loaders that default to FP16 with VIRNUCPRO_DISABLE_FP16 rollback capability.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@virnucpro/models/esm2_flash.py
@virnucpro/models/dnabert_flash.py
@virnucpro/models/packed_attention.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add FP16 conversion to ESM-2 and DNABERT-S model loaders</name>
  <files>virnucpro/models/esm2_flash.py, virnucpro/models/dnabert_flash.py</files>
  <action>
  Add a module-level `should_use_fp16()` function to `esm2_flash.py` (DNABERT will import from same pattern):

  ```python
  def should_use_fp16() -> bool:
      """Check if FP16 should be enabled (default: True)."""
      disable = os.getenv("VIRNUCPRO_DISABLE_FP16", "").lower() in ("1", "true", "yes")
      if disable:
          logger.warning(
              "FP16 precision DISABLED via VIRNUCPRO_DISABLE_FP16. "
              "Using FP32 (slower, more memory). "
              "This is a diagnostic mode for troubleshooting."
          )
      return not disable
  ```

  **In ESM2WithFlashAttention.__init__:**
  - Remove the EXPERIMENTAL FP32 forcing block (`self.use_bf16 = False` and its log message)
  - Add `enable_fp16` parameter (default: True)
  - After `self.model = self.model.to(device)` and `self.model.eval()`:
    - If `enable_fp16` is True: call `self.model = self.model.half()`, log "Model converted to FP16 precision"
    - Store `self.use_fp16 = enable_fp16`
  - Update `__repr__` to show actual dtype: `'float16' if self.use_fp16 else 'float32'`

  **In load_esm2_model:**
  - Add `enable_fp16: Optional[bool] = None` parameter
  - If `enable_fp16 is None`: use `should_use_fp16()` to check env var
  - Pass `enable_fp16` to `ESM2WithFlashAttention.__init__`

  **In DNABERTWithFlashAttention.__init__ (dnabert_flash.py):**
  - Same pattern: remove EXPERIMENTAL FP32 forcing block
  - Add `enable_fp16` parameter (default: True)
  - After `self.model = self.model.to(device)` and `self.model.eval()`:
    - If `enable_fp16`: call `self.model = self.model.half()`
  - Store `self.use_fp16 = enable_fp16`
  - Update `__repr__`

  **In load_dnabert_model:**
  - Add `enable_fp16: Optional[bool] = None` parameter
  - If `enable_fp16 is None`: use same `should_use_fp16()` pattern (duplicate the function or import from a shared location -- duplicating is fine for 5 lines)
  - Pass to `DNABERTWithFlashAttention.__init__`

  Add `import os` to both files if not already present.
  </action>
  <verify>
  Run: `python -c "from virnucpro.models.esm2_flash import should_use_fp16; print(should_use_fp16())"` -- should print True
  Run: `VIRNUCPRO_DISABLE_FP16=1 python -c "from virnucpro.models.esm2_flash import should_use_fp16; print(should_use_fp16())"` -- should print False
  Run: `grep -n 'model.half()' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- both files should contain model.half()
  Run: `grep -n 'EXPERIMENTAL' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- should return nothing (removed)
  </verify>
  <done>
  ESM-2 and DNABERT-S models load in FP16 by default via model.half(). VIRNUCPRO_DISABLE_FP16=1 reverts to FP32. EXPERIMENTAL FP32 forcing removed from both model wrappers.
  </done>
</task>

<task type="auto">
  <name>Task 2: Align FlashAttention to FP16 (remove BF16 auto-convert)</name>
  <files>virnucpro/models/esm2_flash.py</files>
  <action>
  **In ESM2WithFlashAttention.forward_packed (esm2_flash.py, around line 210-219):**

  Replace the BF16 auto-convert block:
  ```python
  # OLD (remove this):
  if embeddings.dtype not in [torch.float16, torch.bfloat16]:
      logger.warning(
          f"Model loaded in {embeddings.dtype}, but FlashAttention requires FP16/BF16. "
          "Auto-converting to BF16 for packed inference. "
          ...
      )
      self.model = self.model.to(dtype=torch.bfloat16)
      embeddings = self.model.embed_tokens(input_ids)
  ```

  With FP16-first logic:
  ```python
  # FP16 alignment: if model is FP32 (VIRNUCPRO_DISABLE_FP16=1 or explicit),
  # convert to FP16 for FlashAttention (required). This is a one-time conversion
  # warning since FP32 model path still needs FlashAttention.
  if embeddings.dtype not in (torch.float16, torch.bfloat16):
      logger.warning(
          f"Model in {embeddings.dtype} but FlashAttention requires FP16/BF16. "
          "Auto-converting to FP16 for packed inference. "
          "For best performance, remove VIRNUCPRO_DISABLE_FP16."
      )
      self.model = self.model.half()
      embeddings = self.model.embed_tokens(input_ids)
  ```

  Key change: Auto-convert to FP16 (not BF16) to align with model precision. This handles the edge case where user sets VIRNUCPRO_DISABLE_FP16=1 but still uses packed inference.

  Also update the warning about BF16 in `_apply_rotary_embeddings` comment (line 374):
  - Change "Casting to BF16 too early" to "Casting to FP16 too early"
  - The actual sin/cos FP32 computation is correct and should remain
  </action>
  <verify>
  Run: `grep -n 'bfloat16\|BF16' virnucpro/models/esm2_flash.py` -- should only appear in FlashAttention dtype validation (valid_dtypes tuple) and the auto-convert warning, NOT in the actual conversion call
  Run: `grep -n 'self.model.half()' virnucpro/models/esm2_flash.py` -- should appear in __init__ and forward_packed auto-convert
  Run existing tests: `pytest tests/unit/test_esm2_packed.py -v` -- all should pass
  </verify>
  <done>
  FlashAttention auto-convert switched from BF16 to FP16. Entire inference pipeline aligned to FP16 precision. BF16 references removed from conversion paths (kept only in dtype validation where both are accepted).
  </done>
</task>

</tasks>

<verification>
- `grep -rn 'VIRNUCPRO_DISABLE_FP16' virnucpro/models/` confirms feature flag wired
- `grep -n 'model.half()' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` shows FP16 conversion in both
- `grep -n 'EXPERIMENTAL' virnucpro/models/` returns no results (cleaned up)
- `pytest tests/unit/ -v -k "esm2 or dnabert or packing"` passes (no regressions)
</verification>

<success_criteria>
- ESM-2 and DNABERT-S default to FP16 via model.half()
- VIRNUCPRO_DISABLE_FP16 env var provides FP32 rollback
- FlashAttention auto-converts to FP16 (not BF16) when needed
- Embeddings still stored in FP32 (existing pattern in _extract_embeddings unchanged)
- All existing unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-01-SUMMARY.md`
</output>
