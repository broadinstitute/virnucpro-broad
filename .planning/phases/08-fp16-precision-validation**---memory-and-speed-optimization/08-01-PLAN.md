---
phase: 08-fp16-precision-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/utils/precision.py
  - virnucpro/models/esm2_flash.py
  - virnucpro/models/dnabert_flash.py
  - virnucpro/models/packed_attention.py
autonomous: true

must_haves:
  truths:
    - "ESM-2 model loads in FP16 by default (model.half())"
    - "DNABERT-S model loads in FP16 by default (model.half())"
    - "VIRNUCPRO_DISABLE_FP16 env var reverts to FP32"
    - "FlashAttention receives FP16 inputs (not BF16 auto-convert)"
    - "Embeddings stored in FP32 regardless of model precision"
  artifacts:
    - path: "virnucpro/utils/precision.py"
      provides: "Shared should_use_fp16() utility (single source of truth)"
      contains: "should_use_fp16"
    - path: "virnucpro/models/esm2_flash.py"
      provides: "FP16 model loading with feature flag"
      contains: "model.half()"
    - path: "virnucpro/models/dnabert_flash.py"
      provides: "FP16 model loading with feature flag"
      contains: "model.half()"
    - path: "virnucpro/models/packed_attention.py"
      provides: "FlashAttention FP16 validation (fail-fast, no mutation)"
  key_links:
    - from: "virnucpro/models/esm2_flash.py"
      to: "VIRNUCPRO_DISABLE_FP16"
      via: "os.getenv check in should_use_fp16()"
      pattern: "VIRNUCPRO_DISABLE_FP16"
    - from: "virnucpro/models/esm2_flash.py"
      to: "virnucpro/models/packed_attention.py"
      via: "forward_packed uses FP16 inputs natively"
      pattern: "forward_packed"
---

<objective>
Convert ESM-2 and DNABERT-S model loading to FP16 precision with feature flag rollback.

Purpose: FP16 is the foundation for throughput improvement. Models must load in FP16 by default, FlashAttention must receive FP16 inputs (not BF16 auto-convert), and a feature flag must allow emergency rollback to FP32.

Output: Modified model loaders that default to FP16 with VIRNUCPRO_DISABLE_FP16 rollback capability.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-CONTEXT.md
@.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-RESEARCH.md
@virnucpro/models/esm2_flash.py
@virnucpro/models/dnabert_flash.py
@virnucpro/models/packed_attention.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add FP16 conversion to ESM-2 and DNABERT-S model loaders</name>
  <files>virnucpro/utils/precision.py, virnucpro/models/esm2_flash.py, virnucpro/models/dnabert_flash.py</files>
  <action>
  **Create `virnucpro/utils/precision.py` (new file) with shared utility:**

  ```python
  """Precision utilities for FP16/FP32 model configuration.

  This module provides shared utilities for controlling model precision via
  the VIRNUCPRO_DISABLE_FP16 environment variable.
  """
  import os
  import logging

  logger = logging.getLogger('virnucpro.utils.precision')

  def should_use_fp16() -> bool:
      """Check if FP16 should be enabled (default: True).

      Set VIRNUCPRO_DISABLE_FP16=1 for:
      - **Debugging NaN/Inf issues:** Run same data in FP32 to isolate precision problems
      - **Autopsy mode:** Compare FP16 vs FP32 embeddings when investigating accuracy issues
      - **Legacy compatibility:** If FP16 causes model-specific problems in production
      - **Baseline comparison:** Establish FP32 baseline for performance validation

      Note: FP32 is 2x slower and uses 2x memory. Only disable for diagnostics.

      Returns:
          bool: True if FP16 should be used, False if FP32 (diagnostic mode)

      Environment:
          VIRNUCPRO_DISABLE_FP16: Set to "1", "true", or "yes" to disable FP16

      Example:
          # Normal production (FP16)
          $ python -m virnucpro predict input.fasta

          # Diagnostic mode (FP32 for troubleshooting)
          $ VIRNUCPRO_DISABLE_FP16=1 python -m virnucpro predict input.fasta
      """
      disable = os.getenv("VIRNUCPRO_DISABLE_FP16", "").lower() in ("1", "true", "yes")
      if disable:
          logger.warning(
              "FP16 precision DISABLED via VIRNUCPRO_DISABLE_FP16. "
              "Using FP32 (slower, more memory). "
              "This is a diagnostic mode for troubleshooting."
          )
      return not disable
  ```

  **In esm2_flash.py:**
  - Add import at top: `from virnucpro.utils.precision import should_use_fp16`

  **In ESM2WithFlashAttention.__init__:**
  - Remove the EXPERIMENTAL FP32 forcing block (`self.use_bf16 = False` and its log message)
  - Add `enable_fp16` parameter (default: True)
  - After `self.model = self.model.to(device)` and `self.model.eval()`:
    - If `enable_fp16` is True: call `self.model = self.model.half()`, log "Model converted to FP16 precision"
    - Store `self.use_fp16 = enable_fp16`
  - Update `__repr__` to show actual dtype: `'float16' if self.use_fp16 else 'float32'`

  **In load_esm2_model:**
  - Add `enable_fp16: Optional[bool] = None` parameter
  - If `enable_fp16 is None`: use `should_use_fp16()` (imported from virnucpro.utils.precision)
  - Pass `enable_fp16` to `ESM2WithFlashAttention.__init__`

  **In dnabert_flash.py:**
  - Add import at top: `from virnucpro.utils.precision import should_use_fp16`

  **In DNABERTWithFlashAttention.__init__:**
  - Same pattern: remove EXPERIMENTAL FP32 forcing block
  - Add `enable_fp16` parameter (default: True)
  - After `self.model = self.model.to(device)` and `self.model.eval()`:
    - If `enable_fp16`: call `self.model = self.model.half()`
  - Store `self.use_fp16 = enable_fp16`
  - Update `__repr__`

  **In load_dnabert_model:**
  - Add `enable_fp16: Optional[bool] = None` parameter
  - If `enable_fp16 is None`: use `should_use_fp16()` (imported from virnucpro.utils.precision)
  - Pass to `DNABERTWithFlashAttention.__init__`
  </action>
  <verify>
  Run: `python -c "from virnucpro.utils.precision import should_use_fp16; print(should_use_fp16())"` -- should print True
  Run: `VIRNUCPRO_DISABLE_FP16=1 python -c "from virnucpro.utils.precision import should_use_fp16; print(should_use_fp16())"` -- should print False
  Run: `grep -n 'from virnucpro.utils.precision import should_use_fp16' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- both files should import from shared utility
  Run: `grep -n 'model.half()' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- both files should contain model.half()
  Run: `grep -n 'EXPERIMENTAL' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` -- should return nothing (removed)
  Run: `test -f virnucpro/utils/precision.py` -- shared utility file exists
  </verify>
  <done>
  ESM-2 and DNABERT-S models load in FP16 by default via model.half(). VIRNUCPRO_DISABLE_FP16=1 reverts to FP32. EXPERIMENTAL FP32 forcing removed from both model wrappers.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add fail-fast FP16 validation to forward_packed (no model mutation)</name>
  <files>virnucpro/models/esm2_flash.py</files>
  <action>
  **In ESM2WithFlashAttention.forward_packed (esm2_flash.py, around line 210-219):**

  Replace the BF16 auto-convert block with fail-fast validation:
  ```python
  # OLD (remove this - it mutates self.model):
  if embeddings.dtype not in [torch.float16, torch.bfloat16]:
      logger.warning(
          f"Model loaded in {embeddings.dtype}, but FlashAttention requires FP16/BF16. "
          "Auto-converting to BF16 for packed inference. "
          ...
      )
      self.model = self.model.to(dtype=torch.bfloat16)
      embeddings = self.model.embed_tokens(input_ids)
  ```

  **NEW (fail-fast validation - no mutation):**
  ```python
  # Defensive validation: FlashAttention varlen requires FP16/BF16 inputs
  # Fail fast if user explicitly disabled FP16 but tries to use packed inference
  if embeddings.dtype not in (torch.float16, torch.bfloat16):
      raise TypeError(
          f"Packed inference requires FP16/BF16 model. Got {embeddings.dtype}. "
          f"Either remove VIRNUCPRO_DISABLE_FP16 (recommended) or use unpacked inference. "
          f"Packed inference cannot run in FP32 due to FlashAttention requirements."
      )

  # Verify FP16 is used (not BF16) for best throughput
  if embeddings.dtype == torch.bfloat16:
      logger.warning(
          "Model using BF16 but FP16 is recommended for better throughput. "
          "Ensure load_esm2_model uses enable_fp16=True (default)."
      )
  ```

  **Key changes:**
  1. **No model mutation:** Don't call `self.model.half()` or change dtype
  2. **Fail fast:** Raise TypeError immediately if FP32 model used with packed inference
  3. **Clear guidance:** Error message tells user to remove VIRNUCPRO_DISABLE_FP16 or use unpacked inference
  4. **BF16 warning:** If somehow BF16 is used, warn that FP16 is better (but don't block)

  Also update the comment in `_apply_rotary_embeddings` (line 374):
  - Change "Casting to BF16 too early" to "Casting to FP16 too early"
  - The actual sin/cos FP32 computation is correct and should remain
  </action>
  <verify>
  Run: `grep -n 'raise TypeError' virnucpro/models/esm2_flash.py` -- should appear in forward_packed for FP32 detection
  Run: `grep -n 'Packed inference requires FP16/BF16' virnucpro/models/esm2_flash.py` -- fail-fast error message present
  Run: `grep -n 'self.model.half()' virnucpro/models/esm2_flash.py` -- should appear ONLY in __init__, NOT in forward_packed (no mutation)
  Run: `grep -n 'self.model = self.model.to' virnucpro/models/esm2_flash.py` -- should NOT appear in forward_packed (no dtype mutation)
  Run existing tests: `pytest tests/unit/test_esm2_packed.py -v` -- all should pass
  </verify>
  <done>
  FlashAttention validation changed from silent model mutation to fail-fast TypeError. FP32 models cannot use packed inference - user must remove VIRNUCPRO_DISABLE_FP16 or use unpacked inference. No model state mutation in forward_packed.
  </done>
</task>

</tasks>

<verification>
- `grep -rn 'VIRNUCPRO_DISABLE_FP16' virnucpro/models/` confirms feature flag wired
- `grep -n 'model.half()' virnucpro/models/esm2_flash.py virnucpro/models/dnabert_flash.py` shows FP16 conversion in both
- `grep -n 'EXPERIMENTAL' virnucpro/models/` returns no results (cleaned up)
- `pytest tests/unit/ -v -k "esm2 or dnabert or packing"` passes (no regressions)
</verification>

<success_criteria>
- ESM-2 and DNABERT-S default to FP16 via model.half()
- VIRNUCPRO_DISABLE_FP16 env var provides FP32 rollback
- FlashAttention auto-converts to FP16 (not BF16) when needed
- Embeddings still stored in FP32 (existing pattern in _extract_embeddings unchanged)
- All existing unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-fp16-precision-validation**---memory-and-speed-optimization/08-01-SUMMARY.md`
</output>
