---
phase: 03-dimension-compatibility
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - train.py
  - prediction.py
autonomous: true

must_haves:
  truths:
    - "MLPClassifier uses input_dim=2048 (was 3328)"
    - "Training checkpoint saves with metadata including model type, dimensions, and version 2.0.0"
    - "Old ESM2 3B checkpoints (no metadata or version 1.x) are rejected with helpful migration message"
    - "prediction.py uses extract_fast_esm() instead of extract_esm()"
    - "MLPClassifier.forward() validates input dimensions (critical path, always runs)"
  artifacts:
    - path: "train.py"
      provides: "Updated MLPClassifier with input_dim=2048, checkpoint save with metadata"
      contains: "input_dim = MERGED_DIM"
    - path: "prediction.py"
      provides: "Checkpoint load validation, namespace protection, FastESM2 prediction pipeline"
      contains: "load_checkpoint_with_validation"
  key_links:
    - from: "train.py"
      to: "units.py:MERGED_DIM"
      via: "import and use for input_dim"
      pattern: "from units import.*MERGED_DIM"
    - from: "train.py"
      to: "units.py:CHECKPOINT_VERSION"
      via: "import and embed in checkpoint metadata"
      pattern: "CHECKPOINT_VERSION"
    - from: "prediction.py"
      to: "units.py:DimensionError"
      via: "import for checkpoint validation"
      pattern: "from units import.*DimensionError"
    - from: "prediction.py:make_predictdata()"
      to: "units.py:extract_fast_esm()"
      via: "function call replacing extract_esm()"
      pattern: "extract_fast_esm"
    - from: "train.py:MLPClassifier.forward()"
      to: "units.py:DimensionError"
      via: "raises on input shape mismatch"
      pattern: "raise DimensionError"
---

<objective>
Update MLPClassifier to use 2048-dim input, add checkpoint metadata with version tracking, add checkpoint load validation that rejects old ESM2 3B checkpoints, and switch prediction.py to use extract_fast_esm().

Purpose: Without updating the model dimensions and adding checkpoint protection, training would silently fail with wrong dimensions and old checkpoints could be loaded producing nonsensical results. This completes the dimension migration for the training and inference pipelines.

Output: Updated train.py with MERGED_DIM-based MLPClassifier, checkpoint save with metadata (version 2.0.0). Updated prediction.py with checkpoint validation, namespace protection, and FastESM2 extraction.
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-dimension-compatibility/03-CONTEXT.md
@.planning/phases/03-dimension-compatibility/03-RESEARCH.md
@.planning/phases/03-dimension-compatibility/03-01-SUMMARY.md
@train.py
@prediction.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update train.py - MLPClassifier dimension, forward validation, and checkpoint metadata</name>
  <files>train.py</files>
  <action>
Modify train.py with the following changes:

1. **Add imports** at the top:
```python
from units import DimensionError, DNA_DIM, PROTEIN_DIM, MERGED_DIM, CHECKPOINT_VERSION
import datetime
```

2. **Update MLPClassifier.forward()** to add critical-path dimension validation (always runs, per user decision):
- Store `input_dim` as `self.input_dim` in `__init__`
- In `forward()`, before the hidden layer: check `x.shape[-1] != self.input_dim`, if so raise DimensionError with expected_dim=self.input_dim, actual_dim=x.shape[-1], tensor_name="model_input", location="MLPClassifier.forward()"

3. **Update input_dim** (line 105): Change `input_dim = 3328` to `input_dim = MERGED_DIM` (which equals 2048). Keep hidden_dim=512 and num_class=2 unchanged.

4. **Create save_checkpoint_with_metadata() function** (add before train_model):
Per user decision, save checkpoint with metadata dict containing:
- checkpoint_version: CHECKPOINT_VERSION ("2.0.0")
- model_type: 'fastesm650'
- huggingface_model_id: 'Synthyra/FastESM2_650'
- dna_dim: DNA_DIM (768)
- protein_dim: PROTEIN_DIM (1280)
- merged_dim: MERGED_DIM (2048)
- input_dim: MERGED_DIM
- hidden_dim: 512
- num_class: 2
- training_date: datetime.datetime.now().isoformat()
- pytorch_version: torch.__version__

Save format: {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'best_loss': best_loss, 'metadata': {...}}

Per user decision on filename convention: default filepath should be 'model_fastesm650.pth' (not 'model.pth').

5. **Update train_model()**: Replace `torch.save(model, 'model.pth')` (line 168) with call to `save_checkpoint_with_metadata(model, mlp_optimizer, epoch, early_stopping.best_score)`. This requires passing optimizer and epoch info. Adjust the early_stopping block accordingly. The function should accept the optimizer as a parameter or access it from the enclosing scope.

Note: train.py currently saves the entire model object with `torch.save(model, 'model.pth')`. The new approach saves a state_dict-based checkpoint with metadata instead. This is a deliberate change for Phase 3 - the new checkpoint format with metadata is required for version tracking (DIM-04).
  </action>
  <verify>
Run: `python -c "
from train import MLPClassifier, save_checkpoint_with_metadata
from units import MERGED_DIM, DimensionError
import torch

# Test 1: MLPClassifier uses correct input dim
model = MLPClassifier(MERGED_DIM, 512, 2)
print(f'MLPClassifier input_dim = {model.input_dim} (expected {MERGED_DIM})')
assert model.input_dim == MERGED_DIM

# Test 2: Forward pass with correct dimensions works
x = torch.randn(4, MERGED_DIM)
model.eval()
with torch.no_grad():
    output = model(x)
print(f'Forward pass shape: {output.shape} (expected [4, 2])')
assert output.shape == (4, 2)

# Test 3: Forward pass with wrong dimensions raises DimensionError
try:
    x_wrong = torch.randn(4, 3328)  # Old dimension
    with torch.no_grad():
        model(x_wrong)
    print('ERROR: Should have raised DimensionError!')
except DimensionError as e:
    print(f'Correctly caught: {e}')
    assert e.expected_dim == MERGED_DIM
    assert e.actual_dim == 3328

print('All train.py tests passed')
"` -- Note: train.py has module-level code that expects data files. If this fails due to missing data directory, test the imports and class directly by extracting them or verify with a simpler import test.

If module-level execution blocks testing, verify by reading the file and confirming:
1. `input_dim = MERGED_DIM` replaces `input_dim = 3328`
2. MLPClassifier.__init__ stores self.input_dim
3. MLPClassifier.forward() checks x.shape[-1] != self.input_dim
4. save_checkpoint_with_metadata() function exists with correct metadata structure
5. train_model() calls save_checkpoint_with_metadata instead of torch.save(model, ...)
  </verify>
  <done>
train.py uses MERGED_DIM (2048) for input_dim instead of hardcoded 3328. MLPClassifier.forward() validates input dimensions with DimensionError on mismatch (critical path, always runs). save_checkpoint_with_metadata() saves checkpoint with version 2.0.0 metadata. Filename convention uses model_fastesm650.pth. DIM-03 and DIM-04 requirements satisfied.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update prediction.py - Checkpoint validation, namespace protection, and FastESM2 extraction</name>
  <files>prediction.py</files>
  <action>
Modify prediction.py with the following changes:

1. **Add imports** at the top (after `from units import *`):
```python
from units import DimensionError, DNA_DIM, PROTEIN_DIM, MERGED_DIM, CHECKPOINT_VERSION
```
(Note: `from units import *` already imports everything, but explicit imports make dependencies clear and serve as documentation.)

2. **Update MLPClassifier** (duplicate class in prediction.py at line 48):
- Store `self.input_dim = input_dim` in `__init__`
- Add dimension validation in `forward()` identical to train.py: check x.shape[-1] != self.input_dim, raise DimensionError on mismatch
- This is a critical path validation that always runs per user decision

3. **Add load_checkpoint_with_validation() function** (add before predict()):
Per user decision, this function:
- Loads checkpoint with `torch.load(checkpoint_path, weights_only=False)`
- Checks if 'metadata' key exists. If not: raise ValueError with message "This checkpoint is from ESM2 3B pipeline (no metadata found). Re-extract features with FastESM2_650 and retrain. Old checkpoint: {checkpoint_path}"
- Checks checkpoint_version from metadata. Parse major version (int(version.split('.')[0])). If major < 2: raise ValueError with message "This checkpoint uses ESM2 3B (2560-dim, version {version}). Re-extract features with FastESM2_650 and retrain. Incompatible checkpoint: {checkpoint_path}"
- Validates merged_dim from metadata matches MERGED_DIM. If mismatch: raise DimensionError with expected=MERGED_DIM, actual from metadata
- Prints checkpoint info (version, model type, dimensions, training date)
- Returns checkpoint dict

4. **Update make_predictdata()** to use FastESM2 instead of ESM2:
- Replace `extract_esm(fasta_file=file, out_file=...)` calls (line 149) with `extract_fast_esm(fasta_file=file, out_file=...)`. Note: extract_fast_esm() needs model and tokenizer parameters. Load FastESM2 model at the top of make_predictdata() the same way features_extract.py does:
```python
FastESM_model = AutoModel.from_pretrained("Synthyra/FastESM2_650", trust_remote_code=True, torch_dtype=torch.float16).eval().cuda()
FastESM_tokenizer = FastESM_model.tokenizer
```
Then pass model=FastESM_model, tokenizer=FastESM_tokenizer to extract_fast_esm().

- Per user decision on filename convention: Update output filenames for protein features from `*_ESM.pt` to `*_fastesm.pt` to distinguish new vs old feature files.

5. **Update model loading** in make_predictdata():
Replace `mlp_model = torch.load(model_path, weights_only=False)` (line 168) with:
```python
checkpoint = load_checkpoint_with_validation(model_path)
mlp_model = MLPClassifier(
    input_dim=checkpoint['metadata']['input_dim'],
    hidden_dim=checkpoint['metadata']['hidden_dim'],
    num_class=checkpoint['metadata']['num_class']
)
mlp_model.load_state_dict(checkpoint['model_state_dict'])
```

This ensures old checkpoints without metadata or with version 1.x are rejected with clear error messages (DIM-05 namespace protection).
  </action>
  <verify>
Verify by reading prediction.py and confirming:
1. load_checkpoint_with_validation() function exists
2. It rejects checkpoints without 'metadata' key (ValueError with migration message)
3. It rejects checkpoints with version 1.x (ValueError with migration message)
4. It validates merged_dim against MERGED_DIM constant
5. MLPClassifier.forward() has DimensionError validation
6. make_predictdata() uses extract_fast_esm() instead of extract_esm()
7. Model loading uses load_checkpoint_with_validation() + state_dict pattern

Test checkpoint rejection: `python -c "
import torch, tempfile, os
from prediction import load_checkpoint_with_validation

# Test 1: Reject checkpoint without metadata (old format)
old_checkpoint = {'model_state_dict': {}, 'some_key': 'value'}
with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as f:
    torch.save(old_checkpoint, f.name)
    old_path = f.name

try:
    load_checkpoint_with_validation(old_path)
    print('ERROR: Should have rejected old checkpoint!')
except ValueError as e:
    print(f'Correctly rejected no-metadata checkpoint: {e}')
    assert 'ESM2 3B' in str(e)
os.unlink(old_path)

# Test 2: Reject checkpoint with version 1.x
v1_checkpoint = {'model_state_dict': {}, 'metadata': {'checkpoint_version': '1.0.0', 'merged_dim': 3328}}
with tempfile.NamedTemporaryFile(suffix='.pth', delete=False) as f:
    torch.save(v1_checkpoint, f.name)
    v1_path = f.name

try:
    load_checkpoint_with_validation(v1_path)
    print('ERROR: Should have rejected v1 checkpoint!')
except ValueError as e:
    print(f'Correctly rejected v1 checkpoint: {e}')
    assert '2560-dim' in str(e)
os.unlink(v1_path)

print('All checkpoint validation tests passed')
"` -- Note: if module-level sys.argv parsing blocks import, the verification may need to be done by file inspection instead.
  </verify>
  <done>
prediction.py uses load_checkpoint_with_validation() that rejects old ESM2 3B checkpoints (no metadata or version 1.x) with clear migration guidance. MLPClassifier.forward() validates input dimensions. make_predictdata() uses extract_fast_esm() with FastESM2_650 model. Model loading uses state_dict pattern with metadata-driven dimensions. DIM-05 namespace protection complete.
  </done>
</task>

</tasks>

<verification>
1. train.py: input_dim = MERGED_DIM (2048, not 3328)
2. train.py: MLPClassifier.forward() raises DimensionError on wrong input shape
3. train.py: save_checkpoint_with_metadata() embeds version 2.0.0 and dimension metadata
4. prediction.py: load_checkpoint_with_validation() rejects checkpoints without metadata
5. prediction.py: load_checkpoint_with_validation() rejects version 1.x checkpoints
6. prediction.py: extract_fast_esm() replaces extract_esm() in prediction pipeline
7. prediction.py: MLPClassifier.forward() has same dimension validation as train.py
8. No hardcoded 3328 or 2560 remaining in train.py or prediction.py
</verification>

<success_criteria>
- input_dim = MERGED_DIM (2048) in train.py
- MLPClassifier validates input dimensions in forward() (critical path, always runs)
- Checkpoint saved with metadata: version 2.0.0, model_type fastesm650, all dimensions
- Old checkpoints (no metadata) produce ValueError with "Re-extract features with FastESM2_650 and retrain"
- Version 1.x checkpoints produce ValueError with "ESM2 3B (2560-dim)" message
- prediction.py uses extract_fast_esm() instead of extract_esm()
</success_criteria>

<output>
After completion, create `.planning/phases/03-dimension-compatibility/03-02-SUMMARY.md`
</output>
