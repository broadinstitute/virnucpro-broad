---
phase: 02.1-parallel-embedding-merge
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/parallel_merge.py
  - virnucpro/pipeline/features.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Multiple file pairs can merge simultaneously using CPU cores"
    - "Workers process file pairs independently without shared state"
    - "Spawn context ensures compatibility with GPU-initialized environment"
  artifacts:
    - path: "virnucpro/pipeline/parallel_merge.py"
      provides: "Parallel merge worker functions and orchestration"
      min_lines: 200
      exports: ["merge_file_pair_worker", "parallel_merge_features", "parallel_merge_with_progress"]
    - path: "virnucpro/pipeline/features.py"
      provides: "Updated merge_features with checkpoint support"
      contains: "output_file.exists()"
  key_links:
    - from: "parallel_merge.py"
      to: "features.merge_features"
      via: "worker function import"
      pattern: "from virnucpro.pipeline.features import merge_features"
    - from: "parallel_merge_features"
      to: "multiprocessing.Pool"
      via: "spawn context pool creation"
      pattern: "ctx.Pool\\(num_workers\\)"
---

<objective>
Create parallel merge worker functions following Phase 1.1 patterns for CPU multiprocessing of embedding concatenation.

Purpose: Eliminate the sequential bottleneck in Stage 7 (Feature Merging) by parallelizing file pair processing across CPU cores.
Output: parallel_merge.py module with worker functions, parallel orchestration, and progress reporting.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-parallel-embedding-merge/02.1-RESEARCH.md

# Reference implementation from Phase 1.1
@virnucpro/pipeline/parallel_translate.py

# Current sequential implementation to parallelize
@virnucpro/pipeline/features.py:192-260
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create parallel merge worker functions</name>
  <files>virnucpro/pipeline/parallel_merge.py</files>
  <action>
    Create new module parallel_merge.py following Phase 1.1 pattern:

    1. Define top-level worker functions (required for spawn context picklability):
       - merge_file_pair_worker(file_pair) - processes single (nuc, pro, output) tuple
       - merge_batch_worker(file_pair_batch) - processes batch of file pairs

    2. Import features.merge_features INSIDE worker functions (avoid pickling heavy objects)

    3. Create parallel_merge_features() orchestration function:
       - Accept nucleotide_files, protein_files, output_dir, num_workers
       - Use multiprocessing.get_context('spawn') for consistency with GPU workers
       - Create file pair tuples with output paths
       - Use Pool.imap() with chunksize=1 for typical workloads (10-100 files)
       - Return list of successfully merged files

    4. Create parallel_merge_with_progress() for user-facing operations:
       - Same as parallel_merge_features but with ProgressReporter integration
       - Use progress.create_file_bar() matching existing pattern in prediction.py:573
       - Update progress bar after each file pair completes

    5. Add get_optimal_settings() helper for merge-specific tuning:
       - For merge, simpler than translation: num_workers = cpu_count()
       - Batch size = 1 (each file pair is already substantial work)
       - Chunksize = max(1, len(file_pairs) // (num_workers * 4))

    Reference parallel_translate.py for exact patterns, especially:
    - Lines 24-47: Worker function structure
    - Lines 211-212: Spawn context creation
    - Lines 228-233: Pool.imap() usage
    - Lines 186-193: Progress reporting pattern
  </action>
  <verify>python -c "from virnucpro.pipeline.parallel_merge import parallel_merge_features, merge_file_pair_worker"</verify>
  <done>parallel_merge.py exists with worker functions, orchestration, and progress reporting</done>
</task>

<task type="auto">
  <name>Task 2: Update features.py with checkpoint support</name>
  <files>virnucpro/pipeline/features.py</files>
  <action>
    Enhance merge_features() function to support resume capability:

    1. Add checkpoint check at start of merge_features():
       ```python
       # Skip if already merged (checkpoint support)
       if output_file.exists() and output_file.stat().st_size > 0:
           logger.info(f"Skipping merge - output exists: {output_file}")
           return output_file
       ```

    2. Add atomic write pattern for robustness:
       ```python
       # Save to temp file first (atomic write)
       temp_file = output_file.with_suffix('.tmp')
       torch.save(merged_dict, temp_file)
       temp_file.rename(output_file)
       ```

    3. Enhanced error handling with partial failure tolerance:
       - Log warning but don't raise if sequences missing
       - Return output_file even if some sequences couldn't merge

    Reference patterns:
    - prediction.py:336-340 for checkpoint skip pattern
    - Phase 1 GPU workers for atomic write pattern
  </action>
  <verify>grep -n "output_file.exists()" virnucpro/pipeline/features.py</verify>
  <done>features.py updated with checkpoint support and atomic writes</done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for parallel merge</name>
  <files>tests/test_parallel_merge.py</files>
  <action>
    Create comprehensive unit tests following Phase 1.1 test patterns:

    1. Test worker function with valid file pairs:
       - Create mock .pt files with torch.save()
       - Call merge_file_pair_worker() directly
       - Verify output file created with correct shape (3328-dim)

    2. Test parallel orchestration:
       - Create 10 mock file pairs
       - Run parallel_merge_features() with num_workers=2
       - Verify all 10 merged files created
       - Compare output to sequential merge_features() (identical results)

    3. Test error handling:
       - Test mismatched file pairs (different sequence IDs)
       - Test missing files (worker returns None)
       - Test partial failure (8/10 succeed, verify 8 outputs)

    4. Test spawn context usage:
       - Mock multiprocessing.get_context()
       - Verify called with 'spawn' argument

    5. Test progress reporting:
       - Mock ProgressReporter
       - Call parallel_merge_with_progress()
       - Verify progress.create_file_bar() called with correct count
       - Verify pbar.update() called for each file

    6. Test checkpoint behavior:
       - Pre-create some output files
       - Run merge, verify skipped (not reprocessed)

    Use unittest.TestCase and mock where appropriate.
    Reference tests/test_parallel_translate.py for patterns.
  </action>
  <verify>python -m pytest tests/test_parallel_merge.py -xvs</verify>
  <done>Unit tests pass for parallel merge worker and orchestration functions</done>
</task>

</tasks>

<verification>
# Module imports correctly
python -c "from virnucpro.pipeline import parallel_merge"

# Worker functions accessible
python -c "from virnucpro.pipeline.parallel_merge import merge_file_pair_worker, parallel_merge_features, parallel_merge_with_progress"

# Tests pass
python -m pytest tests/test_parallel_merge.py -xvs

# Checkpoint support in features.py
grep "exists()" virnucpro/pipeline/features.py | grep output_file
</verification>

<success_criteria>
- parallel_merge.py module created with worker functions and orchestration
- features.py updated with checkpoint support
- Unit tests validate correctness and parallelization
- Worker functions use spawn context matching Phase 1.1 pattern
- Progress reporting integrated for user visibility
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-parallel-embedding-merge/02.1-01-SUMMARY.md`
</output>