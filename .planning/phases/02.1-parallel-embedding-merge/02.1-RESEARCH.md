# Phase 2.1: Parallel Embedding Merge - Research

**Researched:** 2026-01-23
**Domain:** CPU multiprocessing for I/O-bound tensor merge operations
**Confidence:** HIGH

## Summary

Research investigated parallelizing the embedding merge step that concatenates DNABERT-S (768-dim) and ESM-2 (2560-dim) features to create merged embeddings (3328-dim). This operation was identified as a post-extraction bottleneck after GPU-optimized feature extraction in Phases 1 and 2.

The current implementation in `merge_features()` (features.py:192-260) processes file pairs sequentially: loads two .pt files via `torch.load()`, creates lookup dictionaries, concatenates tensors with `torch.cat()`, and saves merged output. With multi-GPU extraction now producing features rapidly, the sequential merge becomes the critical path.

The merge operation is embarrassingly parallel - each file pair (nucleotide + protein features) can be processed independently with no shared state. The workload is mixed I/O-bound (torch.load() reads from disk) and CPU-bound (dictionary construction, tensor concatenation). Python's multiprocessing.Pool with spawn context is the standard approach, matching established patterns from Phase 1.1 (Parallel Translation) and Phase 1/2 (GPU workers).

**Primary recommendation:** Use multiprocessing.Pool with spawn context and Pool.imap() for memory-efficient parallel merge. Reuse existing patterns from Phase 1.1 (batched worker, optimal settings) for consistency.

## Standard Stack

The established libraries/tools for this domain:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| multiprocessing | stdlib (3.9+) | Process-based parallelism for CPU tasks | Bypasses GIL, already used for translation (Phase 1.1) and GPU workers |
| torch.load/save | PyTorch 2.8.0+ | Load/save tensor files (.pt format) | Existing format, used throughout pipeline |
| torch.cat | PyTorch 2.8.0+ | Tensor concatenation along feature dimension | Core operation for merging embeddings |
| os.cpu_count() | stdlib | Detect available CPU cores | Standard for determining default worker count |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| multiprocessing.Queue | stdlib | Inter-process progress reporting | Already used in Phase 1.1 for progress updates |
| Path (pathlib) | stdlib | File path manipulation | Codebase standard for file operations |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| multiprocessing.Pool | concurrent.futures.ProcessPoolExecutor | ProcessPoolExecutor has cleaner API but Pool has better chunksize control and matches existing patterns |
| CPU multiprocessing | threading (multiprocessing.dummy) | Threading better for I/O-bound but GIL limits CPU-bound dict/concat; multiprocessing handles mixed workload |
| Pool.imap() | Pool.map() | map() is simpler but eager evaluation; imap() allows streaming results and progress updates |
| spawn context | fork context | fork faster startup but unsafe with CUDA (existing pattern), spawn is Python 3.14 default |

**Installation:**
No additional packages required - uses Python standard library multiprocessing and existing PyTorch.

## Architecture Patterns

### Recommended Project Structure
```
virnucpro/
├── pipeline/
│   ├── features.py           # Existing merge_features() - refactor for parallel
│   ├── parallel_merge.py     # NEW: Worker function for parallel merge
│   ├── parallel_translate.py # REFERENCE: Phase 1.1 pattern to follow
│   └── prediction.py         # Update Stage 7 to use parallel merge
```

### Pattern 1: Pool.imap() with File Pair Processing
**What:** Use Pool.imap() to process (nucleotide_file, protein_file) pairs in parallel.
**When to use:** When merging multiple file pairs (typical: 10-100+ pairs from multi-GPU extraction).
**Example:**
```python
# Source: Phase 1.1 parallel_translate.py pattern
from multiprocessing import Pool
from pathlib import Path
import torch

def merge_file_pair_worker(file_pair):
    """
    Worker function for parallel merge of a single file pair.

    Top-level function (not nested) required for picklability with spawn context.

    Args:
        file_pair: Tuple of (nucleotide_feature_file, protein_feature_file, output_file)

    Returns:
        output_file Path if successful, None if failed
    """
    nuc_file, pro_file, output_file = file_pair

    # Import inside worker to avoid pickling heavy objects
    from virnucpro.pipeline.features import merge_features

    try:
        merge_features(nuc_file, pro_file, output_file)
        return output_file
    except Exception as e:
        logger.error(f"Merge failed for {nuc_file.name}: {e}")
        return None

def parallel_merge_features(
    nucleotide_files: List[Path],
    protein_files: List[Path],
    output_dir: Path,
    num_workers: int = None
) -> List[Path]:
    """Parallelize feature merging across CPU cores"""
    if num_workers is None:
        num_workers = os.cpu_count()

    # Create file pair tuples for workers
    file_pairs = []
    for nuc_file, pro_file in zip(nucleotide_files, protein_files):
        base_name = nuc_file.stem.replace('_DNABERT_S', '')
        output_file = output_dir / f"{base_name}_merged.pt"
        file_pairs.append((nuc_file, pro_file, output_file))

    # Use spawn context (matches Phase 1.1 pattern)
    ctx = multiprocessing.get_context('spawn')

    merged_files = []
    with ctx.Pool(num_workers) as pool:
        # imap returns iterator - process results as they complete
        results = pool.imap(merge_file_pair_worker, file_pairs, chunksize=1)

        for result in results:
            if result is not None:
                merged_files.append(result)

    return merged_files
```

### Pattern 2: Batched Merge Worker (Memory Optimization)
**What:** Process multiple file pairs in a single worker call to reduce serialization overhead.
**When to use:** When processing 100+ file pairs (similar to Phase 1.1 batch worker pattern).
**Example:**
```python
# Source: Phase 1.1 translate_batch_worker pattern
def merge_batch_worker(file_pair_batch):
    """
    Worker function for parallel merge of a batch of file pairs.

    Reduces serialization overhead by processing multiple pairs per worker call.

    Args:
        file_pair_batch: List of (nuc_file, pro_file, output_file) tuples

    Returns:
        List of output files (None for failures)
    """
    from virnucpro.pipeline.features import merge_features

    results = []
    for nuc_file, pro_file, output_file in file_pair_batch:
        try:
            merge_features(nuc_file, pro_file, output_file)
            results.append(output_file)
        except Exception as e:
            logger.error(f"Merge failed for {nuc_file.name}: {e}")
            results.append(None)

    return results
```

### Pattern 3: Progress Reporting with Queue
**What:** Workers report progress through multiprocessing.Queue for live updates.
**When to use:** User-facing operations where progress visibility is important.
**Example:**
```python
# Source: Phase 1.1 parallel_translate_with_progress pattern
from virnucpro.utils.progress import ProgressReporter

def parallel_merge_with_progress(
    nucleotide_files: List[Path],
    protein_files: List[Path],
    output_dir: Path,
    num_workers: int = None,
    show_progress: bool = True
) -> List[Path]:
    """Parallel merge with progress bar"""
    if num_workers is None:
        num_workers = os.cpu_count()

    file_pairs = []
    for nuc_file, pro_file in zip(nucleotide_files, protein_files):
        base_name = nuc_file.stem.replace('_DNABERT_S', '')
        output_file = output_dir / f"{base_name}_merged.pt"
        file_pairs.append((nuc_file, pro_file, output_file))

    ctx = multiprocessing.get_context('spawn')
    progress = ProgressReporter(disable=not show_progress)

    merged_files = []
    with ctx.Pool(num_workers) as pool:
        results = pool.imap(merge_file_pair_worker, file_pairs, chunksize=1)

        with progress.create_file_bar(len(file_pairs), desc="Merging features") as pbar:
            for result in results:
                if result is not None:
                    merged_files.append(result)
                pbar.update(1)

    return merged_files
```

### Anti-Patterns to Avoid
- **Processing in main process:** Don't loop through file pairs sequentially in main process. The whole point is parallelizing I/O and CPU work.
- **Loading all tensors into memory first:** Don't load all feature files before merging. Process and save incrementally to avoid memory explosion.
- **Fine-grained progress updates:** Don't report progress for every sequence in a file. Report per-file (like current implementation) to avoid Queue flooding.
- **Fork context:** Don't use fork context. Codebase standardized on spawn for GPU worker compatibility and Python 3.14 compatibility.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Process pool lifecycle | Manual Process creation/joining | multiprocessing.Pool context manager | Handles cleanup, prevents resource leaks, established pattern |
| Worker function batching | Custom chunking logic | Phase 1.1 create_sequence_batches() pattern | Already tested, reduces serialization overhead 100x |
| Optimal worker count | Hardcode CPU count | Phase 1.1 get_optimal_settings() helper | Calculates workers, batch_size, chunksize based on data |
| Progress reporting | Custom Queue handling | Phase 1.1 parallel_translate_with_progress() pattern | Already integrated with ProgressReporter |
| File pairing logic | Custom zip with validation | Existing prediction.py:574 zip() pattern | Simple, already handles mismatched counts |

**Key insight:** Phase 1.1 (Parallel Translation) already solved the same problem: parallelizing an embarrassingly parallel CPU-bound operation with file I/O. The `parallel_translate.py` module provides battle-tested patterns for worker functions, batching, progress reporting, and optimal settings calculation. Reusing these patterns ensures consistency and avoids reinventing solutions.

## Common Pitfalls

### Pitfall 1: Mixed I/O and CPU Bottleneck Misconception
**What goes wrong:** Assuming merge is purely I/O-bound and threading would be better than multiprocessing.
**Why it happens:** torch.load() is I/O-bound, but dictionary construction and torch.cat() are CPU-bound. GIL limits threading performance for CPU work.
**How to avoid:** Use multiprocessing.Pool to parallelize both I/O (torch.load) and CPU work (dict creation, concatenation). Mixed workload benefits from process parallelism.
**Warning signs:** Profiling shows significant CPU time in merge_features(), not just I/O wait. Dictionary construction with 10k+ sequences takes non-trivial CPU time.

### Pitfall 2: Chunksize = 1 Overhead
**What goes wrong:** Using default chunksize=1 with Pool.imap() causes excessive serialization overhead when processing 100+ file pairs.
**Why it happens:** Each file pair tuple requires pickling, IPC, and result collection. With 100 pairs and chunksize=1, that's 100 round-trips.
**How to avoid:** For small numbers of files (<20), chunksize=1 is fine. For larger workloads, use Phase 1.1 pattern with batched workers or increase chunksize to 5-10.
**Warning signs:** High CPU usage in main process during pool.imap(), workers spend time waiting for work rather than processing.

### Pitfall 3: Forgetting spawn Context Requirement
**What goes wrong:** Using default context (fork on Linux) instead of spawn, causing subtle bugs or incompatibilities with CUDA-initialized environment.
**Why it happens:** Multiprocessing defaults to fork on Linux for historical reasons. This can inherit CUDA context state from parent process.
**How to avoid:** Explicitly use `multiprocessing.get_context('spawn')` like existing GPU workers and Phase 1.1. Consistency with codebase pattern, future-proof for Python 3.14.
**Warning signs:** Works in isolation but fails when pipeline runs after GPU extraction stages. Inconsistent with existing multiprocessing code.

### Pitfall 4: Not Handling Mismatched File Pairs
**What goes wrong:** Merge crashes if nucleotide_files and protein_files lists don't match (different lengths or ordering).
**Why it happens:** Multi-GPU extraction might fail on some files, or checkpoint resume skips different files for each stage.
**How to avoid:** Validate file pairing explicitly before merge. Match files by name/index, log warnings for missing pairs, skip gracefully.
**Warning signs:** IndexError or mismatched tensor shapes during merge. Silent data corruption if files paired incorrectly.

### Pitfall 5: Large Tensor Memory Accumulation
**What goes wrong:** Loading multiple merged tensors into memory before writing causes OOM for large datasets.
**Why it happens:** Each merged file is ~33MB for 10k sequences (3328 floats * 10k * 4 bytes). 100 files = 3.3GB. Multiple workers accumulating results.
**How to avoid:** Use Pool.imap() to stream results, write each merged file immediately. Don't accumulate results list unnecessarily.
**Warning signs:** Memory usage grows linearly with number of files, OOM crashes on large datasets, swapping/thrashing.

### Pitfall 6: Worker Function Not Picklable
**What goes wrong:** Workers fail to start with `PicklingError` if worker function uses closures or local imports incorrectly.
**Why it happens:** Spawn context requires worker function to be importable from top-level module scope.
**How to avoid:** Define worker function at module level (not nested in another function). Import heavy dependencies inside worker function body. Test: `pickle.dumps(worker_function)`.
**Warning signs:** `PicklingError: Can't pickle <function>`, workers never start, Pool creation fails.

## Code Examples

Verified patterns from Phase 1.1 and official sources:

### Basic Parallel Merge (Minimal)
```python
# Source: Phase 1.1 parallel_translate_sequences() pattern
from multiprocessing import Pool, cpu_count
from pathlib import Path
from typing import List
import torch

def merge_file_pair_worker(file_pair):
    """Top-level worker function for picklability"""
    nuc_file, pro_file, output_file = file_pair

    # Import inside worker to avoid pickling heavy objects
    from virnucpro.pipeline.features import merge_features

    try:
        merge_features(nuc_file, pro_file, output_file)
        return output_file
    except Exception as e:
        logger.error(f"Merge failed for {nuc_file.name}: {e}")
        return None

def parallel_merge_features(
    nucleotide_files: List[Path],
    protein_files: List[Path],
    output_dir: Path,
    num_workers: int = None
) -> List[Path]:
    """Parallelize feature merging across CPU cores"""
    if num_workers is None:
        num_workers = cpu_count()

    # Create file pairs
    file_pairs = [
        (nuc, pro, output_dir / f"{nuc.stem.replace('_DNABERT_S', '')}_merged.pt")
        for nuc, pro in zip(nucleotide_files, protein_files)
    ]

    merged_files = []
    with Pool(num_workers) as pool:
        results = pool.imap(merge_file_pair_worker, file_pairs, chunksize=1)
        for result in results:
            if result is not None:
                merged_files.append(result)

    return merged_files
```

### With Spawn Context (Production)
```python
# Source: Phase 1.1 parallel_translate_spawn() pattern
import multiprocessing
from pathlib import Path
from typing import List

def parallel_merge_spawn(
    nucleotide_files: List[Path],
    protein_files: List[Path],
    output_dir: Path,
    num_workers: int = None
) -> List[Path]:
    """Production version with spawn context"""
    if num_workers is None:
        num_workers = multiprocessing.cpu_count()

    # Explicit spawn context for safety and consistency
    ctx = multiprocessing.get_context('spawn')

    file_pairs = [
        (nuc, pro, output_dir / f"{nuc.stem.replace('_DNABERT_S', '')}_merged.pt")
        for nuc, pro in zip(nucleotide_files, protein_files)
    ]

    merged_files = []
    with ctx.Pool(num_workers) as pool:
        results = pool.imap(merge_file_pair_worker, file_pairs, chunksize=1)
        for result in results:
            if result is not None:
                merged_files.append(result)

    return merged_files
```

### With Progress Reporting (Full Integration)
```python
# Source: Phase 1.1 parallel_translate_with_progress() pattern
import multiprocessing
from pathlib import Path
from typing import List
from virnucpro.utils.progress import ProgressReporter

def parallel_merge_with_progress(
    nucleotide_files: List[Path],
    protein_files: List[Path],
    output_dir: Path,
    num_workers: int = None,
    show_progress: bool = True
) -> List[Path]:
    """Full integration with progress reporting"""
    if num_workers is None:
        num_workers = multiprocessing.cpu_count()

    ctx = multiprocessing.get_context('spawn')

    file_pairs = [
        (nuc, pro, output_dir / f"{nuc.stem.replace('_DNABERT_S', '')}_merged.pt")
        for nuc, pro in zip(nucleotide_files, protein_files)
    ]

    progress = ProgressReporter(disable=not show_progress)
    merged_files = []

    with ctx.Pool(num_workers) as pool:
        results = pool.imap(merge_file_pair_worker, file_pairs, chunksize=1)

        with progress.create_file_bar(len(file_pairs), desc="Merging features") as pbar:
            for result in results:
                if result is not None:
                    merged_files.append(result)
                pbar.update(1)
                pbar.set_postfix_str(f"Merged: {len(merged_files)}/{len(file_pairs)}")

    return merged_files
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Sequential file processing | Parallel Pool.imap() for independent files | Standard pattern since multiprocessing inception | Linear speedup with CPU cores for embarrassingly parallel tasks |
| fork as default context | spawn as default | Python 3.14 (2025) | Safer multiprocessing, consistent with GPU worker pattern |
| Eager Pool.map() | Lazy Pool.imap() for large datasets | Long-standing best practice | Memory-efficient streaming for 100+ files |
| Threading for I/O | Multiprocessing for mixed I/O+CPU | Community consensus (2020s) | Better performance for workloads with both I/O and CPU components |

**Deprecated/outdated:**
- **Sequential merge in main process:** Parallelizing independent file merges is standard practice for multi-file workloads.
- **Fork context with CUDA environment:** Fork can inherit CUDA state causing subtle bugs. Spawn is safer and Python 3.14 default.
- **Accumulating all results before processing:** Streaming results with imap() prevents memory bloat.

## Open Questions

Things that couldn't be fully resolved:

1. **Optimal chunksize for typical workload**
   - What we know: Phase 1.1 uses chunksize=1000 for 22M sequences. Merge typically has 10-100 file pairs.
   - What's unclear: With smaller numbers, chunksize=1 may be optimal. Trade-off between overhead and parallelism.
   - Recommendation: Start with chunksize=1 for simplicity. Add `--merge-chunk-size` CLI parameter if profiling shows benefit from batching.

2. **Threading vs Multiprocessing for small workloads**
   - What we know: Multiprocessing has higher overhead (process creation, IPC). Threading limited by GIL for CPU work.
   - What's unclear: At what workload size (number of files) does multiprocessing overhead exceed GIL cost?
   - Recommendation: Use multiprocessing to match existing patterns. Consistency with Phase 1.1 and GPU workers. Overhead negligible for 10+ files.

3. **Checkpoint integration for resume capability**
   - What we know: Current merge doesn't checkpoint intermediate results. Multi-GPU extraction has per-file checkpoints.
   - What's unclear: Whether merge should check for existing merged files and skip (like extraction stages do).
   - Recommendation: Check `output_file.exists()` before merge, skip if present. Allows resume if merge interrupted. Matches extraction pattern from prediction.py:336-340.

4. **Error handling strategy for failed merges**
   - What we know: GPU extraction logs failed files to failed_files.txt. Merge currently would fail entire pipeline on single file error.
   - What's unclear: Should merge follow same pattern (partial failure, log to file) or fail fast?
   - Recommendation: Return (merged_files, failed_pairs) tuple matching GPU worker pattern. Log failures but continue. Allows partial pipeline completion.

## Sources

### Primary (HIGH confidence)
- Python multiprocessing documentation: https://docs.python.org/3/library/multiprocessing.html (verified stdlib patterns)
- Codebase parallel_translate.py: /home/unix/carze/projects/virnucpro-broad/virnucpro/pipeline/parallel_translate.py (HIGH - Phase 1.1 production code)
- Codebase features.py: /home/unix/carze/projects/virnucpro-broad/virnucpro/pipeline/features.py (HIGH - existing merge_features implementation)
- Codebase prediction.py: /home/unix/carze/projects/virnucpro-broad/virnucpro/pipeline/prediction.py (HIGH - existing pipeline integration)
- PyTorch multiprocessing best practices: https://docs.pytorch.org/docs/stable/notes/multiprocessing.html (official PyTorch docs)

### Secondary (MEDIUM confidence)
- [Multiprocessing with Python - PythonIQ](https://medium.com/pythoniq/multiprocessing-with-python-ffa5e158ff01) - Best practices for Pool.imap()
- [Multiprocessing Pool.imap() in Python - Super Fast Python](https://superfastpython.com/multiprocessing-pool-imap/) - Usage patterns and performance
- [Multiprocessing in Python and PyTorch - TheLinuxCode](https://thelinuxcode.com/multiprocessing-in-python-and-pytorch-faster-training-safer-pipelines-realworld-patterns/) - Modern 2026 patterns
- [Python Multiprocessing Pool Ultimate Guide - Finxter](https://blog.finxter.com/python-multiprocessing-pool-ultimate-guide/) - Comprehensive Pool patterns

### Tertiary (LOW confidence - context only)
- [Embeddings Merge 101 - Medium](https://medium.com/mantisnlp/how-to-combine-several-embeddings-models-8e7bc9a00330) - Conceptual background on embedding concatenation (not specific to parallel processing)
- [How to Concatenate layers in PyTorch - Medium](https://medium.com/biased-algorithms/how-to-concatenate-layers-in-pytorch-402852d03b8d) - torch.cat usage examples

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Uses Python stdlib multiprocessing and PyTorch (already in codebase)
- Architecture: HIGH - Directly reuses Phase 1.1 patterns with minimal adaptation
- Pitfalls: HIGH - Well-documented in Python community and verified with official sources

**Research date:** 2026-01-23
**Valid until:** 2026-02-23 (30 days - stable domain, stdlib-based)

**Notes:**
- Phase 1.1 provides directly applicable pattern (parallel_translate.py)
- Merge operation is simpler than translation (no ORF detection, just load+concat+save)
- Existing checkpoint pattern from extraction stages (prediction.py:336-340) should be followed
- Error handling should match GPU worker pattern (partial failure tolerance)
