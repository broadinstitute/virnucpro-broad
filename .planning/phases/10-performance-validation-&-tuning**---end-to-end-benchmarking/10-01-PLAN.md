---
phase: 10-performance-validation-tuning
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/utils/telemetry.py
  - virnucpro/pipeline/progress_reporter.py
autonomous: true

must_haves:
  truths:
    - "TelemetryLogger records per-batch metrics (tokens/sec, GPU util, packing efficiency, wait times)"
    - "TelemetryLogger writes both JSON summary and per-batch detail to disk on finalize()"
    - "InferenceProgressReporter shows real-time progress bar with throughput, GPU util, and packing efficiency"
    - "Progress reporter integrates with existing NvitopMonitor metrics without modifying gpu_monitor.py"
    - "Telemetry supports multi-GPU shard aggregation for global metrics"
    - "Scaling efficiency calculation validates PERF-03 (2 GPUs = 1.9x+ speedup)"
    - "Hardware metadata captured for reproducible benchmarks"
  artifacts:
    - path: "virnucpro/utils/telemetry.py"
      provides: "TelemetryLogger, aggregate_telemetry_shards, calculate_scaling_efficiency, capture_hardware_info"
      min_lines: 300
    - path: "virnucpro/pipeline/progress_reporter.py"
      provides: "InferenceProgressReporter with rich.progress live metrics display"
      min_lines: 100
  key_links:
    - from: "virnucpro/utils/telemetry.py"
      to: "json"
      via: "JSON serialization for per-batch and summary metrics"
      pattern: "json\\.dump"
    - from: "virnucpro/utils/telemetry.py"
      to: "torch.cuda"
      via: "capture_hardware_info reads GPU device names and CUDA version"
      pattern: "torch\\.cuda\\.get_device_name"
    - from: "virnucpro/pipeline/progress_reporter.py"
      to: "rich.progress"
      via: "Rich progress bar with custom columns for live GPU metrics"
      pattern: "from rich\\.progress import"
    - from: "virnucpro/pipeline/progress_reporter.py"
      to: "virnucpro.utils.gpu_monitor"
      via: "Reads NvitopMonitor statistics for progress display"
      pattern: "NvitopMonitor"
---

<objective>
Create telemetry infrastructure: TelemetryLogger for per-batch metrics persistence and InferenceProgressReporter for real-time progress display.

Purpose: These are the foundation components for all Phase 10 benchmarks. TelemetryLogger captures detailed per-batch metrics to JSON for post-run analysis. InferenceProgressReporter provides live feedback during production runs showing throughput, GPU utilization, and packing efficiency. Both integrate with existing NvitopMonitor without modifying it.

Output: `virnucpro/utils/telemetry.py` and `virnucpro/pipeline/progress_reporter.py`
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-CONTEXT.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-RESEARCH.md
@virnucpro/utils/gpu_monitor.py
@virnucpro/pipeline/async_inference.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: TelemetryLogger for per-batch metrics capture and JSON persistence</name>
  <files>virnucpro/utils/telemetry.py</files>
  <action>
Create `virnucpro/utils/telemetry.py` with:

1. **TelemetryLogger** class:
   - `__init__(self, output_dir: Path, run_id: Optional[str] = None)`:
     - Creates output_dir if not exists
     - Generates timestamp-based run_id if not provided: `datetime.now().strftime("%Y%m%d_%H%M%S")`
     - JSON path: `output_dir / f"telemetry_{run_id}.json"`
     - Initializes `self.batch_metrics: List[Dict[str, Any]] = []`
     - Initializes `self.run_metadata: Dict[str, Any] = {}` for global run info
     - Stores `self.start_time = time.time()`

   - `set_run_metadata(self, **kwargs)` - set global run metadata (world_size, model_name, fasta_files, etc.)

   - `record_batch(self, batch_idx: int, metrics: Dict[str, Any])` - record per-batch metrics. Automatically adds `timestamp` field from `time.time()`. Expected metrics keys:
     - `tokens_per_sec`: float
     - `sequences_per_sec`: float
     - `gpu_util`: float (0-100)
     - `packing_efficiency`: float (0.0-1.0)
     - `wait_time_ms`: float (DataLoader fetch time)
     - `num_sequences`: int
     - `num_tokens`: int
     - `queue_state`: str ('full'|'normal'|'starved')
     - `checkpoint_write_ms`: Optional[float] (time spent writing checkpoint, only populated when checkpoint occurs)
     - `checkpoint_queue_depth`: Optional[int] (async writer queue depth, only populated when checkpoint occurs)
     - Additional keys allowed (extensible)
     - NOTE: checkpoint_write_ms and checkpoint_queue_depth are optional - only populated if checkpoint metrics are available from the caller. When absent, they are simply not included in the per-batch record.

   - `finalize(self, summary: Optional[Dict[str, Any]] = None) -> Path` - write telemetry to disk. Returns JSON path.
     - Calculates elapsed_seconds from start_time
     - If summary not provided, auto-generates from batch_metrics:
       - `total_sequences`: sum of num_sequences across batches
       - `total_tokens`: sum of num_tokens across batches
       - `total_batches`: len(batch_metrics)
       - `elapsed_seconds`: time since start
       - `avg_tokens_per_sec`: total_tokens / elapsed if elapsed > 0
       - `avg_gpu_util`: mean of gpu_util values
       - `avg_packing_efficiency`: mean of packing_efficiency values
       - `pct_starved`: percentage of batches with queue_state=='starved'
       - `sequences_per_hour`: total_sequences / elapsed * 3600
       - `projected_6M_hours`: 6_000_000 / sequences_per_hour if > 0 (sequence-based projection)
       - `avg_tokens_per_seq`: total_tokens / total_sequences if total_sequences > 0
       - `projected_6M_hours_tokens`: (6_000_000 * avg_tokens_per_seq) / total_tokens * elapsed_seconds / 3600 if total_tokens > 0
         (token-based projection - more accurate with varied sequence lengths like viral 400-3000 AA)
       - Include BOTH projected_6M_hours and projected_6M_hours_tokens in summary for comparison
       - `avg_checkpoint_write_ms`: mean of checkpoint_write_ms values (only from batches where checkpoint occurred)
       - `max_checkpoint_queue_depth`: max of checkpoint_queue_depth values (only from batches where checkpoint occurred)
       - NOTE: checkpoint aggregate fields only present if any batch had checkpoint metrics
     - Writes JSON: `{"run_metadata": ..., "summary": ..., "per_batch_metrics": [...]}`
     - Uses `json.dump` with indent=2
     - Logs path to logger

   - `get_summary(self) -> Dict[str, Any]` - calculate summary without writing to disk (for live queries)

2. **generate_performance_report** function:
   - `generate_performance_report(telemetry_path: Path, output_path: Optional[Path] = None) -> str`
   - Loads JSON telemetry file
   - Generates human-readable text report with sections:
     - Workload summary (total sequences, runtime, sequences/hour, projected 6M time - show both seq-based and token-based projections)
     - Throughput (tokens/sec, sequences/sec)
     - Packing Efficiency (average, minimum, batches below 80%)
     - DataLoader I/O (avg wait time, P95 wait time, % starved)
     - GPU Utilization (per-device if available)
     - Checkpoint Overhead (avg write time, max queue depth - only if checkpoint data present)
     - Hardware Info (if present in run_metadata)
     - Requirements Validation:
       - PERF-01 (<10h): projected_6M_hours < 10.0 (use token-based projection if available)
       - PERF-02 (>70% GPU): avg_gpu_util >= 70.0
       - PERF-05 (>90% pack): avg_packing_efficiency >= 0.90
     - Each requirement shows PASS or FAIL
   - If output_path provided, writes report to file
   - Returns report string

3. **aggregate_telemetry_shards** function:
   - `aggregate_telemetry_shards(shard_paths: List[Path]) -> Dict[str, Any]`
   - Takes list of per-shard telemetry JSON paths (one per GPU from multi-GPU runs)
   - Loads each JSON file and merges:
     - `per_batch_metrics`: concatenate all per-batch records from all shards, adding `shard_rank` field to each record
     - `total_sequences`: sum across all shards
     - `total_tokens`: sum across all shards
     - `total_batches`: sum across all shards
     - `elapsed_seconds`: max across shards (wall-clock time = slowest shard)
     - `avg_tokens_per_sec`: global total_tokens / elapsed_seconds
     - `avg_gpu_util`: mean of avg_gpu_util across shards (each shard weighted equally)
     - `avg_packing_efficiency`: mean across shards
     - `pct_starved`: mean across shards
     - `sequences_per_hour`: global total_sequences / elapsed * 3600
     - `projected_6M_hours`: 6_000_000 / sequences_per_hour (global)
     - `projected_6M_hours_tokens`: token-based projection from global totals
     - `num_shards`: len(shard_paths)
     - `per_shard_summary`: list of each shard's individual summary for comparison
   - Returns Dict: `{"global_summary": ..., "per_shard_summary": [...], "per_batch_metrics": [...]}`
   - Validates: all shards have compatible run_metadata (same model_name, warns if mismatch)

4. **calculate_scaling_efficiency** function:
   - `calculate_scaling_efficiency(baseline_runtime: float, multi_gpu_runtime: float, num_gpus: int) -> Dict[str, Any]`
   - `speedup`: baseline_runtime / multi_gpu_runtime
   - `efficiency`: (speedup / num_gpus) * 100.0 (as percentage)
   - `ideal_runtime`: baseline_runtime / num_gpus
   - `overhead_seconds`: multi_gpu_runtime - ideal_runtime
   - `meets_perf03`: efficiency >= 95.0 (True/False - PERF-03 requires 2 GPUs = 1.9x+ = 95% efficiency)
   - Returns Dict with all above fields
   - Validates inputs: baseline_runtime > 0, multi_gpu_runtime > 0, num_gpus >= 1

5. **capture_hardware_info** function:
   - `capture_hardware_info() -> Dict[str, Any]`
   - Captures GPU info (guarded by torch.cuda.is_available()):
     - `gpu_count`: torch.cuda.device_count()
     - `gpu_models`: list of torch.cuda.get_device_name(i) for each device
     - `gpu_memory_gb`: list of torch.cuda.get_device_properties(i).total_mem / (1024**3) for each device
   - Captures CUDA version: `torch.version.cuda` (None if CPU-only)
   - Captures PyTorch version: `torch.__version__`
   - Captures driver version: try subprocess.run(['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'], capture_output=True) - set to None if nvidia-smi not available
   - Captures CPU info: `platform.processor()` and `os.cpu_count()`
   - Captures Python version: `platform.python_version()`
   - Returns Dict with all fields. Gracefully handles missing CUDA (returns gpu_count=0, gpu_models=[], etc.)

Module-level logger: `logger = logging.getLogger('virnucpro.utils.telemetry')`
Module docstring explaining this captures per-batch telemetry for production runs and performance validation, including multi-GPU aggregation and hardware metadata.

Imports: time, json, logging, statistics, platform, os, subprocess from stdlib; Path, Optional, Dict, List, Any from typing.
  </action>
  <verify>
    python -c "from virnucpro.utils.telemetry import TelemetryLogger, generate_performance_report, aggregate_telemetry_shards, calculate_scaling_efficiency, capture_hardware_info; print('Telemetry imports OK')"
  </verify>
  <done>
    TelemetryLogger records per-batch metrics with timestamp (including optional checkpoint_write_ms and checkpoint_queue_depth), auto-generates summary statistics with both sequence-based and token-based 6M projections, writes JSON to disk on finalize(). generate_performance_report produces human-readable report with PERF-01/02/05 requirement validation. aggregate_telemetry_shards merges per-GPU telemetry into global metrics. calculate_scaling_efficiency computes speedup and efficiency for PERF-03 validation. capture_hardware_info captures GPU/CUDA/CPU metadata for reproducible benchmarks.
  </done>
</task>

<task type="auto">
  <name>Task 2: InferenceProgressReporter with rich.progress live metrics</name>
  <files>virnucpro/pipeline/progress_reporter.py</files>
  <action>
Create `virnucpro/pipeline/progress_reporter.py` with:

1. **InferenceProgressReporter** class:
   - Context manager (`__enter__`/`__exit__`) for clean lifecycle

   - `__init__(self, total_sequences: int, monitor: Optional[NvitopMonitor] = None, telemetry: Optional[TelemetryLogger] = None)`:
     - Stores total_sequences, monitor, telemetry references
     - Creates rich.progress.Progress with columns:
       - TextColumn("[bold blue]{task.description}")
       - BarColumn()
       - TaskProgressColumn()
       - TextColumn("[yellow]{task.fields[tokens_per_sec]:>10,.0f} tok/s")
       - TextColumn("[green]{task.fields[gpu_util]:>5.1f}% GPU")
       - TextColumn("[cyan]{task.fields[packing_eff]:>5.1f}% pack")
       - TimeRemainingColumn()
     - self.task_id = None (set in __enter__)

   - `__enter__(self)`:
     - Start progress
     - Add task with initial zero values for tokens_per_sec, gpu_util, packing_eff
     - Return self

   - `__exit__(self, *args)`:
     - Stop progress

   - `update(self, sequences_processed: int, batch_metrics: Optional[Dict] = None)`:
     - If monitor provided, pull live metrics from monitor.get_throughput() and monitor.get_dataloader_statistics() and monitor.get_statistics()
     - If batch_metrics provided, use those values directly (takes precedence)
     - Extract:
       - tokens_per_sec from throughput dict or batch_metrics
       - gpu_util: average across all devices from gpu_stats, or from batch_metrics
       - packing_eff: from dl_stats avg_packing_efficiency * 100, or from batch_metrics
     - Update progress bar: self.progress.update(self.task_id, completed=sequences_processed, tokens_per_sec=..., gpu_util=..., packing_eff=...)
     - If telemetry provided: call telemetry.record_batch() with current metrics

2. **create_progress_callback** factory function:
   - `create_progress_callback(reporter: InferenceProgressReporter) -> Callable[[int, int], None]`
   - Returns a closure that tracks cumulative sequences and calls reporter.update()
   - Compatible with AsyncInferenceRunner.run(progress_callback=...) signature: callback(batch_idx, num_sequences)

Module-level logger: `logger = logging.getLogger('virnucpro.pipeline.progress_reporter')`
Module docstring explaining this provides real-time progress feedback during inference runs.

Imports:
- `from rich.progress import Progress, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn`
- `from virnucpro.utils.gpu_monitor import NvitopMonitor` (TYPE_CHECKING import for type hints)
- `from virnucpro.utils.telemetry import TelemetryLogger` (TYPE_CHECKING import)
- Use `from __future__ import annotations` and TYPE_CHECKING pattern to avoid circular imports
  </action>
  <verify>
    python -c "from virnucpro.pipeline.progress_reporter import InferenceProgressReporter, create_progress_callback; print('Progress reporter imports OK')"
  </verify>
  <done>
    InferenceProgressReporter displays real-time progress bar with tokens/sec, GPU util, and packing efficiency columns via rich.progress. Integrates with NvitopMonitor for live GPU metrics and TelemetryLogger for concurrent metric capture. create_progress_callback provides compatible adapter for AsyncInferenceRunner.run() progress_callback parameter.
  </done>
</task>

</tasks>

<verification>
- `python -c "from virnucpro.utils.telemetry import TelemetryLogger, generate_performance_report, aggregate_telemetry_shards, calculate_scaling_efficiency, capture_hardware_info"`
- `python -c "from virnucpro.pipeline.progress_reporter import InferenceProgressReporter, create_progress_callback"`
- TelemetryLogger: instantiate, call record_batch with sample metrics (including checkpoint_write_ms and checkpoint_queue_depth), call finalize(), verify JSON written with summary containing both projected_6M_hours and projected_6M_hours_tokens
- generate_performance_report: verify it loads JSON telemetry and produces human-readable report with PERF-01/02/05 validation, checkpoint overhead section, and hardware info
- aggregate_telemetry_shards: create 2 shard JSONs, merge, verify global totals are correct sums and elapsed is max across shards
- calculate_scaling_efficiency: verify speedup=baseline/multi_gpu, efficiency=(speedup/num_gpus)*100, meets_perf03 flag
- capture_hardware_info: verify returns dict with gpu_count, gpu_models, cuda_version, pytorch_version, cpu info (graceful on CPU-only)
- InferenceProgressReporter: verify context manager lifecycle, verify update() pulls metrics from monitor or batch_metrics
- create_progress_callback: verify returns callable with (batch_idx, num_sequences) signature
</verification>

<success_criteria>
TelemetryLogger captures per-batch metrics (including optional checkpoint overhead) and writes JSON with auto-generated summary containing both sequence-based and token-based 6M projections on finalize(). aggregate_telemetry_shards merges multi-GPU shard telemetry into global metrics (total sequences/tokens summed, elapsed = max across shards). calculate_scaling_efficiency computes speedup and efficiency with PERF-03 validation (95% efficiency for 2 GPUs). capture_hardware_info captures GPU/CUDA/CPU metadata for reproducible benchmarks. generate_performance_report produces text report with requirements validation (PERF-01 <10h, PERF-02 >70% GPU, PERF-05 >90% pack). InferenceProgressReporter shows live rich.progress bar with throughput, GPU util, and packing efficiency. create_progress_callback adapts reporter for AsyncInferenceRunner.run() interface.
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-01-SUMMARY.md`
</output>
