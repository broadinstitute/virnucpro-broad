---
phase: 10-performance-validation-tuning
plan: 03
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - tests/benchmarks/test_scaling_v2.py
autonomous: true

must_haves:
  truths:
    - "2-GPU scaling test validates 1.9x+ speedup (95% efficiency) over 1 GPU"
    - "Scaling test uses v2.0 pipeline (run_multi_gpu_inference) not v1.0 CLI"
    - "Per-GPU work balance validated by token count (within 10% token imbalance)"
    - "Results printed in comprehensive table format for manual review"
    - "GPU utilization >70% in both 1-GPU and 2-GPU runs proves compute-bound scaling"
    - "Cross-device synchronization verified (both GPUs process >0 sequences)"
    - "Per-GPU timing reported to diagnose straggler GPUs"
  artifacts:
    - path: "tests/benchmarks/test_scaling_v2.py"
      provides: "v2.0 multi-GPU scaling validation with 2-GPU 1.9x requirement"
      min_lines: 150
  key_links:
    - from: "tests/benchmarks/test_scaling_v2.py"
      to: "virnucpro.pipeline.multi_gpu_inference"
      via: "run_multi_gpu_inference for controlled 1-GPU vs 2-GPU comparison"
      pattern: "run_multi_gpu_inference"
---

<objective>
Create multi-GPU scaling validation test that verifies PERF-03: 2 GPUs = 1.9x+ speedup (95% efficiency).

Purpose: Validates that stride-based index distribution provides near-linear scaling. Tests both speedup and work balance across GPUs. Verifies GPU utilization proves compute-bound (not I/O-bound) scaling. Reports per-GPU timing to diagnose stragglers. This is a focused test for the v2.0 architecture (not the v1.0 CLI-based scaling test in test_scaling.py).

Output: `tests/benchmarks/test_scaling_v2.py`
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-CONTEXT.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-01-SUMMARY.md
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/runtime_config.py
@virnucpro/utils/telemetry.py
@tests/benchmarks/test_scaling.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: v2.0 multi-GPU scaling validation (2-GPU 1.9x requirement)</name>
  <files>tests/benchmarks/test_scaling_v2.py</files>
  <action>
Create `tests/benchmarks/test_scaling_v2.py` with:

Module docstring: "v2.0 multi-GPU scaling validation. Tests PERF-03: 2 GPUs = 1.9x+ speedup (95% efficiency) using the async DataLoader + packing + FP16 pipeline."

Pytest markers: `pytestmark = [pytest.mark.slow, pytest.mark.gpu]`

1. **Helper: generate_scaling_workload**
   - `generate_scaling_workload(output_dir: Path, num_sequences: int = 5000, seed: int = 42) -> Path`
   - Generates FASTA with varied lengths (same distribution as test_production_validation)
   - Returns path to FASTA file
   - 5000 sequences is enough to show scaling effects without excessive test runtime

2. **TestV2Scaling** class:

   a. **test_2gpu_scaling_efficiency** (main scaling test):
   - Skip if torch.cuda.device_count() < 2
   - Generate 5K sequence workload
   - model_config: model_type='esm2', model_name='esm2_t36_3B_UR50D', enable_fp16=True
   - runtime_config: RuntimeConfig(enable_checkpointing=False) (no checkpoint overhead during scaling test)

   - Run 1 GPU:
     - output_dir_1gpu = tmp_path / "1gpu"
     - torch.cuda.synchronize() before timing
     - start_1 = time.perf_counter()
     - run_multi_gpu_inference(fasta_files, output_dir_1gpu, model_config, world_size=1, runtime_config=runtime_config)
     - torch.cuda.synchronize() after
     - elapsed_1gpu = time.perf_counter() - start_1

   - **GPU utilization check (1 GPU):**
     - Read telemetry from 1-GPU output directory
     - Extract avg_gpu_util from telemetry data
     - Assert avg_gpu_util >= 70.0, message: "I/O bottleneck detected: GPU utilization {avg_gpu_util:.1f}% < 70% indicates DataLoader starvation (1-GPU run)"

   - Run 2 GPUs:
     - output_dir_2gpu = tmp_path / "2gpu"
     - torch.cuda.synchronize() before timing
     - start_2 = time.perf_counter()
     - run_multi_gpu_inference(fasta_files, output_dir_2gpu, model_config, world_size=2, runtime_config=runtime_config)
     - torch.cuda.synchronize() after
     - elapsed_2gpu = time.perf_counter() - start_2

   - **GPU utilization check (2 GPUs):**
     - Read telemetry from both shards in 2-GPU output directory
     - Extract per-GPU avg_gpu_util for GPU 0 and GPU 1
     - Assert BOTH >= 70.0, message: "I/O bottleneck detected: GPU {rank} utilization {util:.1f}% < 70% indicates DataLoader starvation (2-GPU run)"

   - **Cross-device synchronization verification:**
     - Verify both shard HDF5 files exist in output_dir_2gpu (shard_0.h5 and shard_1.h5)
     - Open each with h5py and count sequences
     - Assert shard_0_count > 0, message: "Cross-device synchronization failure: GPU 0 processed no data"
     - Assert shard_1_count > 0, message: "Cross-device synchronization failure: GPU 1 processed no data"

   - **Per-GPU timing report:**
     - Extract individual GPU elapsed times from checkpoint metadata or HDF5 shard timestamps
     - Calculate imbalance_pct = abs(time_gpu0 - time_gpu1) / max(time_gpu0, time_gpu1) * 100

   - Calculate:
     - speedup = elapsed_1gpu / elapsed_2gpu
     - efficiency = (speedup / 2) * 100
     - seq_per_sec_1gpu = 5000 / elapsed_1gpu
     - seq_per_sec_2gpu = 5000 / elapsed_2gpu

   - Print comprehensive table:
     ```
     v2.0 Multi-GPU Scaling Results
     ==============================
     Configuration   | Time (s)  | Seq/sec   | Speedup | Efficiency
     1 GPU           | XXX.XX    | XXX.X     | 1.00x   | 100.0%
     2 GPUs          | XXX.XX    | XXX.X     | X.XXx   | XX.X%
     ==============================
     GPU Utilization:
       1-GPU run:  XX.X%
       2-GPU GPU 0: XX.X%
       2-GPU GPU 1: XX.X%
     Per-GPU Timing:
       GPU 0: XX.Xs (YY.Y%)
       GPU 1: XX.Xs (YY.Y%)
       Imbalance: Z.Z%
     Cross-device: GPU 0: N sequences, GPU 1: M sequences
     ==============================
     PERF-03 Target: 2 GPUs >= 1.9x speedup (95% efficiency)
     Result: PASS/FAIL
     ```

   - Assertions:
     - speedup >= 1.9 (PERF-03: 95% efficiency for 2 GPUs)
     - failed_ranks_2gpu is empty (both GPUs complete successfully)

   b. **test_work_balance_2gpu** (balance validation):
   - Skip if torch.cuda.device_count() < 2
   - Generate 5K sequence workload
   - Run with 2 GPUs (checkpointing enabled to get per-shard timing)
   - After completion, check per-shard HDF5 files:
     - Count TOKENS per shard (not sequences)
     - Open shard_0.h5 and shard_1.h5 with h5py
     - Extract embedding shape [num_tokens, hidden_dim] from each shard, sum num_tokens
     - Also count sequences per shard for reporting
     - Calculate token_imbalance_pct = abs(tokens_0 - tokens_1) / max(tokens_0, tokens_1) * 100
   - Assert token_imbalance_pct < 10 (stride distribution should balance token count within 10%)
   - Print: "Work balance: GPU 0: X tokens (Y sequences), GPU 1: Z tokens (W sequences), token imbalance: A.A%"

Imports:
- pytest, torch, time, random, logging from stdlib
- Path from pathlib
- run_multi_gpu_inference from virnucpro.pipeline.multi_gpu_inference
- RuntimeConfig from virnucpro.pipeline.runtime_config
- h5py (import at use site for sequence/token counting)
  </action>
  <verify>
    python -c "from tests.benchmarks.test_scaling_v2 import TestV2Scaling; print('Scaling v2 imports OK')"
  </verify>
  <done>
    2-GPU scaling test runs same workload on 1 GPU and 2 GPUs, calculates speedup, validates >= 1.9x (95% efficiency). GPU utilization >= 70% asserted for both 1-GPU and 2-GPU runs to prove compute-bound scaling. Cross-device verification confirms both GPUs process >0 sequences. Per-GPU timing reported with imbalance percentage. Work balance test validates stride distribution keeps GPUs within 10% TOKEN imbalance (not sequence count). Both print comprehensive results tables.
  </done>
</task>

</tasks>

<verification>
- `python -c "from tests.benchmarks.test_scaling_v2 import TestV2Scaling"`
- 2-GPU scaling: 1-GPU vs 2-GPU timing comparison, speedup >= 1.9x
- GPU utilization: >= 70% in both 1-GPU and 2-GPU runs (proves compute-bound)
- Cross-device: both shard HDF5 files have >0 sequences
- Per-GPU timing: reported with imbalance percentage
- Work balance: shard TOKEN counts within 10% of each other
- Comprehensive results table printed for manual review
</verification>

<success_criteria>
Scaling test validates PERF-03 (2 GPUs = 1.9x+ speedup) with GPU utilization >= 70% proving compute-bound scaling. Cross-device verification confirms both GPUs did work. Per-GPU timing reported for straggler diagnosis. Work balance test validates stride distribution by token count (within 10% token imbalance). Both use v2.0 pipeline API (run_multi_gpu_inference) and print comprehensive results for manual review.
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-03-SUMMARY.md`
</output>
