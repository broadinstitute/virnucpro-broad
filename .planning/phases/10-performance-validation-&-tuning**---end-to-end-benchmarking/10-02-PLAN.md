---
phase: 10-performance-validation-tuning
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - tests/benchmarks/test_production_validation.py
autonomous: true

must_haves:
  truths:
    - "Production benchmark runs full pipeline with real FASTA data and captures all PERF metrics"
    - "Benchmark validates <10h projected time for 6M sequences (PERF-01)"
    - "Benchmark validates >70% average GPU utilization (PERF-02)"
    - "Benchmark validates >90% packing efficiency (PERF-05)"
    - "Telemetry JSON persisted to disk with per-batch and summary metrics"
    - "Human-readable performance report generated after run"
  artifacts:
    - path: "tests/benchmarks/test_production_validation.py"
      provides: "Production workload benchmark with telemetry capture and requirements validation"
      min_lines: 200
  key_links:
    - from: "tests/benchmarks/test_production_validation.py"
      to: "virnucpro.pipeline.multi_gpu_inference"
      via: "run_multi_gpu_inference for full pipeline execution"
      pattern: "from virnucpro\\.pipeline\\.multi_gpu_inference import"
    - from: "tests/benchmarks/test_production_validation.py"
      to: "virnucpro.utils.telemetry"
      via: "TelemetryLogger for per-batch metrics capture"
      pattern: "from virnucpro\\.utils\\.telemetry import"
    - from: "tests/benchmarks/test_production_validation.py"
      to: "virnucpro.pipeline.progress_reporter"
      via: "InferenceProgressReporter for live progress display"
      pattern: "from virnucpro\\.pipeline\\.progress_reporter import"
---

<objective>
Create production workload benchmark that validates PERF-01 (<10h), PERF-02 (>70% GPU util), and PERF-05 (>90% packing, telemetry) requirements.

Purpose: This is the primary validation test. It runs the full v2.0 pipeline (multi-GPU, packing, FP16, checkpointing) on real or large synthetic workloads to project whether 6M sequences complete in <10 hours. Captures all telemetry metrics for analysis and generates a performance summary report.

Output: `tests/benchmarks/test_production_validation.py`
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-CONTEXT.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-RESEARCH.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-01-SUMMARY.md
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/runtime_config.py
@virnucpro/utils/telemetry.py
@virnucpro/pipeline/progress_reporter.py
@tests/benchmarks/test_fp16_throughput.py
@tests/benchmarks/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Production workload benchmark with full telemetry</name>
  <files>tests/benchmarks/test_production_validation.py</files>
  <action>
Create `tests/benchmarks/test_production_validation.py` with:

Module docstring explaining this validates the v2.0 pipeline meets <10h target on production-scale workloads with 2 GPUs.

Pytest markers: `pytestmark = [pytest.mark.slow, pytest.mark.gpu]`

1. **Helper function: generate_production_subset**
   - `generate_production_subset(output_dir: Path, num_sequences: int = 10000, seed: int = 42) -> Path`
   - Generates a FASTA file with protein sequences whose length distribution mimics real viral nucleotide data:
     - 40% short (50-100 aa)
     - 35% medium (100-300 aa)
     - 20% long (300-500 aa)
     - 5% very long (500-1000 aa)
   - Uses random.Random(seed) for reproducibility
   - Returns path to generated FASTA file
   - Uses amino acid alphabet "ACDEFGHIKLMNPQRSTVWY"

2. **TestProductionValidation** class:

   a. **test_production_throughput_validation** (main test):
   - Skip if torch.cuda.device_count() < 1
   - Generate test workload: 10K sequences (representative subset for timing)
   - Setup output_dir, telemetry_dir
   - Create TelemetryLogger with telemetry_dir as output_dir
   - Set run_metadata: model_name, world_size, num_sequences, hardware info (torch.cuda.get_device_name)
   - Create RuntimeConfig with enable_checkpointing=True
   - Create model_config dict: model_type='esm2', model_name='esm2_t36_3B_UR50D', enable_fp16=True
   - Determine world_size: min(torch.cuda.device_count(), 2) (per CONTEXT.md: 2 GPUs typical)
   - Create InferenceProgressReporter(total_sequences=num_sequences, telemetry=telemetry) as context manager wrapping the pipeline run (validates 10-01 progress_reporter integration)
   - Time the full pipeline: start_time, run_multi_gpu_inference, end_time
   - Calculate:
     - elapsed_seconds
     - sequences_per_hour = num_sequences / elapsed_seconds * 3600
     - projected_6M_hours = 6_000_000 / sequences_per_hour
   - Generate summary dict with all metrics
   - Call telemetry.finalize(summary)
   - Generate performance report via generate_performance_report()
   - Print comprehensive results table to stdout (logger.info)
   - After run_multi_gpu_inference completes, read telemetry JSON file(s) from telemetry_dir:
     - Glob for `telemetry_*.json` files in telemetry_dir
     - Load JSON, extract `summary.avg_gpu_util` and `summary.avg_packing_efficiency`
     - If multiple telemetry files (one per GPU worker), average the values across workers
   - Assertions (HARD requirements from CONTEXT.md):
     - projected_6M_hours < 10.0 (PERF-01) - print actual value in failure message
     - avg_gpu_util >= 70.0 (PERF-02) - extracted from telemetry JSON summary, print actual value in failure message
     - avg_packing_efficiency >= 0.90 (PERF-05) - extracted from telemetry JSON summary, print actual value in failure message

   b. **test_throughput_subset_profiling** (quick profiling test):
   - Generate smaller workload: 1K sequences
   - Single GPU only (world_size=1)
   - Measures throughput without multi-GPU overhead
   - No hard assertions - prints results table for manual review:
     - Sequences/sec
     - Tokens/sec (if available from telemetry)
     - Peak memory (from torch.cuda.max_memory_allocated)
     - Elapsed time
     - Projected 6M time
   - Purpose: Quick check that can run in CI without 2 GPUs

   c. **test_checkpoint_overhead** (checkpoint impact measurement):
   - Generate 5K sequence workload
   - Run TWICE on single GPU:
     - Once with enable_checkpointing=True, checkpoint_seq_threshold=1000 (frequent)
     - Once with enable_checkpointing=False
   - Compare elapsed times
   - Calculate checkpoint_overhead_pct = (with_ckpt - without_ckpt) / without_ckpt * 100
   - Assert checkpoint_overhead_pct < 15.0 (checkpointing should add <15% overhead)
   - Print: "Checkpoint overhead: X.X% (with: Ys, without: Zs)"

All tests use `tmp_path` fixture for output isolation.

Imports:
- pytest, torch, time, random, logging, json, glob from stdlib
- Path from pathlib
- run_multi_gpu_inference from virnucpro.pipeline.multi_gpu_inference
- RuntimeConfig from virnucpro.pipeline.runtime_config
- TelemetryLogger, generate_performance_report from virnucpro.utils.telemetry
- InferenceProgressReporter from virnucpro.pipeline.progress_reporter
  </action>
  <verify>
    python -c "from tests.benchmarks.test_production_validation import TestProductionValidation; print('Production validation imports OK')"
  </verify>
  <done>
    Production benchmark generates representative workloads, runs full multi-GPU pipeline with InferenceProgressReporter, captures telemetry, reads telemetry JSON to validate PERF-01 (<10h projected), PERF-02 (>=70% GPU util), and PERF-05 (>=90% packing efficiency), and generates performance report. Quick profiling test runs on single GPU for CI. Checkpoint overhead test validates <15% impact.
  </done>
</task>

</tasks>

<verification>
- `python -c "from tests.benchmarks.test_production_validation import TestProductionValidation"`
- Production test generates FASTA, runs run_multi_gpu_inference with InferenceProgressReporter, measures elapsed time, projects 6M hours
- Telemetry JSON written to disk with summary metrics
- Test reads telemetry JSON and asserts avg_gpu_util >= 70.0 and avg_packing_efficiency >= 0.90
- Performance report generated with PERF-01/02/05 validation
- Checkpoint overhead test compares with/without checkpointing
</verification>

<success_criteria>
Production benchmark runs full pipeline on 10K representative sequences with InferenceProgressReporter for live progress, measures throughput, projects 6M sequence runtime, writes telemetry JSON, reads telemetry JSON to assert projected time <10 hours (PERF-01), avg GPU utilization >=70% (PERF-02), and avg packing efficiency >=90% (PERF-05). Quick subset test runs on single GPU for CI validation. Checkpoint overhead test validates <15% impact.
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-02-SUMMARY.md`
</output>
