---
phase: 10-performance-validation-tuning
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - tests/benchmarks/test_production_validation.py
autonomous: true

must_haves:
  truths:
    - "Production benchmark runs full pipeline with real FASTA data and captures all PERF metrics"
    - "Benchmark validates <10h projected time for 6M sequences (PERF-01)"
    - "Benchmark validates >70% GPU utilization per-GPU (PERF-02)"
    - "Benchmark validates >90% packing efficiency per-GPU (PERF-05)"
    - "Benchmark validates 2-GPU scaling >=1.9x speedup via 1-GPU vs 2-GPU comparison (PERF-03)"
    - "Benchmark uses real production FASTA if available via VIRNUCPRO_PRODUCTION_FASTA env var"
    - "Benchmark excludes warmup period from projections for accuracy"
    - "Benchmark validates DataLoader I/O wait time <100ms (p95)"
    - "Telemetry JSON persisted to disk with per-batch and summary metrics"
    - "Human-readable performance report generated after run"
  artifacts:
    - path: "tests/benchmarks/test_production_validation.py"
      provides: "Production workload benchmark with telemetry capture, scaling validation, warmup exclusion, and per-GPU requirements validation"
      min_lines: 300
  key_links:
    - from: "tests/benchmarks/test_production_validation.py"
      to: "virnucpro.pipeline.multi_gpu_inference"
      via: "run_multi_gpu_inference for full pipeline execution"
      pattern: "from virnucpro\\.pipeline\\.multi_gpu_inference import"
    - from: "tests/benchmarks/test_production_validation.py"
      to: "virnucpro.utils.telemetry"
      via: "TelemetryLogger and calculate_scaling_efficiency for metrics and scaling validation"
      pattern: "from virnucpro\\.utils\\.telemetry import"
    - from: "tests/benchmarks/test_production_validation.py"
      to: "virnucpro.pipeline.progress_reporter"
      via: "InferenceProgressReporter for live progress display"
      pattern: "from virnucpro\\.pipeline\\.progress_reporter import"
---

<objective>
Create production workload benchmark that validates PERF-01 (<10h), PERF-02 (>70% GPU util per-GPU), PERF-03 (>=1.9x scaling), PERF-05 (>90% packing per-GPU), and DataLoader I/O (<100ms p95 wait) requirements.

Purpose: This is the primary validation test. It runs the full v2.0 pipeline (multi-GPU, packing, FP16, checkpointing) on real production FASTA (or synthetic fallback) to project whether 6M sequences complete in <10 hours. Runs both 1-GPU baseline and 2-GPU to validate scaling efficiency. Excludes warmup period from projections. Asserts per-GPU metrics (not just averages) to catch load imbalance. Validates DataLoader keeps up via I/O wait time assertions.

Output: `tests/benchmarks/test_production_validation.py`
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-CONTEXT.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-RESEARCH.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-01-SUMMARY.md
@virnucpro/pipeline/multi_gpu_inference.py
@virnucpro/pipeline/runtime_config.py
@virnucpro/utils/telemetry.py
@virnucpro/pipeline/progress_reporter.py
@tests/benchmarks/test_fp16_throughput.py
@tests/benchmarks/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Production workload benchmark with full telemetry, scaling validation, and per-GPU assertions</name>
  <files>tests/benchmarks/test_production_validation.py</files>
  <action>
Create `tests/benchmarks/test_production_validation.py` with:

Module docstring explaining this validates the v2.0 pipeline meets <10h target on production-scale workloads with 2 GPUs, validates scaling efficiency via 1-GPU vs 2-GPU comparison, excludes warmup from projections, and asserts per-GPU metrics.

Pytest markers: `pytestmark = [pytest.mark.slow, pytest.mark.gpu]`

1. **Helper function: generate_production_subset**
   - `generate_production_subset(output_dir: Path, num_sequences: int = 10000, seed: int = 42) -> Path`
   - Generates a FASTA file with protein sequences whose length distribution mimics real viral nucleotide data:
     - 40% short (50-100 aa)
     - 35% medium (100-300 aa)
     - 20% long (300-500 aa)
     - 5% very long (500-1000 aa)
   - Uses random.Random(seed) for reproducibility
   - Returns path to generated FASTA file
   - Uses amino acid alphabet "ACDEFGHIKLMNPQRSTVWY"

2. **Helper function: get_fasta_path**
   - `get_fasta_path(output_dir: Path, num_sequences: int = 10000) -> Tuple[Path, bool]`
   - Checks env var `VIRNUCPRO_PRODUCTION_FASTA`:
     - If set AND file exists: return (Path(env_value), True) and log "Using production data: {path}"
     - If set but file does NOT exist: log WARNING with path, fall through to synthetic
     - If not set: fall through to synthetic
   - Synthetic fallback: call generate_production_subset(output_dir, num_sequences), log WARNING: "Using synthetic data. Set VIRNUCPRO_PRODUCTION_FASTA for real production validation."
   - Returns (fasta_path, is_production) tuple so test can report which data source was used

3. **Helper function: exclude_warmup**
   - `exclude_warmup(batch_metrics: List[Dict], warmup_seconds: float = 60.0, warmup_batches: int = 20) -> Tuple[List[Dict], List[Dict]]`
   - Takes list of per-batch metric dicts (each has a `timestamp` field)
   - Identifies warmup period: the LARGER of (first `warmup_seconds` seconds from first batch timestamp) OR (first `warmup_batches` batches)
   - Returns (warmup_metrics, steady_state_metrics) tuple
   - If all batches fall within warmup (very short run), return ([], batch_metrics) with a logged warning - do not discard everything

4. **Helper function: recalculate_summary_from_steady_state**
   - `recalculate_summary_from_steady_state(steady_state_metrics: List[Dict]) -> Dict[str, Any]`
   - Recalculates summary statistics from only steady-state batches:
     - `total_sequences`: sum of num_sequences
     - `total_tokens`: sum of num_tokens
     - `total_batches`: len(steady_state_metrics)
     - `steady_state_seconds`: last timestamp - first timestamp (of steady-state batch range)
     - `avg_tokens_per_sec`: total_tokens / steady_state_seconds
     - `avg_gpu_util`: mean of gpu_util values
     - `avg_packing_efficiency`: mean of packing_efficiency values
     - `sequences_per_hour`: total_sequences / steady_state_seconds * 3600
     - `projected_6M_hours`: 6_000_000 / sequences_per_hour
     - `p95_wait_time_ms`: 95th percentile of wait_time_ms values (use sorted list, index at int(0.95 * len))
   - Returns dict with all fields

5. **TestProductionValidation** class:

   a. **test_production_throughput_validation** (main test):
   - Skip if torch.cuda.device_count() < 2 (this test REQUIRES 2 GPUs for scaling validation)
   - Get FASTA path via get_fasta_path(output_dir) - reports whether production or synthetic data
   - Setup output_dir, telemetry_dir_1gpu, telemetry_dir_2gpu (separate telemetry dirs for each run)
   - Create model_config dict: model_type='esm2', model_name='esm2_t36_3B_UR50D', enable_fp16=True
   - Create RuntimeConfig with enable_checkpointing=True

   **Phase A: 1-GPU baseline run**
   - Create TelemetryLogger with telemetry_dir_1gpu as output_dir
   - Set run_metadata: model_name, world_size=1, num_sequences, hardware info, data_source=("production" or "synthetic")
   - Time the pipeline: start_1gpu, run_multi_gpu_inference(world_size=1, ...), end_1gpu
   - elapsed_1gpu = end_1gpu - start_1gpu
   - Call telemetry_1gpu.finalize()

   **Phase B: 2-GPU run**
   - Create TelemetryLogger with telemetry_dir_2gpu as output_dir
   - Set run_metadata: model_name, world_size=2, num_sequences, hardware info, data_source
   - Create InferenceProgressReporter(total_sequences=num_sequences, telemetry=telemetry_2gpu) as context manager
   - Time the pipeline: start_2gpu, run_multi_gpu_inference(world_size=2, ...), end_2gpu
   - elapsed_2gpu = end_2gpu - start_2gpu
   - Call telemetry_2gpu.finalize()

   **Phase C: Scaling efficiency validation**
   - Import calculate_scaling_efficiency from virnucpro.utils.telemetry
   - scaling_result = calculate_scaling_efficiency(elapsed_1gpu, elapsed_2gpu, num_gpus=2)
   - Log: "Scaling: 1-GPU={elapsed_1gpu:.1f}s, 2-GPU={elapsed_2gpu:.1f}s, speedup={scaling_result['speedup']:.2f}x, efficiency={scaling_result['efficiency']:.1f}%"
   - Assert scaling_result['speedup'] >= 1.9, f"PERF-03 FAIL: 2-GPU speedup {scaling_result['speedup']:.2f}x < 1.9x required"
   - Assert scaling_result['efficiency'] >= 95.0, f"PERF-03 FAIL: scaling efficiency {scaling_result['efficiency']:.1f}% < 95% required"

   **Phase D: Read telemetry, exclude warmup, recalculate projections**
   - Glob for `telemetry_*.json` files in telemetry_dir_2gpu (one per GPU worker shard)
   - For EACH telemetry JSON file (per-GPU shard):
     - Load JSON, extract per_batch_metrics
     - Call exclude_warmup(per_batch_metrics) to split warmup vs steady-state
     - Call recalculate_summary_from_steady_state(steady_state_metrics)
     - Store per-shard steady-state summary and original summary for comparison
   - Log warmup exclusion stats: "Excluded {N} warmup batches ({M}s), using {K} steady-state batches for projections"
   - Calculate global steady-state projected_6M_hours from aggregated steady-state metrics (sum sequences across shards / sum steady_state_seconds across shards * 3600, then 6M / that rate)

   **Phase E: Per-GPU assertions (NOT averages)**
   - For each shard telemetry JSON:
     - Extract shard's steady-state avg_gpu_util
     - Assert >= 70.0, f"PERF-02 FAIL: GPU {shard_rank} utilization {util:.1f}% < 70% required"
     - Extract shard's steady-state avg_packing_efficiency
     - Assert >= 0.90, f"PERF-05 FAIL: GPU {shard_rank} packing efficiency {eff:.3f} < 0.90 required"
   - Assert global steady-state projected_6M_hours < 10.0, f"PERF-01 FAIL: projected {hours:.2f}h >= 10h target"

   **Phase F: DataLoader I/O wait time assertion**
   - From steady-state metrics across all shards, collect all wait_time_ms values
   - Calculate p95_wait_time_ms (95th percentile)
   - Assert p95_wait_time_ms < 100.0, f"DataLoader I/O FAIL: p95 wait time {p95:.1f}ms >= 100ms. Consider increasing num_workers or prefetch_factor."

   **Phase G: Report generation**
   - Generate summary dict with ALL metrics (both warmup and steady-state, scaling results)
   - Include in summary: warmup_batches_excluded, steady_state_batches, data_source, scaling_speedup, scaling_efficiency, p95_wait_time_ms
   - Write final telemetry with summary via telemetry_2gpu (or aggregate)
   - Generate performance report via generate_performance_report()
   - Log comprehensive results table including scaling comparison

   b. **test_throughput_subset_profiling** (quick profiling test - UNCHANGED):
   - Generate smaller workload: 1K sequences
   - Single GPU only (world_size=1)
   - Measures throughput without multi-GPU overhead
   - No hard assertions - prints results table for manual review:
     - Sequences/sec
     - Tokens/sec (if available from telemetry)
     - Peak memory (from torch.cuda.max_memory_allocated)
     - Elapsed time
     - Projected 6M time
   - Purpose: Quick check that can run in CI without 2 GPUs

   c. **test_checkpoint_overhead** (checkpoint impact measurement - UNCHANGED):
   - Generate 5K sequence workload
   - Run TWICE on single GPU:
     - Once with enable_checkpointing=True, checkpoint_seq_threshold=1000 (frequent)
     - Once with enable_checkpointing=False
   - Compare elapsed times
   - Calculate checkpoint_overhead_pct = (with_ckpt - without_ckpt) / without_ckpt * 100
   - Assert checkpoint_overhead_pct < 15.0 (checkpointing should add <15% overhead)
   - Print: "Checkpoint overhead: X.X% (with: Ys, without: Zs)"

All tests use `tmp_path` fixture for output isolation.

Imports:
- pytest, torch, time, random, logging, json, os from stdlib
- glob from glob module
- Path from pathlib
- Tuple, List, Dict, Any from typing
- statistics from stdlib (for mean)
- run_multi_gpu_inference from virnucpro.pipeline.multi_gpu_inference
- RuntimeConfig from virnucpro.pipeline.runtime_config
- TelemetryLogger, generate_performance_report, calculate_scaling_efficiency from virnucpro.utils.telemetry
- InferenceProgressReporter from virnucpro.pipeline.progress_reporter
  </action>
  <verify>
    python -c "from tests.benchmarks.test_production_validation import TestProductionValidation; print('Production validation imports OK')"
  </verify>
  <done>
    Production benchmark uses real FASTA via VIRNUCPRO_PRODUCTION_FASTA env var (with synthetic fallback and warning). Runs 1-GPU baseline then 2-GPU and validates scaling >=1.9x speedup / 95% efficiency via calculate_scaling_efficiency (PERF-03). Excludes warmup period (60s or 20 batches, whichever is larger) from throughput projections. Asserts per-GPU metrics: each GPU >=70% utilization (PERF-02) and >=90% packing efficiency (PERF-05) - not just averages. Asserts p95 DataLoader wait time <100ms. Projects <10h for 6M sequences from steady-state metrics (PERF-01). Generates performance report. Quick profiling test and checkpoint overhead test unchanged.
  </done>
</task>

</tasks>

<verification>
- `python -c "from tests.benchmarks.test_production_validation import TestProductionValidation"`
- Test checks VIRNUCPRO_PRODUCTION_FASTA env var and uses real data if available, synthetic with warning if not
- Test runs 1-GPU baseline then 2-GPU, calculates scaling via calculate_scaling_efficiency, asserts speedup >= 1.9 and efficiency >= 95%
- Warmup exclusion: first 60s or 20 batches (whichever larger) excluded from projection calculations
- Per-GPU assertions: each shard's telemetry checked individually for GPU util >= 70% and packing efficiency >= 0.90
- p95_wait_time_ms < 100.0 asserted from steady-state DataLoader wait times
- Telemetry JSON written to disk with both warmup and steady-state metrics
- Performance report generated with all PERF requirement validations
- Checkpoint overhead test compares with/without checkpointing
</verification>

<success_criteria>
Production benchmark accepts real FASTA via VIRNUCPRO_PRODUCTION_FASTA (falls back to synthetic with warning). Runs 1-GPU then 2-GPU, validates scaling speedup >=1.9x and efficiency >=95% (PERF-03). Excludes warmup (60s or 20 batches) from projections. Asserts per-GPU (not average): utilization >=70% (PERF-02), packing efficiency >=90% (PERF-05). Asserts p95 DataLoader wait time <100ms. Projects <10h for 6M sequences from steady-state metrics (PERF-01). Generates performance report. Quick subset and checkpoint overhead tests unchanged.
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-02-SUMMARY.md`
</output>
