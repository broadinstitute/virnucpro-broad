---
phase: 10-performance-validation-tuning
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "Full pipeline completes on 1M subset with 1x RTX 4090 in <1 hour"
    - "v2.0 predictions have >=99% consensus label agreement with v1.0"
    - "Per-stage timing breakdown is logged for all 9 stages"
    - "GPU utilization >80% during ESM-2 embedding step"
    - "Packing efficiency >90% during ESM-2 embedding step"
    - "Telemetry metrics (tokens/sec, packing efficiency, I/O wait) appear in log output"
  artifacts: []
  key_links:
    - from: "v2.0 prediction output"
      to: "v1.0 prediction output"
      via: "compare_virnucpro_outputs.py consensus comparison"
      pattern: "consensus label agreement"
---

<objective>
Run the full v2.0 pipeline on the real 1M subset dataset with 1x RTX 4090, validate correctness against v1.0 output, and capture per-stage timing breakdown.

Purpose: This is the primary benchmark run that validates the pipeline meets the <1 hour target on single GPU, produces correct predictions (>=99% consensus label agreement with v1.0), and captures all required telemetry metrics. This run also establishes the single-GPU baseline for multi-GPU scaling measurement in Plan 03.

Output: Complete v2.0 pipeline run with timing data, correctness validation results, and telemetry metrics in the log.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-01-SUMMARY.md
@compare_virnucpro_outputs.py
@virnucpro/cli/predict.py
@virnucpro/pipeline/prediction.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run full pipeline on 1M subset with 1x RTX 4090</name>
  <files></files>
  <action>
Run the complete v2.0 prediction pipeline on the real 1M subset FASTA using a single RTX 4090 GPU. This exercises ALL 9 pipeline stages (chunking, translation, splitting, DNABERT-S, ESM-2, merging, prediction, consensus).

**Input FASTA:**
`/data/Broad_Viral_Respiratory/data/in/raw/full/USA-MA-Broad_MGH-22989-2024.lNDM_F11.HLWLWDRX5.1.hs_depleted.subset_1M.fasta`

**Output directory:**
`/tmp/virnucpro_bench_1gpu/`

**Command:**
```bash
CUDA_VISIBLE_DEVICES=0 python -m virnucpro predict \
  /data/Broad_Viral_Respiratory/data/in/raw/full/USA-MA-Broad_MGH-22989-2024.lNDM_F11.HLWLWDRX5.1.hs_depleted.subset_1M.fasta \
  -o /tmp/virnucpro_bench_1gpu \
  --force \
  --keep-intermediate \
  --gpus 0 \
  2>&1 | tee /tmp/virnucpro_bench_1gpu_log.txt
```

Note: With `--gpus 0` (single GPU), the CLI will set parallel=False (single GPU does not auto-enable parallel). But v2.0 routing requires `--parallel`. So the correct command is:

```bash
CUDA_VISIBLE_DEVICES=0 python -m virnucpro predict \
  /data/Broad_Viral_Respiratory/data/in/raw/full/USA-MA-Broad_MGH-22989-2024.lNDM_F11.HLWLWDRX5.1.hs_depleted.subset_1M.fasta \
  -o /tmp/virnucpro_bench_1gpu \
  --force \
  --keep-intermediate \
  --parallel \
  --gpus 0 \
  2>&1 | tee /tmp/virnucpro_bench_1gpu_log.txt
```

This runs the FULL pipeline: Stages 1-9. ESM-2 (Stage 6) uses v2.0 async architecture because `--parallel` triggers `use_v2=True`. DNABERT-S (Stage 5) stays v1.0 (hybrid architecture).

**Expected behavior:**
- 1M input sequences -> ~600K translated sequences after 6-frame translation
- Per-stage timing logged via PipelineTelemetry (from Plan 01)
- Pipeline summary block printed at completion
- Total time target: <1 hour (3600 seconds)

**Timeout:** Set bash timeout to 5400 seconds (90 minutes) to allow generous margin. If the command exceeds this, it indicates a performance problem.

**After the run completes, extract and record:**
1. Total wall time from the summary block
2. Per-stage timing breakdown from the summary block
3. ESM-2 stage timing specifically (this is the bottleneck)
4. Whether the <1 hour target was met
5. Any errors or warnings in the log

If the pipeline fails for any reason, document the error and check:
- GPU memory: Is ESM-2 3B model fitting in 24GB VRAM?
- CUDA version compatibility
- FlashAttention kernel errors
  </action>
  <verify>
1. Check exit code of the pipeline command (should be 0)
2. Verify output files exist: `/tmp/virnucpro_bench_1gpu/*_merged/prediction_results.txt` and `prediction_results_highestscore.csv`
3. Check total wall time from log: `grep "Total wall time" /tmp/virnucpro_bench_1gpu_log.txt`
4. Check per-stage breakdown from log: `grep -A 15 "PIPELINE SUMMARY" /tmp/virnucpro_bench_1gpu_log.txt`
5. Verify telemetry metrics in log: `grep -i "tokens.*sec\|packing.*efficiency\|queue.*state\|gpu.*util" /tmp/virnucpro_bench_1gpu_log.txt | head -20`
  </verify>
  <done>Full pipeline completes on 1M subset with 1x RTX 4090. Total wall time and per-stage breakdown recorded. Output files (prediction_results.txt, prediction_results_highestscore.csv) exist.</done>
</task>

<task type="auto">
  <name>Task 2: Validate correctness against v1.0 output</name>
  <files></files>
  <action>
Compare v2.0 predictions against existing v1.0 output to validate correctness. Use the existing compare_virnucpro_outputs.py tool which has PredictionComparator class.

**v1.0 output paths:**
- Per-frame: `/data/Broad_Viral_Respiratory/data/in/raw/full/USA-MA-Broad_MGH-22989-2024.lNDM_F11.HLWLWDRX5.1.hs_depleted.subset_1M_merged/prediction_results.txt`
- Consensus: `/data/Broad_Viral_Respiratory/data/in/raw/full/USA-MA-Broad_MGH-22989-2024.lNDM_F11.HLWLWDRX5.1.hs_depleted.subset_1M_merged/prediction_results_highestscore.csv`

**v2.0 output paths (from Task 1):**
- Per-frame: `/tmp/virnucpro_bench_1gpu/*_merged/prediction_results.txt`
- Consensus: `/tmp/virnucpro_bench_1gpu/*_merged/prediction_results_highestscore.csv`

**Run consensus comparison (this is the primary correctness metric):**
```bash
python compare_virnucpro_outputs.py \
  --vanilla /data/Broad_Viral_Respiratory/data/in/raw/full/USA-MA-Broad_MGH-22989-2024.lNDM_F11.HLWLWDRX5.1.hs_depleted.subset_1M_merged/prediction_results_highestscore.csv \
  --refactored /tmp/virnucpro_bench_1gpu/*_merged/prediction_results_highestscore.csv \
  --mode consensus \
  --tolerance 0.01 \
  --output detailed \
  --mismatch-limit 50
```

**Run per-frame comparison (secondary check):**
```bash
python compare_virnucpro_outputs.py \
  --vanilla /data/Broad_Viral_Respiratory/data/in/raw/full/USA-MA-Broad_MGH-22989-2024.lNDM_F11.HLWLWDRX5.1.hs_depleted.subset_1M_merged/prediction_results.txt \
  --refactored /tmp/virnucpro_bench_1gpu/*_merged/prediction_results.txt \
  --mode per-frame \
  --tolerance 0.01 \
  --output detailed \
  --mismatch-limit 50
```

**Success criteria:**
- Consensus label agreement (Is_Virus match): >=99% of sequences
- This aligns with Phase 10.2 findings (99.87% consensus label agreement on 18 test sequences; 0.13% mismatches are all borderline cases)
- Note: Score differences are expected due to FlashAttention FP32 accumulation vs v1.0 FP16 accumulation. This is cosmetic and validated in Phase 10.2.

**Record and report:**
1. Consensus label agreement percentage
2. Number of mismatches (label disagreements)
3. Score difference statistics (mean, max, std)
4. Whether >=99% consensus label agreement threshold is met
5. Characterize any mismatches (are they borderline cases near 0.5 decision boundary?)
  </action>
  <verify>
1. Consensus comparison completes without error
2. Consensus label agreement >= 99%
3. No missing sequences in either direction (missing_in_refactored=0, missing_in_vanilla=0)
4. Per-frame comparison provides additional context on score differences
  </verify>
  <done>v2.0 predictions validated against v1.0 output. Consensus label agreement >= 99%. Score differences characterized and documented.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
1x RTX 4090 full pipeline benchmark on 1M subset dataset with correctness validation against v1.0 output.
  </what-built>
  <how-to-verify>
Review the results reported in the summary:

1. **Timing**: Did the pipeline complete in <1 hour on 1x RTX 4090?
   - If YES: Single-GPU target met
   - If NO: Note the actual time and identify bottleneck stage

2. **Correctness**: Is consensus label agreement >= 99%?
   - Expected: ~99.87% based on Phase 10.2 findings (0.13% borderline mismatches)
   - If YES: Correctness validated
   - If NO: Review mismatches - are they borderline (scores near 0.5)?

3. **Per-stage breakdown**: Review which stages dominate total time
   - ESM-2 should be the primary bottleneck
   - Translation should be fast (~minutes)
   - DNABERT-S timing provides the non-v2.0 component baseline

4. **Decision**: Proceed to 2x GPU benchmark (Plan 03)?
  </how-to-verify>
  <resume-signal>Type "approved" to proceed to 2x GPU benchmark, or describe issues to address</resume-signal>
</task>

</tasks>

<verification>
1. Full pipeline completed successfully (exit code 0) on 1M subset with 1x RTX 4090
2. Total wall time < 3600 seconds (1 hour target)
3. Per-stage timing breakdown logged for all 9 stages
4. Consensus label agreement >= 99% with v1.0 output
5. No missing sequences in comparison
6. Telemetry metrics (tokens/sec, packing efficiency) visible in log
</verification>

<success_criteria>
- Pipeline completes on 1M subset with 1x RTX 4090 in <1 hour
- Consensus label agreement >= 99% with v1.0 predictions
- Per-stage timing breakdown captured and logged
- Single-GPU baseline time recorded for scaling comparison in Plan 03
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-02-SUMMARY.md`
</output>
