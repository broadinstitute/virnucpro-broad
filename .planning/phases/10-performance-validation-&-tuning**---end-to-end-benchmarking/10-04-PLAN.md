---
phase: 10-performance-validation-tuning
plan: 04
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - tests/benchmarks/test_parameter_tuning.py
autonomous: true

must_haves:
  truths:
    - "Parameter sweep tests DataLoader parameters (num_workers, prefetch_factor) systematically"
    - "Packing parameter sweep tests token_budget and buffer_size impact on efficiency"
    - "Each sweep measures throughput, GPU utilization, and packing efficiency"
    - "Best configuration identified and printed for user evaluation"
    - "Results persisted as JSON for reproducibility"
    - "Parameter sweeps handle OOM gracefully and continue testing remaining configurations"
    - "Cleanup between configurations prevents worker process contamination"
    - "Baseline configuration measured first for fair comparison"
    - "Inter-parameter interactions tested (DataLoader x packing parameter combinations)"
    - "Results use TelemetryLogger schema for compatibility with generate_performance_report"
  artifacts:
    - path: "tests/benchmarks/test_parameter_tuning.py"
      provides: "Automated parameter sweep for DataLoader and packing optimization with OOM safety, cleanup, baseline comparison, and interaction testing"
      min_lines: 350
  key_links:
    - from: "tests/benchmarks/test_parameter_tuning.py"
      to: "virnucpro.data.dataloader_utils"
      via: "create_async_dataloader with configurable parameters"
      pattern: "create_async_dataloader"
    - from: "tests/benchmarks/test_parameter_tuning.py"
      to: "virnucpro.pipeline.async_inference"
      via: "AsyncInferenceRunner for single-GPU throughput measurement"
      pattern: "AsyncInferenceRunner"
    - from: "tests/benchmarks/test_parameter_tuning.py"
      to: "virnucpro.utils.telemetry"
      via: "TelemetryLogger for schema-compatible metric recording"
      pattern: "TelemetryLogger"
---

<objective>
Create parameter tuning benchmark suite that sweeps DataLoader and packing parameters to find optimal configuration, with OOM safety, cleanup between runs, baseline comparison, and inter-parameter interaction testing.

Purpose: If the <10h target is not met, this test identifies which parameters to adjust. Even if the target IS met, it documents optimal settings and their impact. Sweeps num_workers, prefetch_factor, token_budget, and buffer_size while measuring throughput and efficiency. Uses TelemetryLogger for schema-compatible output so results feed directly into generate_performance_report().

Output: `tests/benchmarks/test_parameter_tuning.py`
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-CONTEXT.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-RESEARCH.md
@.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-01-SUMMARY.md
@virnucpro/data/dataloader_utils.py
@virnucpro/data/collators.py
@virnucpro/pipeline/async_inference.py
@virnucpro/models/esm2_flash.py
@virnucpro/utils/telemetry.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: DataLoader and packing parameter sweep benchmarks with OOM safety, cleanup, baseline, and interaction testing</name>
  <files>tests/benchmarks/test_parameter_tuning.py</files>
  <action>
Create `tests/benchmarks/test_parameter_tuning.py` with:

Module docstring: "Parameter tuning benchmarks for DataLoader and packing optimization. Sweeps key parameters to find optimal configuration for production workloads. Handles OOM errors gracefully, cleans up between configurations, establishes baseline for fair comparison, and tests inter-parameter interactions."

Pytest markers: `pytestmark = [pytest.mark.slow, pytest.mark.gpu]`

1. **Helper: generate_tuning_workload**
   - `generate_tuning_workload(output_dir: Path, num_sequences: int = 2000, seed: int = 42) -> Path`
   - Generates FASTA with varied protein sequence lengths (same distribution as production)
   - 2000 sequences is enough to measure parameter impact without excessive time
   - Returns path to FASTA file

2. **Helper: cleanup_between_runs**
   - `cleanup_between_runs(dataloader=None, runner=None) -> None`
   - If dataloader is not None: call `del dataloader` (triggers worker shutdown)
   - If runner is not None: call `del runner`
   - Call `torch.cuda.empty_cache()` to release GPU memory back to allocator
   - Call `gc.collect()` to force garbage collection of Python objects (releases file handles, shared memory)
   - Call `time.sleep(2)` to allow DataLoader worker processes to terminate fully
   - Log at DEBUG level: "Cleanup complete: GPU cache cleared, GC collected, worker cooldown done"

3. **Helper: run_single_gpu_benchmark**
   - `run_single_gpu_benchmark(fasta_path: Path, output_dir: Path, model, device, num_workers: int = 4, prefetch_factor: int = 4, token_budget: int = 4096, buffer_size: int = 2000) -> Dict[str, Any]`
   - **Wrap entire function body in try/except** catching `torch.cuda.OutOfMemoryError` and `MemoryError`:
     - On OOM: log warning with config details, call `cleanup_between_runs()`, return dict:
       `{"status": "failed", "reason": "OOM", "config": {"num_workers": num_workers, "prefetch_factor": prefetch_factor, "token_budget": token_budget, "buffer_size": buffer_size}, "peak_memory_gb": torch.cuda.max_memory_allocated(device) / 1e9 if torch.cuda.is_available() else 0.0}`
     - On success: return dict with `"status": "success"` plus all metrics
   - **Use TelemetryLogger** for metric recording:
     - Create `TelemetryLogger(output_dir=output_dir, run_id=f"tune_{num_workers}w_{prefetch_factor}pf_{token_budget}tb_{buffer_size}bs")`
     - Set run_metadata with config parameters
     - During inference iteration: call `telemetry.record_batch()` for each batch with available metrics
     - After inference: call `telemetry.finalize()` to write JSON and get summary
     - Extract metrics from telemetry summary (NOT custom dict) to ensure schema compatibility:
       - `elapsed_sec`: from summary `elapsed_seconds`
       - `sequences_per_sec`: from summary (total_sequences / elapsed_seconds)
       - `tokens_per_sec`: from summary `avg_tokens_per_sec`
       - `gpu_util`: from summary `avg_gpu_util`
       - `packing_efficiency`: from summary `avg_packing_efficiency`
       - `peak_memory_gb`: `torch.cuda.max_memory_allocated(device) / 1e9`
       - `telemetry_path`: path to the JSON file from `telemetry.finalize()`
   - Creates IndexBasedDataset from fasta_path (or SequenceDataset if IndexBasedDataset requires index file)
   - Creates VarlenCollator with specified token_budget and buffer_size
   - Creates DataLoader with specified num_workers and prefetch_factor, pin_memory=True, batch_size=None
   - Creates AsyncInferenceRunner with the model on device
   - Runs inference via runner.run(dataloader)
   - Collects all results (force iteration)
   - **After collection, ALWAYS call cleanup_between_runs(dataloader, runner)** (in a finally block so cleanup happens even on non-OOM errors)

4. **TestParameterTuning** class:

   a. **test_baseline_validation** (runs FIRST - establishes comparison point):
   - Skip if not torch.cuda.is_available()
   - Generate 2K sequence workload
   - Load model ONCE using load_esm2_model with enable_fp16=True
   - Run default config: `num_workers=4, prefetch_factor=4, token_budget=4096, buffer_size=2000`
   - Save results as `tmp_path / "baseline.json"` (json.dump the full result dict)
   - Print:
     ```
     Baseline Configuration (defaults)
     ==================================
     num_workers=4, prefetch_factor=4, token_budget=4096, buffer_size=2000
     Throughput: XXX.X seq/sec, XX,XXX tok/sec
     GPU Utilization: XX.X%
     Packing Efficiency: XX.X%
     Peak Memory: X.XX GB
     ==================================
     ```
   - This establishes the comparison point for all subsequent sweeps

   b. **test_dataloader_parameter_sweep** (DataLoader tuning):
   - Skip if not torch.cuda.is_available()
   - Generate 2K sequence workload
   - Load model ONCE (expensive) using load_esm2_model with enable_fp16=True
   - **Load baseline**: attempt to load `tmp_path / "baseline.json"`. If not found, run baseline config first inline and save it.
   - Parameter grid:
     - num_workers: [2, 4, 8]
     - prefetch_factor: [2, 4, 8]
   - For each (num_workers, prefetch_factor) combination:
     - Run run_single_gpu_benchmark
     - Result includes status field - check it
   - Separate results into `successful` and `failed` lists based on `status` field
   - Print results table with speedup column:
     ```
     DataLoader Parameter Sweep
     ===================================================================
     Workers | Prefetch | Seq/sec | Tok/sec  | GPU Util | Memory  | Speedup
     2       | 2        | XXX.X   | XX,XXX   | XX.X%    | X.XXG   | 0.95x
     2       | 4        | XXX.X   | XX,XXX   | XX.X%    | X.XXG   | 1.02x
     ...
     ===================================================================
     Best: num_workers=X, prefetch_factor=Y (XXX.X seq/sec, X.XXx vs baseline)
     Baseline: num_workers=4, prefetch_factor=4 (XXX.X seq/sec)
     ```
   - If any configs failed (OOM), print separately:
     ```
     Failed configurations (OOM):
       num_workers=8, prefetch_factor=8 (peak memory: X.XX GB before OOM)
     ```
   - Speedup = config_seq_per_sec / baseline_seq_per_sec
   - Find best config from SUCCESSFUL results only (highest sequences_per_sec)
   - Save results JSON (including failed configs with status) to tmp_path / "dataloader_sweep.json"
   - No hard assertions - this is exploratory. Print recommendation.

   c. **test_packing_parameter_sweep** (Packing tuning):
   - Skip if not torch.cuda.is_available()
   - Generate 2K sequence workload
   - Load model ONCE
   - **Load baseline** (same pattern as DataLoader sweep - attempt load, fallback to inline run)
   - Parameter grid:
     - token_budget: [2048, 4096, 8192]
     - buffer_size: [500, 1000, 2000]
   - For each combination: run run_single_gpu_benchmark, separate successful/failed
   - Print results table with speedup column:
     ```
     Packing Parameter Sweep
     ======================================================================
     Budget  | Buffer | Seq/sec | Pack Eff | GPU Util | Memory  | Speedup
     2048    | 500    | XXX.X   | XX.X%    | XX.X%    | X.XXG   | 0.85x
     ...
     ======================================================================
     Best: token_budget=X, buffer_size=Y (XXX.X seq/sec, XX.X% eff, X.XXx vs baseline)
     Baseline: token_budget=4096, buffer_size=2000 (XXX.X seq/sec)
     ```
   - Print failed configs (OOM) separately, same format as DataLoader sweep
   - Find best config from SUCCESSFUL results considering BOTH throughput AND packing efficiency (efficiency must be >= 90%)
   - Save results JSON (including failed configs with status)
   - No hard assertions - exploratory.

   d. **test_interaction_sweep** (combined parameter interaction testing):
   - Skip if not torch.cuda.is_available()
   - Generate 2K sequence workload
   - Load model ONCE
   - **Load baseline** (same pattern)
   - Combined parameter grid - (num_workers, token_budget) pairs:
     - [(4, 2048), (4, 4096), (4, 8192), (8, 2048), (8, 4096), (8, 8192)]
   - Use fixed defaults for other params: prefetch_factor=4, buffer_size=2000
   - For each combination: run run_single_gpu_benchmark, separate successful/failed
   - Print interaction table:
     ```
     Parameter Interaction Sweep (num_workers x token_budget)
     ======================================================================
     Workers | Budget | Seq/sec | Pack Eff | GPU Util | Memory  | Speedup | Status
     4       | 2048   | XXX.X   | XX.X%    | XX.X%    | X.XXG   | 0.90x   | OK
     4       | 4096   | XXX.X   | XX.X%    | XX.X%    | X.XXG   | 1.00x   | OK
     4       | 8192   | XXX.X   | XX.X%    | XX.X%    | X.XXG   | 1.15x   | OK
     8       | 2048   | XXX.X   | XX.X%    | XX.X%    | X.XXG   | 0.95x   | OK
     8       | 4096   | XXX.X   | XX.X%    | XX.X%    | X.XXG   | 1.10x   | OK
     8       | 8192   | ---     | ---      | ---      | X.XXG   | ---     | OOM
     ======================================================================
     Sweet spot: num_workers=X, token_budget=Y (XXX.X seq/sec, >90% packing, no OOM)
     ```
   - Identify sweet spot: highest throughput among successful configs that maintain >90% packing efficiency
   - Save results JSON to tmp_path / "interaction_sweep.json"
   - No hard assertions - exploratory.

   e. **test_optimal_config_validation** (validate best settings):
   - Skip if not torch.cuda.is_available()
   - Loads best config from dataloader_sweep.json, packing_sweep.json, and interaction_sweep.json if they exist
   - If they don't exist, uses current defaults
   - Runs a larger workload (5K sequences) with the best config
   - Prints projected 6M sequence runtime
   - No assertions - informational only

Imports:
- pytest, torch, time, random, json, logging, itertools, gc from stdlib
- Path from pathlib
- AsyncInferenceRunner from virnucpro.pipeline.async_inference
- load_esm2_model from virnucpro.models.esm2_flash
- VarlenCollator from virnucpro.data.collators
- TelemetryLogger from virnucpro.utils.telemetry
- create_async_dataloader from virnucpro.data.dataloader_utils (if exists, else build manually)

IMPORTANT: The model loading is expensive (~30s for ESM-2 3B). Load model ONCE per test class using a class-scoped fixture or at the start of each test method. cleanup_between_runs handles DataLoader/runner cleanup but model stays loaded across configs within a test.
  </action>
  <verify>
    python -c "from tests.benchmarks.test_parameter_tuning import TestParameterTuning; print('Parameter tuning imports OK')"
  </verify>
  <done>
    Baseline config measured first for fair comparison. DataLoader sweep tests num_workers x prefetch_factor grid with speedup vs baseline. Packing sweep tests token_budget x buffer_size grid with efficiency + throughput vs baseline. Interaction sweep tests (num_workers, token_budget) pairs to find combined sweet spot. OOM errors caught gracefully - failed configs reported separately, remaining configs continue. Cleanup (del objects, torch.cuda.empty_cache, gc.collect, 2s sleep) runs between every configuration. All results use TelemetryLogger for schema-compatible JSON output.
  </done>
</task>

</tasks>

<verification>
- `python -c "from tests.benchmarks.test_parameter_tuning import TestParameterTuning"`
- Baseline test: runs default config (4/4/4096/2000) and prints throughput/GPU util/packing
- DataLoader sweep: 3x3 grid of num_workers x prefetch_factor with speedup vs baseline
- Packing sweep: 3x3 grid of token_budget x buffer_size with speedup vs baseline
- Interaction sweep: 6 (num_workers, token_budget) pairs with OOM detection
- OOM handling: failed configs reported with "OOM" status, remaining configs continue
- Cleanup: torch.cuda.empty_cache + gc.collect + 2s sleep between every config
- Results tables include speedup column comparing to baseline
- JSON results use TelemetryLogger schema (compatible with generate_performance_report)
</verification>

<success_criteria>
Baseline configuration measured first and used as comparison point for all sweeps. Parameter sweeps run systematically across DataLoader (num_workers, prefetch_factor), packing (token_budget, buffer_size), and their interactions. OOM errors caught gracefully - failed configs marked as failed(OOM) and remaining configs continue testing. Cleanup between configurations (del objects, empty_cache, gc.collect, 2s sleep) prevents worker process contamination. Each configuration measured for throughput, GPU utilization, and packing efficiency with speedup vs baseline. Best configuration identified from successful runs only. All results persisted as JSON via TelemetryLogger for schema compatibility with generate_performance_report.
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-validation-&-tuning**---end-to-end-benchmarking/10-04-SUMMARY.md`
</output>
