---
phase: 11-code-quality-foundations
plan: 04
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - virnucpro/pipeline/gpu_worker.py
  - tests/unit/test_gpu_worker.py
autonomous: true

must_haves:
  truths:
    - "gpu_worker() delegates to focused helper functions instead of inlining all logic"
    - "Each helper function has a clear single responsibility"
    - "gpu_worker() is a module-level function (pickle-compatible with spawn)"
    - "All existing gpu_worker tests pass unchanged"
  artifacts:
    - path: "virnucpro/pipeline/gpu_worker.py"
      provides: "Refactored gpu_worker with focused helper functions"
      contains: "_load_model"
    - path: "tests/unit/test_gpu_worker.py"
      provides: "Unchanged existing tests (behavior equivalence)"
  key_links:
    - from: "virnucpro/pipeline/gpu_worker.py"
      to: "virnucpro/utils/precision.py"
      via: "should_use_fp16() for FP16 disable check (Plan 05 migrates should_use_fp16 to EnvConfig internally)"
      pattern: "should_use_fp16"
---

<objective>
Refactor `gpu_worker()` (currently 462 lines, one monolithic function) into focused helper functions.

Purpose: Addresses QUAL-04. The gpu_worker function handles logging setup, CUDA init, checkpoint config extraction, manifest loading, checkpoint resumption, index loading/filtering, model loading, dataset/dataloader creation, SIGTERM handler registration, inference execution, shard assembly, HDF5 saving, success reporting, and error handling all in one function. Extract the core logic blocks into named helpers.

Output: gpu_worker() as a clean orchestrator calling focused helpers, all existing tests passing.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@virnucpro/pipeline/gpu_worker.py (gpu_worker function lines 48-462)
@.planning/phases/11-code-quality-foundations/11-01-SUMMARY.md (EnvConfig for env var access)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract model loading, checkpoint resume, and shard saving into helper functions</name>
  <files>virnucpro/pipeline/gpu_worker.py</files>
  <action>
    Extract the following module-level helper functions from gpu_worker(). These MUST be module-level (not nested) for pickle compatibility with multiprocessing spawn context.

    **Functions to extract:**

    1. `_extract_checkpoint_config(model_config: Dict, output_dir: Path) -> Dict`
       Extract lines 150-163 (checkpoint config extraction). Returns dict with:
       enable_checkpointing, force_restart, checkpoint_base_dir, checkpoint_seq_threshold,
       checkpoint_time_threshold, checkpoint_dir.

    2. `_load_checkpoint_manifest(rank: int, enable_checkpointing: bool, checkpoint_base_dir: Path, logger) -> Optional['CheckpointManifest']`
       Extract lines 166-175 (manifest loading). Returns manifest or None.

    3. `_resume_from_existing_checkpoints(rank: int, checkpoint_dir: Path, force_restart: bool, logger) -> Tuple[Set[str], Optional[np.ndarray]]`
       Extract lines 178-216 (checkpoint resumption with v1.0 format detection). Returns (resumed_ids set, resumed_embeddings array or None).

    4. `_load_and_filter_index(rank: int, world_size: int, index_path: Path, resumed_ids: Set[str], logger) -> List[int]`
       Extract lines 218-241 (index loading and filtering). Returns filtered indices.

    5. `_load_model(model_config: Dict, device: torch.device, logger) -> Tuple[torch.nn.Module, Any]`
       Extract lines 246-285 (model loading for ESM-2 or DNABERT-S). Returns (model, batch_converter/tokenizer).

       **Important**: Keep calling `should_use_fp16()` from `virnucpro.utils.precision` as-is.
       Do NOT replace it with direct EnvConfig access. The should_use_fp16() function encapsulates
       the FP16 decision logic + warning message. Plan 05 will migrate should_use_fp16() internally
       to use EnvConfig, maintaining separation of concerns.

    6. `_save_shard(rank: int, output_dir: Path, all_embeddings: List, all_ids: List[str], resumed_ids: Set[str], resumed_embeddings: Optional[np.ndarray], logger) -> Path`
       Extract lines 345-371 (shard assembly and HDF5 saving). Returns shard_path.
       Note: The `all_embeddings.insert(0, ...)` stays as-is (short list, not a queue pattern).

    7. `_report_error(rank: int, error: Exception, results_queue: Queue, logger)`
       Extract lines 385-462 (error categorization and reporting). Handles RuntimeError subcases (numerical instability, OOM, CUDA runtime, generic) and unexpected Exception.

    **The refactored gpu_worker() should look roughly like:**
    ```python
    def gpu_worker(rank, world_size, results_queue, index_path, output_dir, model_config):
        log_dir = output_dir / "logs"
        log_file = setup_worker_logging(rank, log_dir)
        logger = logging.getLogger(f"gpu_worker_{rank}")
        logger.info(f"Worker {rank}/{world_size} starting")

        runner = None
        try:
            # CUDA init
            device = torch.device('cuda:0')
            torch.cuda.set_device(device)

            # Checkpoint config
            ckpt_config = _extract_checkpoint_config(model_config, output_dir)
            manifest = _load_checkpoint_manifest(rank, ckpt_config['enable_checkpointing'], ...)
            resumed_ids, resumed_embeddings = _resume_from_existing_checkpoints(...)
            indices = _load_and_filter_index(rank, world_size, index_path, resumed_ids, logger)

            # Model + DataLoader
            model, batch_converter = _load_model(model_config, device, logger)
            dataset = IndexBasedDataset(index_path, indices)
            collator = VarlenCollator(batch_converter)
            dataloader = create_async_dataloader(dataset, collator, ...)

            # Inference
            runner = AsyncInferenceRunner(model, device, ...)
            # ... SIGTERM handler, inference loop ...

            # Save and report
            shard_path = _save_shard(rank, output_dir, all_embeddings, all_ids, ...)
            results_queue.put({...success status...})

        except (RuntimeError, Exception) as e:
            _report_error(rank, e, results_queue, logger)
    ```

    Constraints:
    - All helper functions MUST be module-level (not nested inside gpu_worker) for pickle compatibility
    - Keep the SIGTERM handler registration inline in gpu_worker (it captures the runner local variable)
    - Keep the inference loop inline (it builds all_embeddings and all_ids)
    - Error handling can be extracted since it just reports and exits
  </action>
  <verify>
    pytest tests/unit/test_gpu_worker.py -v -x
    # Verify helper functions are module-level
    grep -n "^def _" virnucpro/pipeline/gpu_worker.py
    # Should show new module-level helper functions
  </verify>
  <done>
    gpu_worker() is a clean orchestrator under ~80 lines of core logic. Helper functions each handle one concern. All helpers are module-level for pickle compatibility. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
pytest tests/unit/test_gpu_worker.py -v
pytest tests/unit/test_async_inference.py -v  # async_inference is used by gpu_worker
# All tests pass with refactored code
</verification>

<success_criteria>
- gpu_worker() is a clean orchestrator, no longer a 462-line monolith
- Each extracted function has a single clear responsibility
- All helpers are module-level (pickle-compatible)
- should_use_fp16() call preserved as-is (Plan 05 migrates it to EnvConfig internally)
- All existing tests pass (1:1 behavior equivalence)
</success_criteria>

<output>
After completion, create `.planning/phases/11-code-quality-foundations/11-04-SUMMARY.md`
</output>
