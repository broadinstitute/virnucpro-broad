---
phase: 10.2-flashattention-scoring-divergence
plan: 02
type: execute
wave: 2
depends_on: ["10.2-01"]
files_modified:
  - virnucpro/models/esm2_flash.py
  - virnucpro/cli/predict.py
  - virnucpro/pipeline/prediction.py
  - tests/integration/test_prediction_divergence.py
autonomous: true

must_haves:
  truths:
    - "forward_packed() accepts v1_compatible parameter that uses standard attention (matching v1.0 FP16 accumulation behavior)"
    - "CLI exposes --v1-attention flag for exact v1.0 ESM-2 embedding compatibility"
    - "v1_compatible=True path produces embeddings identical to forward() within FP16 precision tolerance"
    - "Decision on divergence acceptance is documented based on Plan 01 measurement results"
  artifacts:
    - path: "virnucpro/models/esm2_flash.py"
      provides: "forward_packed with v1_compatible parameter"
      contains: "v1_compatible"
    - path: "virnucpro/cli/predict.py"
      provides: "CLI --v1-attention flag"
      contains: "v1_attention"
    - path: "tests/integration/test_prediction_divergence.py"
      provides: "Test validating v1_compatible path equivalence"
      contains: "test_v1_compatible_matches_standard"
  key_links:
    - from: "virnucpro/cli/predict.py"
      to: "virnucpro/pipeline/prediction.py"
      via: "v1_attention flag passed through to run_prediction_pipeline"
      pattern: "v1_attention"
    - from: "virnucpro/models/esm2_flash.py"
      to: "virnucpro/models/esm2_flash.py"
      via: "forward_packed delegates to _forward_packed_fallback when v1_compatible=True"
      pattern: "v1_compatible.*_forward_packed_fallback"
---

<objective>
Add v1.0-compatible attention fallback to forward_packed() and wire through CLI.

Purpose: Regardless of Plan 01's label agreement results, having a v1_compatible option provides value: it enables exact v1.0 embedding reproduction when needed (production validation, classifier retraining comparison, debugging). The existing `_forward_packed_fallback` method already unpacks to 2D padded format and runs standard forward() — this plan promotes it from "FlashAttention-unavailable fallback" to "intentional v1.0 compatibility mode" with proper CLI integration.

Output: `forward_packed()` gains `v1_compatible=True` parameter. CLI gains `--v1-attention` flag. Integration test validates v1_compatible path matches standard forward() output exactly. Divergence decision documented in summary based on Plan 01 results.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/10.2-flashattention-scoring-divergence/10.2-RESEARCH.md
@.planning/phases/10.2-flashattention-scoring-divergence/10.2-01-SUMMARY.md
@virnucpro/models/esm2_flash.py
@virnucpro/cli/predict.py
@virnucpro/pipeline/prediction.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add v1_compatible parameter to forward_packed()</name>
  <files>virnucpro/models/esm2_flash.py</files>
  <action>
Modify `forward_packed()` in ESM2WithFlashAttention to accept a `v1_compatible: bool = False` parameter.

When `v1_compatible=True`:
1. Log an info message: "Using v1.0-compatible standard attention path (FP16 accumulation, slower)"
2. Delegate to the existing `_forward_packed_fallback()` method, which already:
   - Unpacks 1D packed tokens to 2D padded format
   - Runs `self.forward()` (standard ESM-2 attention with torch.bmm)
   - Repacks 2D output back to 1D packed format
3. This matches v1.0 behavior exactly because `self.forward()` uses the standard ESM-2 MultiheadAttention which calls torch.bmm with cuBLAS defaults (FP16 accumulation on Ampere+).

When `v1_compatible=False` (default): Existing FlashAttention varlen path (no changes).

Update the docstring to document the new parameter:
```
v1_compatible: If True, use standard attention (torch.bmm with FP16 accumulation)
    instead of FlashAttention varlen (FP32 accumulation). Produces embeddings
    identical to v1.0 pipeline. Sacrifices packing throughput (2-3x slower)
    but ensures exact numerical compatibility with v1.0-trained classifier.
    Default: False (use FlashAttention for maximum throughput).
```

Also update `_forward_packed_fallback()` docstring to note it now serves dual purpose:
- Fallback when FlashAttention is unavailable (existing)
- V1.0 compatibility mode when explicitly requested (new)

Do NOT change the `_forward_packed_fallback()` implementation - it already does exactly what's needed. The only code change is adding the `v1_compatible` parameter check at the top of `forward_packed()`, before the `FLASH_ATTN_AVAILABLE` check.

The logic order should be:
1. If `v1_compatible=True` → use `_forward_packed_fallback()` (regardless of FlashAttention availability)
2. If `not FLASH_ATTN_AVAILABLE` → use `_forward_packed_fallback()` (existing behavior)
3. Otherwise → use FlashAttention varlen path (existing behavior)
  </action>
  <verify>
1. Read the modified `forward_packed()` method and verify the parameter is added
2. Run `pytest tests/integration/test_packed_equivalence.py -v -k "test_short"` to ensure existing packed tests still pass (v1_compatible defaults to False)
3. Run `pytest tests/unit/ -v -k "esm"` to check no unit test regressions
  </verify>
  <done>
`forward_packed()` accepts `v1_compatible` parameter. When True, delegates to existing `_forward_packed_fallback()`. Default is False (FlashAttention path unchanged). Existing tests pass without modification.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire --v1-attention flag through CLI and add validation test</name>
  <files>
    virnucpro/cli/predict.py
    virnucpro/pipeline/prediction.py
    tests/integration/test_prediction_divergence.py
  </files>
  <action>
**Part A: CLI flag**

Add `--v1-attention` flag to the predict command in `virnucpro/cli/predict.py`:

```python
@click.option('--v1-attention',
              is_flag=True,
              help='Use v1.0-compatible standard attention for ESM-2 (exact match, 2-3x slower)')
```

Pass `v1_attention` through to the prediction pipeline call. In the `predict` function body, where `use_v2` is computed (around line 252), add logic:
- If `v1_attention` and `use_v2`: Log info "v2.0 ESM-2 with v1.0-compatible attention (standard bmm, FP16 accumulation)"
- Pass `v1_attention` to `run_prediction_pipeline()` as a new parameter

**Part B: Pipeline wiring**

In `virnucpro/pipeline/prediction.py`, add `v1_attention: bool = False` parameter to `run_prediction_pipeline()`. When the v2.0 architecture path calls `run_multi_gpu_inference()`, pass a model config that includes `v1_compatible=True` so the gpu_worker passes it through to `forward_packed()`.

The simplest approach: Add `v1_compatible` to the RuntimeConfig or model_config dict that flows to the gpu_worker. Check how `enable_fp16` flows through - follow the same pattern. The gpu_worker already receives model configuration and passes it to the model; add `v1_compatible` to that flow.

If the wiring through RuntimeConfig/gpu_worker is complex, a simpler alternative: Just set an environment variable `VIRNUCPRO_V1_ATTENTION=true` from CLI, and read it in `forward_packed()`. This follows the existing `VIRNUCPRO_DISABLE_PACKING` pattern (Phase 6 decision). This is the preferred approach for simplicity.

Specifically:
1. In `predict.py`: If `v1_attention`, set `os.environ['VIRNUCPRO_V1_ATTENTION'] = 'true'`
2. In `forward_packed()`: At the top, check `os.environ.get('VIRNUCPRO_V1_ATTENTION', '').lower() == 'true'` and if so, set `v1_compatible = True` (override the parameter)

This avoids modifying RuntimeConfig, gpu_worker, and the entire v2.0 pipeline config chain.

**Part C: Validation test**

Add a new test class to `tests/integration/test_prediction_divergence.py`:

```python
class TestV1CompatiblePath:
    """Validate that v1_compatible=True produces identical output to forward()."""

    def test_v1_compatible_matches_standard(self, esm_model):
        """v1_compatible path should produce embeddings identical to standard forward."""
```

This test should:
1. Use the same test sequences as Plan 01's test
2. Run sequences through `model.forward(tokens)` (standard path)
3. Run same sequences through `model.forward_packed(input_ids, cu_seqlens, max_seqlen, v1_compatible=True)`
4. Extract mean-pooled embeddings from both (same BOS/EOS handling as Plan 01)
5. Assert cosine similarity > 0.999 for ALL sequences (strict threshold - these should be nearly identical since both use standard attention)
6. Assert max absolute difference < 0.001 (should be very small, only padding/batching differences)

This validates that the v1_compatible flag correctly routes through the standard attention path.
  </action>
  <verify>
1. `pytest tests/integration/test_prediction_divergence.py -v -k "test_v1_compatible"` passes on GPU
2. `pytest tests/unit/test_cli_integration.py -v` passes (no CLI regressions)
3. `python -m virnucpro predict --help` shows `--v1-attention` flag in help text
4. The v1_compatible test shows cosine similarity > 0.999 (much higher than FlashAttention path)
  </verify>
  <done>
CLI has `--v1-attention` flag. Environment variable `VIRNUCPRO_V1_ATTENTION` controls v1.0 compatibility mode. forward_packed() respects both direct parameter and env var. Integration test validates v1_compatible path produces near-identical output to standard forward(). The divergence decision is documented in the plan summary based on Plan 01's measurement results.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/integration/test_prediction_divergence.py -v -s` passes all tests (both Plan 01 and Plan 02 tests)
2. `pytest tests/integration/test_packed_equivalence.py -v` still passes (no regression)
3. `python -m virnucpro predict --help` shows `--v1-attention` flag
4. `VIRNUCPRO_V1_ATTENTION=true` environment variable is respected by forward_packed()
5. v1_compatible=True path produces cosine similarity > 0.999 vs standard forward()
</verification>

<success_criteria>
- forward_packed() has working v1_compatible parameter
- CLI exposes --v1-attention flag for production use
- v1_compatible path validated to match standard attention output (cosine > 0.999)
- Divergence decision documented based on Plan 01's quantitative results
- No regressions in existing packed equivalence or FP16 tests
</success_criteria>

<output>
After completion, create `.planning/phases/10.2-flashattention-scoring-divergence/10.2-02-SUMMARY.md`

In the summary, document the DIVERGENCE DECISION based on Plan 01's results:
- What was the label agreement %?
- What was the confidence correlation?
- Based on the decision tree: accept v2.0, validate further, or default to v1_compatible?
- Recommendation for Phase 10 benchmarks: which attention mode to use?
</output>
