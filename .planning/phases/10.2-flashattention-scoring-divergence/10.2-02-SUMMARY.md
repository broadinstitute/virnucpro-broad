---
phase: 10.2-flashattention-scoring-divergence
plan: 02
subsystem: models
tags: [flashattention, attention, v1-compatibility, esm2, cli]

# Dependency graph
requires:
  - phase: 10.2-01
    provides: Prediction-level divergence test showing 100% label agreement
provides:
  - v1.0-compatible attention fallback in forward_packed() via v1_compatible parameter
  - CLI --v1-attention flag for exact v1.0 embedding reproduction
  - VIRNUCPRO_V1_ATTENTION environment variable for compatibility mode
  - Integration test validating v1_compatible path matches standard forward()
affects: [10-benchmarks, production-validation]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - Environment variable override pattern for runtime configuration

key-files:
  created: []
  modified:
    - virnucpro/models/esm2_flash.py
    - virnucpro/cli/predict.py
    - tests/integration/test_prediction_divergence.py
    - tests/unit/test_esm2_packed.py

key-decisions:
  - "v1_compatible parameter provides safety valve for exact v1.0 embedding reproduction"
  - "Environment variable VIRNUCPRO_V1_ATTENTION enables compatibility mode without code changes"
  - "v1_compatible delegates to existing _forward_packed_fallback method (dual-purpose: FlashAttention unavailable + v1.0 compatibility)"

patterns-established:
  - "Environment variable precedence: env var overrides parameter defaults for runtime control"
  - "Dual-purpose fallback methods: _forward_packed_fallback serves both FlashAttention unavailability and explicit v1.0 compatibility"

# Metrics
duration: 6min
completed: 2026-02-08
---

# Phase 10.2 Plan 02: V1.0-Compatible Attention Fallback Summary

**v1.0-compatible attention fallback via v1_compatible parameter and --v1-attention CLI flag, producing near-identical embeddings (cosine 0.999999-1.0) to standard forward()**

## Performance

- **Duration:** 6 min
- **Started:** 2026-02-08T04:02:23Z
- **Completed:** 2026-02-08T04:08:08Z
- **Tasks:** 2
- **Files modified:** 4

## Accomplishments

- Added v1_compatible parameter to forward_packed() that delegates to standard attention path
- Wired --v1-attention CLI flag through predict command with environment variable control
- Validated v1_compatible path produces embeddings nearly identical to standard forward() (cosine similarity 0.999999-1.0, max abs diff 0.0)
- Documented divergence decision: ACCEPT v2.0 FlashAttention (100% label agreement from Plan 01), but provide v1_compatible as safety valve

## Task Commits

Each task was committed atomically:

1. **Task 1: Add v1_compatible parameter to forward_packed()** - `848eae2` (feat)
   - Added v1_compatible parameter (default False) to ESM2WithFlashAttention.forward_packed()
   - Environment variable VIRNUCPRO_V1_ATTENTION=true overrides parameter
   - Updated _forward_packed_fallback() docstring to document dual purpose
   - Updated unit test to verify new parameter signature and default value

2. **Task 2: Wire --v1-attention flag through CLI and add validation test** - `51adb24` (feat)
   - Added --v1-attention flag to predict command
   - Set VIRNUCPRO_V1_ATTENTION env var when flag enabled
   - Added TestV1CompatiblePath::test_v1_compatible_matches_standard integration test
   - Test validates v1_compatible path produces near-identical embeddings to standard forward()

## Files Created/Modified

- `virnucpro/models/esm2_flash.py` - Added v1_compatible parameter to forward_packed(), env var check, dual-purpose fallback docstring
- `virnucpro/cli/predict.py` - Added --v1-attention flag, environment variable setting, logging for v1.0 attention mode
- `tests/integration/test_prediction_divergence.py` - Added TestV1CompatiblePath class with v1_compatible validation test
- `tests/unit/test_esm2_packed.py` - Updated signature test to include v1_compatible parameter

## Decisions Made

**DIVERGENCE DECISION (based on Plan 01 results):**

**Quantitative Results:**
- Label agreement: 100.0% (all 18 test sequences match)
- Confidence correlation: 1.000000 (perfect Pearson correlation)
- Embedding cosine similarity: 0.999999-1.000000, mean 1.000000
- Mean absolute difference: 0.000000

**Decision: ACCEPT v2.0 FlashAttention** (per research decision tree)
- Rationale: >99% label agreement threshold met (achieved 100%)
- The embedding-level divergence originally reported (70.3% at 1e-3, 96% at 1e-2) was resolved by bug fixes in Phase 10.2-01
- Divergence is cosmetic - does not affect downstream predictions

**v1_compatible Parameter Purpose:**
Despite accepting v2.0, the v1_compatible option provides value:
1. Production validation: Exact v1.0 embedding reproduction when needed
2. Classifier retraining comparison: Baseline for v2.0-trained classifier
3. Debugging: Isolate attention implementation vs other factors
4. Safety valve: Emergency fallback if unforeseen issues arise

**Recommendation for Phase 10 Benchmarks:**
- Use default FlashAttention path (v1_compatible=False) for throughput measurements
- Document v1_compatible option availability in migration guide
- No production deployment concerns - v2.0 FlashAttention is correct and production-ready

## Deviations from Plan

None - plan executed exactly as written.

The plan correctly anticipated that v1_compatible would be valuable regardless of Plan 01's results. The existing _forward_packed_fallback method was promoted from "FlashAttention-unavailable fallback" to "intentional v1.0 compatibility mode" without any implementation changes - exactly as planned.

## Issues Encountered

None - straightforward implementation leveraging existing fallback method.

## User Setup Required

None - no external service configuration required.

## Next Phase Readiness

**Phase 10 Benchmarks Ready:**
- v2.0 FlashAttention validated (100% label agreement)
- v1_compatible fallback available if needed
- CLI integration complete (from Phase 10.1)
- All blockers removed - Phase 10 benchmarks can proceed

**Key Context for Phase 10:**
- Use default attention mode (v1_compatible=False) for throughput measurements
- FlashAttention divergence is cosmetic, does not affect predictions
- v1_compatible option documented but not required for production

**Production Deployment:**
- v2.0 FlashAttention is production-ready (100% label agreement validated)
- --v1-attention flag available if exact v1.0 reproduction needed
- No performance concerns - proceed with v2.0 architecture

---
*Phase: 10.2-flashattention-scoring-divergence*
*Completed: 2026-02-08*
