---
phase: 10.2-flashattention-scoring-divergence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/integration/test_prediction_divergence.py
autonomous: true

must_haves:
  truths:
    - "Prediction-level impact of FlashAttention divergence is measured with label agreement, confidence correlation, and per-sequence metrics"
    - "Test produces a clear diagnostic report showing whether v2.0 predictions match v1.0 within acceptable thresholds"
    - "Both forward() (standard bmm attention) and forward_packed() (FlashAttention varlen) paths are exercised on identical sequences"
  artifacts:
    - path: "tests/integration/test_prediction_divergence.py"
      provides: "GPU integration test comparing v1.0 vs v2.0 attention paths at prediction level"
      contains: "test_prediction_level_divergence"
  key_links:
    - from: "tests/integration/test_prediction_divergence.py"
      to: "virnucpro/models/esm2_flash.py"
      via: "ESM2WithFlashAttention.forward() and .forward_packed()"
      pattern: "model\\.forward\\(|model\\.forward_packed\\("
---

<objective>
Quantify the downstream prediction impact of FlashAttention vs standard attention divergence.

Purpose: The embedding-level divergence (70.3% match at 1e-3, 96% at 1e-2) is alarming but may not affect final predictions. Small embedding differences can cancel out in the downstream MLP classifier. Before implementing any fix, we need to measure what actually matters: do v1.0 and v2.0 produce the same viral/non-viral classification labels? This test provides the data for the accept/fix decision.

Output: Integration test that runs identical sequences through both attention paths, extracts mean-pooled embeddings, and compares the resulting classification labels and confidence scores. Reports label agreement %, confidence correlation, and per-sequence divergence analysis.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/10.2-flashattention-scoring-divergence/10.2-RESEARCH.md
@virnucpro/models/esm2_flash.py
@virnucpro/pipeline/models.py
@virnucpro/pipeline/predictor.py
@tests/integration/test_packed_equivalence.py
@tests/integration/test_fp16_validation.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create prediction-level divergence test</name>
  <files>tests/integration/test_prediction_divergence.py</files>
  <action>
Create a GPU integration test that measures prediction-level impact of FlashAttention vs standard attention divergence. The test must:

1. Load ESM-2 3B model with FP16 (production configuration) using `load_esm2_model()`.

2. Create a set of test protein sequences (15-20 sequences, varying lengths from 10-400 aa). Use deterministic sequences, not random - use repeating amino acid patterns like in test_packed_equivalence.py.

3. For each sequence, extract embeddings via BOTH paths:
   - **v1.0 path**: Use `model.forward(tokens, repr_layers=[36])` which goes through standard ESM-2 attention (torch.bmm with cuBLAS, FP16 accumulation on Ampere+). Extract mean-pooled embedding from positions 1:len+1 (skip BOS, exclude EOS and padding).
   - **v2.0 path**: Use `model.forward_packed(input_ids, cu_seqlens, max_seqlen, repr_layers=[36])` which goes through FlashAttention varlen (FP32 accumulation). Extract mean-pooled embedding from packed output using cu_seqlens boundaries, skipping BOS (position 0) and EOS (last position) for each sequence.

4. For the v2.0 (packed) path, construct packed inputs manually:
   - Tokenize each sequence individually with batch_converter
   - Strip padding from each tokenized sequence
   - Concatenate all stripped tokens into a 1D tensor
   - Build cu_seqlens from the sequence lengths (including BOS and EOS tokens)
   - Compute max_seqlen

5. Compute embedding-level metrics:
   - Per-sequence cosine similarity between v1.0 and v2.0 embeddings
   - Mean absolute difference
   - Report the distribution (min, mean, max cosine similarity)

6. Simulate MLP classification for both embedding sets:
   - Since the real MLP expects concatenated DNABERT-S + ESM-2 features (3328 dim), and we only have ESM-2 (2560 dim), create a simple diagnostic: build a random but deterministic (seeded) linear classifier that maps 2560 -> 2, run both embedding sets through it, compare outputs.
   - Compare: (a) label agreement rate, (b) softmax confidence correlation (Pearson), (c) max confidence score difference.
   - NOTE: This is a PROXY for the real classifier, not a replacement. The point is to measure whether embedding differences survive a linear projection + softmax, which is the same operation structure as the real MLP.

7. Log comprehensive results and assert thresholds:
   - Log all metrics in a clear table format
   - Assert cosine similarity > 0.99 for all sequences (Phase 8 validated threshold)
   - Assert label agreement >= 95% (from research decision tree)
   - Log the recommendation based on research decision tree:
     * >99% label agreement: "ACCEPT v2.0 - divergence cosmetic"
     * 95-99%: "VALIDATE on production dataset before accepting"
     * <95%: "IMPLEMENT v1.0-compatible fallback"

Use `@pytest.mark.gpu` and `@pytest.mark.slow` markers. Use `pytestmark = pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA required")` at module level.

Follow existing test patterns from test_packed_equivalence.py:
- Module-level `esm_model` fixture with `scope="module"` that loads model once
- Class-based test organization
- Detailed logging with logger

IMPORTANT: The mean-pooling for the v1.0 path must skip BOS (position 0) and use positions 1:seq_len+1. The mean-pooling for the v2.0 packed path must also skip the BOS token (first token of each sequence in packed format) and the EOS token (last token of each sequence). Use cu_seqlens boundaries to identify each sequence's tokens in the packed output. This matches the bug fix that was already applied (EOS exclusion from mean pooling).

IMPORTANT: Convert embeddings to FP32 before computing metrics (embeddings come out in FP16 from the model).
  </action>
  <verify>
Run `pytest tests/integration/test_prediction_divergence.py -v -s` on a GPU machine. The test should:
1. Load ESM-2 3B successfully
2. Process all sequences through both paths without errors
3. Report cosine similarity, label agreement, and confidence metrics
4. Pass all assertions (cosine > 0.99, label agreement >= 95%)
  </verify>
  <done>
Test file exists at tests/integration/test_prediction_divergence.py. Test exercises both forward() and forward_packed() paths on identical sequences, extracts mean-pooled embeddings, compares at embedding level (cosine similarity) and prediction level (label agreement through proxy classifier), and produces a diagnostic report with clear recommendation based on the research decision tree.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/integration/test_prediction_divergence.py -v -s` passes on GPU
2. Test output shows cosine similarity, label agreement %, and confidence correlation
3. Test produces actionable recommendation (accept/validate/fallback)
4. No import errors or missing dependencies
</verification>

<success_criteria>
- Prediction-level divergence quantified with concrete metrics
- Clear recommendation produced based on research decision tree thresholds
- Test is reproducible (deterministic sequences and classifier seed)
- Results documented in test output for Phase 10.2 decision-making
</success_criteria>

<output>
After completion, create `.planning/phases/10.2-flashattention-scoring-divergence/10.2-01-SUMMARY.md`
</output>
