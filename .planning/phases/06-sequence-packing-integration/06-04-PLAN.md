---
phase: 06-sequence-packing-integration
plan: 04
type: execute
wave: 3
depends_on: ["06-03"]
files_modified:
  - virnucpro/pipeline/async_inference.py
autonomous: true

must_haves:
  truths:
    - "Packed batches (with cu_seqlens) processed through model.forward_packed"
    - "NotImplementedError replaced with working packed inference path"
    - "VIRNUCPRO_DISABLE_PACKING env var provides emergency rollback (Gap 2)"
    - "Unpacked fallback path maintained for compatibility (Gap 3)"
    - "Embedding extraction uses cu_seqlens boundaries correctly"
    - "Mean-pooling skips BOS token (position 0) per sequence"
    - "Collator buffer flush handling prevents data loss for remaining sequences"
  artifacts:
    - path: "virnucpro/pipeline/async_inference.py"
      provides: "Working packed inference path"
      min_lines: 440
  key_links:
    - from: "virnucpro/pipeline/async_inference.py"
      to: "model.forward_packed"
      via: "method call in _run_inference"
      pattern: "self\\.model\\.forward_packed"
---

<objective>
Wire packed inference path in AsyncInferenceRunner

Purpose: Replace the NotImplementedError gate in async_inference.py with actual packed inference. When batch contains cu_seqlens, call model.forward_packed() and extract embeddings using cu_seqlens boundaries.

Output: Working packed batch processing in AsyncInferenceRunner
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-sequence-packing-integration/06-RESEARCH.md
@.planning/phases/06-sequence-packing-integration/06-CONTEXT.md
@.planning/phases/06-sequence-packing-integration/06-03-SUMMARY.md
@virnucpro/pipeline/async_inference.py
@virnucpro/models/esm2_flash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace NotImplementedError with forward_packed call</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Modify _run_inference method to use forward_packed for packed batches:

Replace lines 140-149 (the NotImplementedError block):
```python
# Kill switch for emergency rollback (Gap 2)
import os
DISABLE_PACKING = os.getenv('VIRNUCPRO_DISABLE_PACKING', 'false').lower() == 'true'

if 'cu_seqlens' in gpu_batch and not DISABLE_PACKING:
    # PHASE 6: Packed format with FlashAttention varlen
    cu_seqlens = gpu_batch['cu_seqlens']
    max_seqlen = gpu_batch['max_seqlen']

    # Forward pass with packed sequences
    outputs = self.model.forward_packed(
        input_ids=input_ids,
        cu_seqlens=cu_seqlens,
        max_seqlen=max_seqlen,
        repr_layers=[36]
    )
    # Packed output shape: [total_tokens, hidden_dim]
    representations = outputs['representations'][36]

    return representations
else:
    # Unpacked path (fallback):
    # - When VIRNUCPRO_DISABLE_PACKING=true (emergency rollback)
    # - When enable_packing=False in collator (testing)
    # - Before buffer fills (first <buffer_size sequences)
    # - When FlashAttention unavailable
    # Standard forward pass with attention mask
    # ... (existing unpacked implementation)
```

Key differences from original:
1. Added DISABLE_PACKING environment variable check (Gap 2 - production safety)
2. Call model.forward_packed() instead of model()
3. Pass cu_seqlens and max_seqlen
4. Representations are 1D packed format [total_tokens, hidden_dim], not 2D [batch, seq, hidden]
5. Keep unpacked fallback path (Gap 3 clarification - backward compatible)
  </action>
  <verify>
grep -n "forward_packed" virnucpro/pipeline/async_inference.py | head -5
grep -n "NotImplementedError" virnucpro/pipeline/async_inference.py || echo "NotImplementedError removed (good)"
  </verify>
  <done>
_run_inference calls model.forward_packed for packed batches
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix embedding extraction for packed format</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Update _extract_embeddings method to handle packed output format:

Current code (lines 182-203) has a bug - it assumes representations shape is [batch, seq, hidden], but packed format is [total_tokens, hidden_dim].

Fix the packed format branch:
```python
if cu_seqlens is not None and len(sequence_ids) > 0:
    # Packed format: representations shape is [total_tokens, hidden_dim]
    # NOT [batch, seq, hidden]
    embeddings = []
    for i in range(len(sequence_ids)):
        start = cu_seqlens[i].item()
        end = cu_seqlens[i + 1].item()
        # Skip BOS token (position 0 of each sequence), mean pool the rest
        # For sequence at [start:end], BOS is at position 'start'
        if end - start > 1:
            # Mean pool positions start+1 to end (exclude BOS)
            seq_repr = representations[start + 1:end].mean(dim=0)
        else:
            # Single token sequence - use as-is
            seq_repr = representations[start:end].mean(dim=0)
        embeddings.append(seq_repr)

    result = torch.stack(embeddings)
```

Critical fix: Remove `representations[0, ...]` indexing - packed format has no batch dimension!
  </action>
  <verify>
grep -A 20 "_extract_embeddings" virnucpro/pipeline/async_inference.py | grep -E "(start\+1|start \+ 1):end" && echo "BOS skip correct"
  </verify>
  <done>
Embedding extraction handles packed 1D format correctly
  </done>
</task>

<task type="auto">
  <name>Task 3: Add buffer flush handling and logging</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
**Part A: Add flush handling in run() method**

After dataloader exhaustion, flush collator buffer for remaining sequences:

```python
def run(self, dataloader):
    """Run inference on all batches from dataloader."""
    for batch in dataloader:
        yield self._process_batch(batch)

    # Flush collator buffer (handles last <buffer_size sequences)
    # VarlenCollator accumulates sequences; flush ensures no data loss
    if hasattr(dataloader.collate_fn, 'flush'):
        logger.debug("Flushing collator buffer for remaining sequences")
        for batch in dataloader.collate_fn.flush():
            yield self._process_batch(batch)
```

**Part B: Add debug logging for packed batch processing**

In _run_inference after forward_packed call:
```python
logger.debug(
    f"Packed inference: {len(gpu_batch['sequence_ids'])} sequences, "
    f"{input_ids.numel()} tokens, max_seqlen={max_seqlen}"
)
```

This ensures all buffered sequences are processed and provides monitoring.
  </action>
  <verify>
grep -n "flush" virnucpro/pipeline/async_inference.py && echo "Flush handling added"
grep -n "Packed inference" virnucpro/pipeline/async_inference.py && echo "Logging added"
  </verify>
  <done>
Buffer flush handling added to run() method; debug logging for packed batches
  </done>
</task>

</tasks>

<verification>
- [ ] _run_inference calls model.forward_packed for packed batches
- [ ] NotImplementedError removed from packed branch
- [ ] _extract_embeddings handles 1D packed format (no batch dimension)
- [ ] BOS token skipped in mean pooling (start+1:end)
- [ ] Debug logging for packed batch processing
</verification>

<success_criteria>
1. Packed batches processed through forward_packed without NotImplementedError
2. Embeddings extracted correctly from 1D packed output
3. BOS token excluded from mean pooling per sequence
4. Debug logging helps track packed batch processing
</success_criteria>

<output>
After completion, create `.planning/phases/06-sequence-packing-integration/06-04-SUMMARY.md`
</output>
