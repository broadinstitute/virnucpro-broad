---
phase: 06-sequence-packing-integration
plan: 04
type: execute
wave: 3
depends_on: ["06-03"]
files_modified:
  - virnucpro/pipeline/async_inference.py
autonomous: true

must_haves:
  truths:
    - "Packed batches (with cu_seqlens) processed through model.forward_packed"
    - "NotImplementedError replaced with working packed inference path"
    - "Embedding extraction uses cu_seqlens boundaries correctly"
    - "Mean-pooling skips BOS token (position 0) per sequence"
  artifacts:
    - path: "virnucpro/pipeline/async_inference.py"
      provides: "Working packed inference path"
      min_lines: 440
  key_links:
    - from: "virnucpro/pipeline/async_inference.py"
      to: "model.forward_packed"
      via: "method call in _run_inference"
      pattern: "self\\.model\\.forward_packed"
---

<objective>
Wire packed inference path in AsyncInferenceRunner

Purpose: Replace the NotImplementedError gate in async_inference.py with actual packed inference. When batch contains cu_seqlens, call model.forward_packed() and extract embeddings using cu_seqlens boundaries.

Output: Working packed batch processing in AsyncInferenceRunner
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-sequence-packing-integration/06-RESEARCH.md
@.planning/phases/06-sequence-packing-integration/06-CONTEXT.md
@.planning/phases/06-sequence-packing-integration/06-03-SUMMARY.md
@virnucpro/pipeline/async_inference.py
@virnucpro/models/esm2_flash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace NotImplementedError with forward_packed call</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Modify _run_inference method to use forward_packed for packed batches:

Replace lines 140-149 (the NotImplementedError block):
```python
if 'cu_seqlens' in gpu_batch:
    # PHASE 6: Packed format with FlashAttention varlen
    cu_seqlens = gpu_batch['cu_seqlens']
    max_seqlen = gpu_batch['max_seqlen']

    # Forward pass with packed sequences
    outputs = self.model.forward_packed(
        input_ids=input_ids,
        cu_seqlens=cu_seqlens,
        max_seqlen=max_seqlen,
        repr_layers=[36]
    )
    # Packed output shape: [total_tokens, hidden_dim]
    representations = outputs['representations'][36]

    return representations
```

Key differences from original:
1. Call model.forward_packed() instead of model()
2. Pass cu_seqlens and max_seqlen
3. Representations are 1D packed format [total_tokens, hidden_dim], not 2D [batch, seq, hidden]
  </action>
  <verify>
grep -n "forward_packed" virnucpro/pipeline/async_inference.py | head -5
grep -n "NotImplementedError" virnucpro/pipeline/async_inference.py || echo "NotImplementedError removed (good)"
  </verify>
  <done>
_run_inference calls model.forward_packed for packed batches
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix embedding extraction for packed format</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Update _extract_embeddings method to handle packed output format:

Current code (lines 182-203) has a bug - it assumes representations shape is [batch, seq, hidden], but packed format is [total_tokens, hidden_dim].

Fix the packed format branch:
```python
if cu_seqlens is not None and len(sequence_ids) > 0:
    # Packed format: representations shape is [total_tokens, hidden_dim]
    # NOT [batch, seq, hidden]
    embeddings = []
    for i in range(len(sequence_ids)):
        start = cu_seqlens[i].item()
        end = cu_seqlens[i + 1].item()
        # Skip BOS token (position 0 of each sequence), mean pool the rest
        # For sequence at [start:end], BOS is at position 'start'
        if end - start > 1:
            # Mean pool positions start+1 to end (exclude BOS)
            seq_repr = representations[start + 1:end].mean(dim=0)
        else:
            # Single token sequence - use as-is
            seq_repr = representations[start:end].mean(dim=0)
        embeddings.append(seq_repr)

    result = torch.stack(embeddings)
```

Critical fix: Remove `representations[0, ...]` indexing - packed format has no batch dimension!
  </action>
  <verify>
grep -A 20 "_extract_embeddings" virnucpro/pipeline/async_inference.py | grep -E "(start\+1|start \+ 1):end" && echo "BOS skip correct"
  </verify>
  <done>
Embedding extraction handles packed 1D format correctly
  </done>
</task>

<task type="auto">
  <name>Task 3: Add logging for packed batch processing</name>
  <files>virnucpro/pipeline/async_inference.py</files>
  <action>
Add debug logging for packed batch processing in _run_inference:

After the forward_packed call:
```python
logger.debug(
    f"Packed inference: {len(gpu_batch['sequence_ids'])} sequences, "
    f"{input_ids.numel()} tokens, max_seqlen={max_seqlen}"
)
```

Also update process_batch to log packing efficiency if available:
```python
if 'num_sequences' in batch and 'input_ids' in batch:
    tokens = batch['input_ids'].numel()
    seqs = batch['num_sequences']
    logger.debug(f"Batch: {seqs} sequences, {tokens} tokens, efficiency={tokens/(seqs*batch.get('max_seqlen', tokens)):.2%}")
```

This helps with debugging and performance monitoring.
  </action>
  <verify>
grep -n "Packed inference" virnucpro/pipeline/async_inference.py && echo "Logging added"
  </verify>
  <done>
Debug logging for packed batch processing
  </done>
</task>

</tasks>

<verification>
- [ ] _run_inference calls model.forward_packed for packed batches
- [ ] NotImplementedError removed from packed branch
- [ ] _extract_embeddings handles 1D packed format (no batch dimension)
- [ ] BOS token skipped in mean pooling (start+1:end)
- [ ] Debug logging for packed batch processing
</verification>

<success_criteria>
1. Packed batches processed through forward_packed without NotImplementedError
2. Embeddings extracted correctly from 1D packed output
3. BOS token excluded from mean pooling per sequence
4. Debug logging helps track packed batch processing
</success_criteria>

<output>
After completion, create `.planning/phases/06-sequence-packing-integration/06-04-SUMMARY.md`
</output>
