---
phase: 06-sequence-packing-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/models/packed_attention.py
  - virnucpro/models/__init__.py
autonomous: true

must_haves:
  truths:
    - "Position IDs reset to 0 at each cu_seqlens boundary"
    - "Position IDs are contiguous within each sequence (0,1,2,...,len-1)"
    - "flash_attn_varlen_func processes packed sequences without cross-attention"
    - "flash-attn version >= 2.6.0 checked at import with helpful warning (Gap 11)"
  artifacts:
    - path: "virnucpro/models/packed_attention.py"
      provides: "Position ID generation and FlashAttention varlen utilities"
      exports: ["create_position_ids_packed", "flash_attn_varlen_wrapper"]
    - path: "virnucpro/models/__init__.py"
      provides: "Export packed attention utilities"
      contains: "packed_attention"
  key_links:
    - from: "virnucpro/models/packed_attention.py"
      to: "flash_attn"
      via: "flash_attn_varlen_func import"
      pattern: "from flash_attn import flash_attn_varlen_func"
---

<objective>
Create position ID generator and FlashAttention varlen wrapper for packed sequences

Purpose: Position IDs must reset to 0 at each sequence boundary (not sequential across batch). This is critical for correct positional embeddings in packed format. The varlen wrapper provides a clean interface to flash_attn_varlen_func.

Output: virnucpro/models/packed_attention.py with position ID generation and varlen wrapper
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-sequence-packing-integration/06-RESEARCH.md
@.planning/phases/06-sequence-packing-integration/06-CONTEXT.md
@virnucpro/models/esm2_flash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create position ID generator for packed sequences</name>
  <files>virnucpro/models/packed_attention.py</files>
  <action>
Create virnucpro/models/packed_attention.py with position ID utilities:

```python
def create_position_ids_packed(cu_seqlens: torch.Tensor) -> torch.Tensor:
    """
    Create position IDs that reset at each sequence boundary.

    For cu_seqlens = [0, 3, 7, 10]:
        Returns: [0, 1, 2, 0, 1, 2, 3, 0, 1, 2]

    NOT: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] (WRONG - sequential)

    Args:
        cu_seqlens: Cumulative sequence lengths [num_sequences + 1]
                    Must start with 0, be monotonically increasing

    Returns:
        Position IDs tensor [total_tokens] with dtype=torch.long
    """
```

Implementation from RESEARCH.md Pattern 2:
- Get total_len from cu_seqlens[-1]
- Iterate over cu_seqlens boundaries
- Generate torch.arange(seq_len) for each sequence
- Place into position_ids tensor at correct offsets

Include validation:
- Assert cu_seqlens[0] == 0
- Assert cu_seqlens is monotonically increasing
- Assert position_ids[cu_seqlens[i]] == 0 for all i (except last)
  </action>
  <verify>
python -c "
import torch
from virnucpro.models.packed_attention import create_position_ids_packed
cu = torch.tensor([0, 3, 7, 10], dtype=torch.int32)
pos = create_position_ids_packed(cu)
expected = torch.tensor([0, 1, 2, 0, 1, 2, 3, 0, 1, 2])
assert torch.equal(pos, expected), f'Got {pos}, expected {expected}'
print('Position ID generation correct')
"
  </verify>
  <done>
Position IDs reset to 0 at each cu_seqlens boundary
  </done>
</task>

<task type="auto">
  <name>Task 2: Create FlashAttention varlen wrapper</name>
  <files>virnucpro/models/packed_attention.py</files>
  <action>
Add flash_attn_varlen_wrapper function to packed_attention.py:

```python
def flash_attn_varlen_wrapper(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    cu_seqlens: torch.Tensor,
    max_seqlen: int,
    dropout_p: float = 0.0,
    causal: bool = False,
) -> torch.Tensor:
    """
    Wrapper around flash_attn_varlen_func with validation.

    Args:
        q, k, v: Query/Key/Value tensors [total_tokens, num_heads, head_dim]
        cu_seqlens: Cumulative sequence lengths [batch_size + 1], dtype=int32
        max_seqlen: Maximum sequence length in batch
        dropout_p: Dropout probability (0.0 for inference)
        causal: Use causal attention (False for BERT/ESM bidirectional)

    Returns:
        Attention output [total_tokens, num_heads, head_dim]

    Raises:
        ImportError: If flash-attn not installed
        RuntimeError: If CUDA not available or GPU incompatible
    """
```

Implementation:
1. Try import flash_attn_varlen_func from flash_attn
2. **Check flash-attn version >= 2.6.0 (Gap 11):**
   ```python
   try:
       import flash_attn
       from packaging import version

       FLASH_ATTN_AVAILABLE = True

       # Version check for compatibility
       if version.parse(flash_attn.__version__) < version.parse("2.6.0"):
           logger.warning(
               f"flash-attn {flash_attn.__version__} < 2.6.0. "
               "Upgrade for best performance: pip install flash-attn>=2.6.0 --no-build-isolation"
           )
   except ImportError:
       FLASH_ATTN_AVAILABLE = False
       logger.info("flash-attn not available. Install for 2-3x speedup: pip install flash-attn>=2.6.0")
   ```
3. Validate cu_seqlens is int32 (required by flash-attn)
4. Validate q/k/v are FP16 or BF16 (FlashAttention requirement)
5. Call flash_attn_varlen_func with cu_seqlens_q=cu_seqlens_k=cu_seqlens
6. Handle ImportError gracefully with helpful message about installation

Add FLASH_ATTN_AVAILABLE module-level constant for feature detection.
  </action>
  <verify>
python -c "
from virnucpro.models.packed_attention import FLASH_ATTN_AVAILABLE
print(f'FlashAttention available: {FLASH_ATTN_AVAILABLE}')
"
  </verify>
  <done>
FlashAttention varlen wrapper with dtype validation exists
  </done>
</task>

<task type="auto">
  <name>Task 3: Update models module exports</name>
  <files>virnucpro/models/__init__.py</files>
  <action>
Add packed_attention exports to virnucpro/models/__init__.py:

1. Add import: `from virnucpro.models.packed_attention import (create_position_ids_packed, flash_attn_varlen_wrapper, FLASH_ATTN_AVAILABLE)`
2. Add to __all__ list if it exists
3. Keep existing exports (ESM2WithFlashAttention, load_esm2_model, etc.)
  </action>
  <verify>
python -c "from virnucpro.models import create_position_ids_packed, FLASH_ATTN_AVAILABLE; print('Packed attention exports work')"
  </verify>
  <done>
Packed attention utilities exportable from virnucpro.models
  </done>
</task>

</tasks>

<verification>
- [ ] create_position_ids_packed resets to 0 at each boundary
- [ ] Validation: cu_seqlens[0] == 0 assertion
- [ ] Validation: position_ids[cu_seqlens[i]] == 0 assertion
- [ ] flash_attn_varlen_wrapper validates dtype (FP16/BF16)
- [ ] FLASH_ATTN_AVAILABLE constant for feature detection
- [ ] All exports work from virnucpro.models
</verification>

<success_criteria>
1. Position IDs [0,1,2,0,1,2,3,0,1,2] for cu_seqlens [0,3,7,10]
2. FlashAttention wrapper validates inputs before calling kernel
3. Graceful handling when flash-attn not installed
</success_criteria>

<output>
After completion, create `.planning/phases/06-sequence-packing-integration/06-02-SUMMARY.md`
</output>
