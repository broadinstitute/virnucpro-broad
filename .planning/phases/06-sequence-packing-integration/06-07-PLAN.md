---
phase: 06-sequence-packing-integration
plan: 07
type: execute
wave: 5
depends_on: ["06-05", "06-06"]
files_modified:
  - tests/integration/test_packing_pipeline.py
autonomous: false

must_haves:
  truths:
    - "End-to-end pipeline processes packed sequences correctly"
    - "Packing density >90% on test dataset (Gap 8 - measured after buffer warmup)"
    - "Throughput comparison test: packed vs unpacked, target â‰¥2.0x speedup (Gap 9)"
    - "All integration tests pass on GPU server"
  artifacts:
    - path: "tests/integration/test_packing_pipeline.py"
      provides: "End-to-end packing integration tests"
      min_lines: 150
  key_links:
    - from: "tests/integration/test_packing_pipeline.py"
      to: "virnucpro/pipeline/async_inference.py"
      via: "AsyncInferenceRunner usage"
      pattern: "AsyncInferenceRunner"
---

<objective>
End-to-end integration tests and verification checkpoint

Purpose: Verify the complete packing pipeline works on real GPU hardware. Test FASTA -> DataLoader -> packed inference -> embeddings flow. Measure throughput improvement and packing efficiency.

Output: Comprehensive integration tests with human verification checkpoint
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-sequence-packing-integration/06-RESEARCH.md
@.planning/phases/06-sequence-packing-integration/06-CONTEXT.md
@.planning/phases/06-sequence-packing-integration/06-05-SUMMARY.md
@.planning/phases/06-sequence-packing-integration/06-06-SUMMARY.md
@virnucpro/pipeline/async_inference.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end pipeline integration test</name>
  <files>tests/integration/test_packing_pipeline.py</files>
  <action>
Create tests/integration/test_packing_pipeline.py:

```python
"""End-to-end integration tests for sequence packing pipeline.

Tests the complete flow: FASTA -> DataLoader -> packed inference -> embeddings.
Requires GPU and ESM-2 model.
"""
import pytest
import torch
import tempfile
import time
from pathlib import Path

pytestmark = pytest.mark.skipif(
    not torch.cuda.is_available(),
    reason="CUDA required for pipeline tests"
)


@pytest.fixture(scope="module")
def test_fasta_file():
    """Create temporary FASTA file with test sequences."""
    sequences = [
        (f"seq_{i}", "MKTAYIAK" * (5 + i % 20))
        for i in range(100)  # 100 sequences of varying lengths
    ]
    with tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False) as f:
        for seq_id, seq in sequences:
            f.write(f">{seq_id}\n{seq}\n")
        return Path(f.name)


@pytest.fixture(scope="module")
def pipeline_components():
    """Load model and create pipeline components."""
    from virnucpro.models.esm2_flash import load_esm2_model
    from virnucpro.data import SequenceDataset, VarlenCollator, create_async_dataloader

    model, batch_converter = load_esm2_model(
        model_name="esm2_t33_650M_UR50D",
        device="cuda:0"
    )
    return model, batch_converter


class TestPackingPipeline:
    """End-to-end pipeline tests."""

    def test_full_pipeline_flow(self, test_fasta_file, pipeline_components):
        """Complete FASTA -> embeddings flow."""
        model, batch_converter = pipeline_components

        from virnucpro.data import SequenceDataset, VarlenCollator, create_async_dataloader
        from virnucpro.pipeline import AsyncInferenceRunner

        dataset = SequenceDataset(fasta_files=[str(test_fasta_file)])
        collator = VarlenCollator(batch_converter, max_tokens_per_batch=4096)
        dataloader = create_async_dataloader(dataset, collator, num_workers=2)

        runner = AsyncInferenceRunner(model, device=torch.device("cuda:0"))
        results = list(runner.run(dataloader))

        # Verify we got embeddings for all sequences
        total_sequences = sum(len(r.sequence_ids) for r in results)
        assert total_sequences == 100, f"Expected 100, got {total_sequences}"

        # Verify embeddings have correct shape
        for result in results:
            assert result.embeddings.dim() == 2
            assert result.embeddings.shape[1] == 2560  # ESM-2 650M hidden dim

    def test_packing_efficiency(self, test_fasta_file, pipeline_components):
        """Verify packing efficiency >90%."""
        model, batch_converter = pipeline_components

        from virnucpro.data import SequenceDataset, VarlenCollator, create_async_dataloader
        from virnucpro.pipeline import AsyncInferenceRunner

        dataset = SequenceDataset(fasta_files=[str(test_fasta_file)])
        collator = VarlenCollator(batch_converter, max_tokens_per_batch=4096)
        dataloader = create_async_dataloader(dataset, collator, num_workers=2)

        runner = AsyncInferenceRunner(model, device=torch.device("cuda:0"))
        list(runner.run(dataloader))

        stats = runner.get_statistics()
        dl_stats = stats.get('dataloader_stats', {})
        efficiency = dl_stats.get('avg_packing_efficiency', 0)

        assert efficiency > 0.9, f"Packing efficiency {efficiency:.1%} < 90%"

    def test_throughput_baseline(self, test_fasta_file, pipeline_components):
        """Measure throughput for baseline comparison."""
        model, batch_converter = pipeline_components

        from virnucpro.data import SequenceDataset, VarlenCollator, create_async_dataloader
        from virnucpro.pipeline import AsyncInferenceRunner

        dataset = SequenceDataset(fasta_files=[str(test_fasta_file)])
        collator = VarlenCollator(batch_converter, max_tokens_per_batch=4096)
        dataloader = create_async_dataloader(dataset, collator, num_workers=2)

        runner = AsyncInferenceRunner(model, device=torch.device("cuda:0"))

        start_time = time.perf_counter()
        results = list(runner.run(dataloader))
        elapsed = time.perf_counter() - start_time

        total_sequences = sum(len(r.sequence_ids) for r in results)
        seqs_per_sec = total_sequences / elapsed

        print(f"\nThroughput: {seqs_per_sec:.1f} sequences/sec")
        print(f"Total time: {elapsed:.2f}s for {total_sequences} sequences")

        # Basic sanity check - should process at least 10 seq/sec on any GPU
        assert seqs_per_sec > 10, f"Throughput too low: {seqs_per_sec:.1f} seq/s"

    def test_throughput_comparison_packed_vs_unpacked(self, test_fasta_file, pipeline_components):
        """Verify packed throughput is 2-3x faster than unpacked (Gap 9)."""
        model, batch_converter = pipeline_components

        from virnucpro.data import SequenceDataset, VarlenCollator, create_async_dataloader
        from virnucpro.pipeline import AsyncInferenceRunner
        import time

        # Baseline: unpacked (enable_packing=False)
        dataset_unpacked = SequenceDataset(fasta_files=[str(test_fasta_file)])
        collator_unpacked = VarlenCollator(batch_converter, enable_packing=False)
        dataloader_unpacked = create_async_dataloader(dataset_unpacked, collator_unpacked, num_workers=2)
        runner = AsyncInferenceRunner(model, device=torch.device("cuda:0"))

        start = time.perf_counter()
        results_unpacked = list(runner.run(dataloader_unpacked))
        unpacked_time = time.perf_counter() - start
        unpacked_seqs = sum(len(r.sequence_ids) for r in results_unpacked)
        unpacked_throughput = unpacked_seqs / unpacked_time

        # Packed (enable_packing=True, buffer_size=2000)
        dataset_packed = SequenceDataset(fasta_files=[str(test_fasta_file)])
        collator_packed = VarlenCollator(batch_converter, enable_packing=True, buffer_size=2000)
        dataloader_packed = create_async_dataloader(dataset_packed, collator_packed, num_workers=2)

        start = time.perf_counter()
        results_packed = list(runner.run(dataloader_packed))
        packed_time = time.perf_counter() - start
        packed_seqs = sum(len(r.sequence_ids) for r in results_packed)
        packed_throughput = packed_seqs / packed_time

        # Calculate speedup
        speedup = packed_throughput / unpacked_throughput

        print(f"\n--- Throughput Comparison ---")
        print(f"Unpacked: {unpacked_throughput:.1f} seq/s ({unpacked_time:.2f}s total)")
        print(f"Packed:   {packed_throughput:.1f} seq/s ({packed_time:.2f}s total)")
        print(f"Speedup:  {speedup:.1f}x")

        # Verify 2-3x target (Gap 9)
        assert speedup >= 2.0, \
            f"Packed speedup {speedup:.1f}x < 2.0x target. " \
            f"Buffer may not be filling (check warmup batches in logs)"

        # Note: May not reach 3x if test dataset is small (buffer doesn't fill)
        # Gap 8 clarification: Trust buffer design, measure after warmup
        if packed_seqs < 2000:
            print(f"  Note: Dataset has only {packed_seqs} sequences. "
                  "Full speedup requires >2000 seqs to fill buffer.")


class TestOversizedSequences:
    """Test handling of sequences exceeding limits."""

    def test_truncation_warning(self, pipeline_components, caplog):
        """Verify oversized sequences are truncated with warning."""
        model, batch_converter = pipeline_components

        from virnucpro.data import GreedyPacker

        # Create sequence exceeding max length
        packer = GreedyPacker(max_tokens_per_batch=4096, max_sequence_length=100)
        sequences = [
            {'id': 'oversized', 'sequence': 'M' * 200},  # 200 aa > 100 limit
        ]

        batches = packer.pack_sequences(sequences)
        assert batches[0][0].get('truncated', False), "Truncation flag not set"
        assert len(batches[0][0]['sequence']) == 100, "Sequence not truncated"
```
  </action>
  <verify>
pytest tests/integration/test_packing_pipeline.py --collect-only 2>/dev/null | grep "test_" || echo "Tests defined"
  </verify>
  <done>
End-to-end integration tests for packing pipeline
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete sequence packing pipeline with:
1. GreedyPacker (FFD algorithm) in virnucpro/data/packing.py
2. Position ID generation in virnucpro/models/packed_attention.py
3. forward_packed method in ESM2WithFlashAttention
4. Packed inference path in AsyncInferenceRunner
5. Packed vs unpacked equivalence validation
6. Packing efficiency metrics and monitoring
7. End-to-end integration tests
  </what-built>
  <how-to-verify>
1. SSH to GPU server (megatron)
2. Activate pixi environment: `cd ~/virnucpro-nf && pixi shell`
3. Run unit tests (no GPU required):
   ```bash
   cd ~/projects/virnucpro_broad
   pytest tests/unit/test_packing.py -v
   pytest tests/unit/test_esm2_packed.py -v
   ```
4. Run integration tests (requires GPU):
   ```bash
   pytest tests/integration/test_packed_equivalence.py -v
   pytest tests/integration/test_packing_pipeline.py -v
   ```
5. Verify key outputs:
   - Packed embeddings match unpacked (cosine sim >0.999)
   - Packing efficiency >90%
   - Throughput printed in test output
6. If all tests pass, run quick manual verification:
   ```python
   from virnucpro.data.packing import validate_packed_equivalence
   from virnucpro.models.esm2_flash import load_esm2_model
   import torch

   model, bc = load_esm2_model(device="cuda:0")
   seqs = [("test1", "MKTAYIAK"), ("test2", "VLSPADKTNV")]
   passed, details = validate_packed_equivalence(model, bc, seqs, torch.device("cuda:0"))
   print(f"Passed: {passed}, Min similarity: {details['min_similarity']:.6f}")
   ```
  </how-to-verify>
  <resume-signal>
Type "approved" if all tests pass and verification shows packed embeddings match unpacked.
If issues found, describe the failures and we'll create a gap closure plan.
  </resume-signal>
</task>

</tasks>

<verification>
- [ ] End-to-end pipeline test processes 100 sequences
- [ ] Embeddings have correct shape (num_seqs, 2560 for 650M model)
- [ ] Packing efficiency >90% verified
- [ ] Throughput measured and printed
- [ ] Truncation warning verified for oversized sequences
- [ ] Human verification of packed vs unpacked equivalence
</verification>

<success_criteria>
1. All unit tests pass (no GPU required)
2. All integration tests pass on GPU server
3. Packed embeddings match unpacked (cosine sim >0.999)
4. Packing efficiency >90%
5. Throughput measurable for Phase 10 comparison
</success_criteria>

<output>
After completion, create `.planning/phases/06-sequence-packing-integration/06-07-SUMMARY.md`
</output>
