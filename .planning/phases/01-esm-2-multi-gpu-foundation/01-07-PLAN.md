---
phase: 01-esm-2-multi-gpu-foundation
plan: 07
type: execute
wave: 2
depends_on: [01-05, 01-06]
files_modified:
  - virnucpro/pipeline/prediction.py
  - virnucpro/pipeline/work_queue.py
  - virnucpro/pipeline/parallel.py
  - virnucpro/pipeline/parallel_esm.py
  - virnucpro/pipeline/dashboard.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Progress bars show during multi-GPU DNABERT-S processing"
    - "Progress bars show during multi-GPU ESM-2 processing"
    - "GPU utilization is balanced across workers"
  artifacts:
    - path: "virnucpro/pipeline/work_queue.py"
      provides: "Progress queue infrastructure"
      contains: "progress_queue"
    - path: "virnucpro/pipeline/parallel.py"
      provides: "DNABERT worker progress updates"
      contains: "progress_queue.put"
    - path: "virnucpro/pipeline/parallel_esm.py"
      provides: "ESM worker progress updates and balanced distribution"
      contains: "bin_packing"
  key_links:
    - from: "workers"
      to: "progress_queue"
      via: "Queue.put() after each file"
      pattern: "progress_queue\\.put"
    - from: "monitor thread"
      to: "dashboard"
      via: "Queue.get() and update display"
      pattern: "progress_queue\\.get"
---

<objective>
Add progress reporting infrastructure to multi-GPU workers and implement balanced work distribution for even GPU utilization.

Purpose: Users need visibility into GPU progress during processing and balanced utilization across GPUs.
Output: Live progress dashboard showing per-GPU progress with balanced work distribution.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-UAT.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-01-SUMMARY.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add progress queue infrastructure to BatchQueueManager</name>
  <files>virnucpro/pipeline/work_queue.py</files>
  <action>
    Modify BatchQueueManager to support progress reporting:
    1. Import multiprocessing.Queue at top
    2. Add progress_queue parameter to __init__ (optional)
    3. Pass progress_queue to worker via kwargs in process_files()
    4. Add a monitoring thread that consumes from progress_queue if provided
    5. Progress events should be dicts: {'gpu_id': N, 'file': path, 'status': 'complete'|'failed'}

    This provides the infrastructure for workers to report progress back to the main process.

    Gap reason from UAT: "BatchQueueManager lacks progress reporting infrastructure (no callback, no progress queue)"
  </action>
  <verify>grep -n "progress_queue" virnucpro/pipeline/work_queue.py | head -5</verify>
  <done>BatchQueueManager supports progress queue for worker reporting</done>
</task>

<task type="auto">
  <name>Task 2: Update workers to send progress updates</name>
  <files>virnucpro/pipeline/parallel.py, virnucpro/pipeline/parallel_esm.py</files>
  <action>
    In virnucpro/pipeline/parallel.py (process_dnabert_files_worker):
    1. Extract progress_queue from kwargs (default None)
    2. After processing each file successfully:
       - Send: progress_queue.put({'gpu_id': gpu_id, 'file': file_path, 'status': 'complete'})
    3. On file failure:
       - Send: progress_queue.put({'gpu_id': gpu_id, 'file': file_path, 'status': 'failed'})

    In virnucpro/pipeline/parallel_esm.py (process_esm_files_worker):
    1. Same pattern - extract progress_queue from kwargs
    2. Send progress updates after each file (complete/failed)

    Gap reason from UAT: "process_dnabert_files_worker has no progress update mechanism"
  </action>
  <verify>grep -n "progress_queue.put" virnucpro/pipeline/parallel.py virnucpro/pipeline/parallel_esm.py</verify>
  <done>Workers send progress updates via queue</done>
</task>

<task type="auto">
  <name>Task 3: Implement balanced work distribution</name>
  <files>virnucpro/pipeline/parallel_esm.py</files>
  <action>
    Replace simple round-robin with balanced bin-packing in assign_files_round_robin:
    1. For each file, count sequences (parse FASTA quickly to count '>' lines)
    2. Create list of (file_path, sequence_count) tuples
    3. Sort files by sequence count (descending)
    4. Use greedy bin-packing algorithm:
       - Initialize bins (one per GPU) with running totals
       - For each file, assign to GPU with lowest current total
       - This balances work by sequence count, not file count
    5. Return the balanced assignment

    This ensures GPUs get similar amounts of work, preventing idle GPUs.

    Gap reason from UAT: "assign_files_round_robin distributes by count not size"
  </action>
  <verify>python -c "from virnucpro.pipeline.parallel_esm import assign_files_round_robin; print('Bin-packing' in assign_files_round_robin.__doc__ or 'Import successful')"</verify>
  <done>Work distribution uses bin-packing for balanced GPU utilization</done>
</task>

<task type="auto">
  <name>Task 4: Integrate dashboard with progress monitoring</name>
  <files>virnucpro/pipeline/prediction.py, virnucpro/pipeline/dashboard.py</files>
  <action>
    In virnucpro/pipeline/prediction.py:
    1. For both DNABERT and ESM-2 multi-GPU paths:
       - Create multiprocessing.Queue for progress
       - Pass to BatchQueueManager
       - Start dashboard monitoring thread that consumes queue
    2. Dashboard thread updates GPU progress bars based on queue events

    In virnucpro/pipeline/dashboard.py:
    1. Add monitor_progress function that:
       - Takes progress_queue and dashboard instance
       - Continuously reads from queue (with timeout)
       - Updates dashboard GPU progress based on events
       - Handles both TTY (Rich Live) and non-TTY (logging) modes

    Gap reason from UAT: "Missing dashboard integration in ESM-2 and DNABERT multi-GPU paths"
  </action>
  <verify>grep -n "monitor_progress" virnucpro/pipeline/dashboard.py && grep -n "progress_queue" virnucpro/pipeline/prediction.py | head -5</verify>
  <done>Dashboard shows live progress for multi-GPU processing</done>
</task>

</tasks>

<verification>
1. Run multi-GPU processing and verify progress bars appear
2. Check that GPU utilization is balanced (similar progress rates)
3. Verify progress updates work in both TTY and non-TTY environments
4. Test with files of varying sizes to confirm bin-packing works
</verification>

<success_criteria>
- Progress bars visible during DNABERT-S multi-GPU processing
- Progress bars visible during ESM-2 multi-GPU processing
- GPU utilization balanced (progress bars move at similar rates)
- Works in both interactive terminals and CI/CD environments
</success_criteria>

<output>
After completion, create `.planning/phases/01-esm-2-multi-gpu-foundation/01-07-SUMMARY.md`
</output>