---
phase: 01-esm-2-multi-gpu-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/parallel_esm.py
  - virnucpro/pipeline/work_queue.py
autonomous: true

must_haves:
  truths:
    - "ESM-2 processes files in parallel across multiple GPUs"
    - "Files are assigned based on sequence count for load balancing"
    - "Worker failures are tracked and reported"
  artifacts:
    - path: "virnucpro/pipeline/parallel_esm.py"
      provides: "ESM-2 parallel worker functions"
      exports: ["process_esm_files_worker", "assign_files_by_size"]
    - path: "virnucpro/pipeline/work_queue.py"
      provides: "Generic batch queue manager"
      exports: ["BatchQueueManager", "WorkerStatus"]
  key_links:
    - from: "parallel_esm.py"
      to: "esm model loading"
      via: "deferred CUDA initialization in worker"
      pattern: "torch.device.*cuda.*device_id"
    - from: "work_queue.py"
      to: "multiprocessing.spawn"
      via: "spawn context for CUDA safety"
      pattern: "multiprocessing.get_context.*spawn"
---

<objective>
Create the foundation for ESM-2 multi-GPU parallelization with size-aware work distribution and failure tracking.

Purpose: Establish the core infrastructure for parallelizing ESM-2 across multiple GPUs, implementing the batch queue manager pattern that will coordinate work distribution.
Output: ESM-2 worker functions and queue manager that can process files in parallel across GPUs.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-CONTEXT.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-RESEARCH.md
@virnucpro/pipeline/parallel.py
@virnucpro/pipeline/features.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ESM-2 parallel worker functions</name>
  <files>virnucpro/pipeline/parallel_esm.py</files>
  <action>
    Create ESM-2 specific parallel processing module with:

    1. Import required libraries (torch, multiprocessing, esm, pathlib, Bio.SeqIO, logging)
    2. Implement `count_sequences(file_path)` helper to count sequences in FASTA file
    3. Implement `assign_files_by_size(files, num_workers)` that:
       - Counts sequences per file using count_sequences
       - Sorts files by size (descending)
       - Uses greedy bin packing to minimize max worker load
       - Returns list of file lists (one per worker)

    4. Implement `process_esm_files_worker(file_subset, device_id, toks_per_batch, output_dir, failed_files_queue)`:
       - Deferred CUDA initialization: device = torch.device(f'cuda:{device_id}')
       - Load ESM-2 model ONLY in worker (esm.pretrained.esm2_t36_3B_UR50D())
       - Move model to device and set eval mode
       - For each file in file_subset:
         - Try processing with extract_esm_features (from features.py)
         - On success: append to output_files list
         - On OOM or other exception:
           - Put (file_path, str(exception)) in failed_files_queue
           - Clear CUDA cache and continue
       - Return list of successfully processed output files

    5. Add comprehensive logging at INFO level for file assignment and processing

    Use spawn context pattern from research to avoid CUDA re-initialization errors.
    Follow the existing parallel.py patterns for consistency.
  </action>
  <verify>python -c "from virnucpro.pipeline import parallel_esm; print(parallel_esm.assign_files_by_size.__doc__)"</verify>
  <done>Module imports successfully and exports required functions</done>
</task>

<task type="auto">
  <name>Task 2: Create batch queue manager</name>
  <files>virnucpro/pipeline/work_queue.py</files>
  <action>
    Create generic batch queue manager for coordinating multi-GPU work:

    1. Import multiprocessing, queue, enum, logging, pathlib, time

    2. Create WorkerStatus enum with states: IDLE, PROCESSING, COMPLETED, FAILED

    3. Implement BatchQueueManager class:
       - __init__(self, num_workers, worker_function, spawn_context=True)
       - Properties:
         - self.num_workers = num_workers
         - self.worker_function = worker_function
         - self.ctx = multiprocessing.get_context('spawn') if spawn_context else multiprocessing
         - self.result_queue = self.ctx.SimpleQueue()  # Avoid Queue deadlocks
         - self.failed_files_queue = self.ctx.SimpleQueue()
         - self.worker_status = {i: WorkerStatus.IDLE for i in range(num_workers)}

       - process_files(self, file_assignments, **worker_kwargs):
         - Create worker pool using self.ctx.Pool(self.num_workers)
         - Use pool.starmap_async to launch workers with file assignments
         - Monitor result_queue for completion updates
         - Track failed files from failed_files_queue
         - Handle systemic failure detection (3+ consecutive failures = abort)
         - Return (processed_files, failed_files) tuple

       - get_worker_status(self): Return current status dict
       - is_complete(self): Check if all workers completed or failed

    4. Add proper exception handling and logging throughout

    Use SimpleQueue instead of Queue to avoid threading deadlocks (from research).
    Follow multiprocessing best practices with spawn context.
  </action>
  <verify>python -c "from virnucpro.pipeline.work_queue import BatchQueueManager, WorkerStatus; print('Queue manager available')"</verify>
  <done>BatchQueueManager class is importable with WorkerStatus enum</done>
</task>

</tasks>

<verification>
- ESM-2 worker function uses deferred CUDA initialization
- Files are assigned based on sequence count, not just file count
- Failed files are tracked and returned for logging
- Spawn context is used throughout to avoid CUDA errors
</verification>

<success_criteria>
- parallel_esm.py module exists with assign_files_by_size and process_esm_files_worker functions
- work_queue.py module exists with BatchQueueManager class
- Both modules follow existing codebase patterns from parallel.py
- Proper error handling for OOM and worker failures
</success_criteria>

<output>
After completion, create `.planning/phases/01-esm-2-multi-gpu-foundation/01-01-SUMMARY.md`
</output>