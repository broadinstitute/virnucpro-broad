---
phase: 01-esm-2-multi-gpu-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/parallel_esm.py
  - virnucpro/pipeline/work_queue.py
  - virnucpro/pipeline/features.py
autonomous: true

must_haves:
  truths:
    - "ESM-2 processes files in parallel across multiple GPUs"
    - "Files are distributed round-robin across GPU workers"
    - "Worker failures are tracked and reported"
  artifacts:
    - path: "virnucpro/pipeline/parallel_esm.py"
      provides: "ESM-2 parallel worker functions"
      exports: ["process_esm_files_worker", "assign_files_round_robin"]
    - path: "virnucpro/pipeline/work_queue.py"
      provides: "Generic batch queue manager"
      exports: ["BatchQueueManager", "WorkerStatus"]
    - path: "virnucpro/pipeline/features.py"
      provides: "BF16 optimized ESM-2 extraction"
      contains: "autocast.*bfloat16"
  key_links:
    - from: "parallel_esm.py"
      to: "features.extract_esm_features"
      via: "import and call with BF16 support"
      pattern: "from virnucpro.pipeline.features import extract_esm_features"
    - from: "parallel_esm.py"
      to: "esm model loading"
      via: "deferred CUDA initialization in worker"
      pattern: "torch.device.*cuda.*device_id"
    - from: "work_queue.py"
      to: "multiprocessing.spawn"
      via: "spawn context for CUDA safety"
      pattern: "multiprocessing.get_context.*spawn"
---

<objective>
Create the foundation for ESM-2 multi-GPU parallelization with round-robin work distribution, BF16 optimization, and failure tracking.

Purpose: Establish the core infrastructure for parallelizing ESM-2 across multiple GPUs, implementing the batch queue manager pattern that will coordinate work distribution with BF16 mixed precision.
Output: ESM-2 worker functions with BF16 support, queue manager that can process files in parallel across GPUs.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-CONTEXT.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-RESEARCH.md
@virnucpro/pipeline/parallel.py
@virnucpro/pipeline/features.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add BF16 optimization to ESM-2 extraction</name>
  <files>virnucpro/pipeline/features.py</files>
  <action>
    Modify extract_esm_features function to use BF16 mixed precision:

    1. Add import at top: from torch.cuda.amp import autocast

    2. In extract_esm_features function, after model loading:
       - Add BF16 support check:
         ```python
         # Check for BF16 support
         use_bf16 = False
         if str(device).startswith('cuda'):
             capability = torch.cuda.get_device_capability(device)
             use_bf16 = capability[0] >= 8  # Ampere or newer
             if use_bf16:
                 logger.info("Using BF16 mixed precision for memory efficiency")
         ```

    3. Wrap the main processing loop with autocast and torch.no_grad:
       - Replace existing "with torch.no_grad():" with:
         ```python
         with torch.no_grad():
             with autocast(device_type='cuda', dtype=torch.bfloat16, enabled=use_bf16):
                 for batch_idx, batch in enumerate(batches):
                     # existing batch processing code
         ```

    4. After autocast block, ensure embeddings are converted back to FP32:
       - Change line storing embedding to:
         ```python
         embedding = representations[i, 1:truncate_len + 1].mean(dim=0).clone().float().to('cpu')
         ```

    5. Optionally increase default toks_per_batch to 3072 (from 2048) when BF16 enabled:
       - Add after BF16 check:
         ```python
         if use_bf16 and toks_per_batch == 2048:
             toks_per_batch = 3072  # Increase batch size with BF16
             logger.info(f"Increased toks_per_batch to {toks_per_batch} with BF16")
         ```

    Ensure BF16 only used on compatible GPUs (Ampere+).
    Convert back to FP32 for storage to maintain compatibility.
  </action>
  <verify>grep -n "autocast.*bfloat16" virnucpro/pipeline/features.py</verify>
  <done>ESM-2 feature extraction uses BF16 autocast with torch.no_grad when available</done>
</task>

<task type="auto">
  <name>Task 2: Create ESM-2 parallel worker functions</name>
  <files>virnucpro/pipeline/parallel_esm.py</files>
  <action>
    Create ESM-2 specific parallel processing module with:

    1. Import required libraries (torch, multiprocessing, esm, pathlib, Bio.SeqIO, logging)

    2. Add explicit import of extract_esm_features at module level:
       ```python
       from virnucpro.pipeline.features import extract_esm_features
       ```

    3. Implement `count_sequences(file_path)` helper to count sequences in FASTA file

    4. Implement `assign_files_round_robin(files, num_workers)` that:
       - Distributes files round-robin across workers
       - Returns list of file lists (one per worker)
       - Example: files [a,b,c,d,e] with 2 workers -> [[a,c,e], [b,d]]

    5. Implement `process_esm_files_worker(file_subset, device_id, toks_per_batch, output_dir, failed_files_queue)`:
       - Function signature MUST be: (file_subset, device_id, **kwargs) -> (processed_files, failed_files)
       - Deferred CUDA initialization: device = torch.device(f'cuda:{device_id}')
       - Load ESM-2 model ONLY in worker (esm.pretrained.esm2_t36_3B_UR50D())
       - Move model to device and set eval mode

       - Wrap inference with torch.no_grad() context:
         ```python
         with torch.no_grad():
             for file in file_subset:
                 # Process file
         ```

       - For each file in file_subset:
         - Try processing:
           ```python
           output_file = output_dir / f"{file.stem}_ESM.pt"
           extract_esm_features(file, output_file, device, toks_per_batch=toks_per_batch)
           ```
         - On success: append to processed_files list
         - On OOM or other exception:
           - Append (file_path, str(exception)) to failed_files list
           - Clear CUDA cache and continue
       - Return (processed_files, failed_files) tuple

    6. Add comprehensive logging at INFO level for file assignment and processing

    Use spawn context pattern from research to avoid CUDA re-initialization errors.
    Follow the existing parallel.py patterns for consistency.
  </action>
  <verify>python -c "from virnucpro.pipeline import parallel_esm; print(parallel_esm.assign_files_round_robin.__doc__)"</verify>
  <done>Module imports successfully and exports required functions with correct signatures</done>
</task>

<task type="auto">
  <name>Task 3: Create batch queue manager</name>
  <files>virnucpro/pipeline/work_queue.py</files>
  <action>
    Create generic batch queue manager for coordinating multi-GPU work:

    1. Import multiprocessing, queue, enum, logging, pathlib, time

    2. Create WorkerStatus enum with states: IDLE, PROCESSING, COMPLETED, FAILED

    3. Implement BatchQueueManager class:
       - __init__(self, num_workers, worker_function, spawn_context=True)
       - Validate worker_function signature on init:
         ```python
         # Validate worker function signature
         import inspect
         sig = inspect.signature(worker_function)
         params = list(sig.parameters.keys())
         if len(params) < 2:
             raise ValueError(f"Worker function must accept (file_subset, device_id, **kwargs)")
         ```
       - Properties:
         - self.num_workers = num_workers
         - self.worker_function = worker_function
         - self.ctx = multiprocessing.get_context('spawn') if spawn_context else multiprocessing
         - self.result_queue = self.ctx.SimpleQueue()  # Avoid Queue deadlocks
         - self.failed_files_queue = self.ctx.SimpleQueue()
         - self.worker_status = {i: WorkerStatus.IDLE for i in range(num_workers)}

       - process_files(self, file_assignments, **worker_kwargs):
         - Ensure worker_function returns (processed_files, failed_files) tuple
         - Create worker pool using self.ctx.Pool(self.num_workers)
         - Use pool.starmap_async to launch workers with file assignments
         - Monitor result_queue for completion updates
         - Track failed files from failed_files_queue
         - Handle systemic failure detection (3+ consecutive failures = abort)
         - Return (processed_files, failed_files) tuple

       - get_worker_status(self): Return current status dict
       - is_complete(self): Check if all workers completed or failed

    4. Add proper exception handling and logging throughout

    Use SimpleQueue instead of Queue to avoid threading deadlocks (from research).
    Follow multiprocessing best practices with spawn context.
  </action>
  <verify>python -c "from virnucpro.pipeline.work_queue import BatchQueueManager, WorkerStatus; print('Queue manager available')"</verify>
  <done>BatchQueueManager class is importable with WorkerStatus enum and validates worker function signature</done>
</task>

</tasks>

<verification>
- ESM-2 worker function uses deferred CUDA initialization with torch.no_grad context
- Files are distributed round-robin across workers
- Failed files are tracked and returned for logging
- Spawn context is used throughout to avoid CUDA errors
- BF16 mixed precision integrated directly in features.py
</verification>

<success_criteria>
- features.py modified with BF16 autocast and torch.no_grad
- parallel_esm.py module exists with assign_files_round_robin and process_esm_files_worker functions
- work_queue.py module exists with BatchQueueManager class that validates worker signatures
- All modules follow existing codebase patterns
- Proper error handling for OOM and worker failures
</success_criteria>

<output>
After completion, create `.planning/phases/01-esm-2-multi-gpu-foundation/01-01-SUMMARY.md`
</output>