---
phase: 01-esm-2-multi-gpu-foundation
plan: 04
type: execute
wave: 4
depends_on: ["01-03"]
files_modified:
  - tests/test_integration_multi_gpu.py
  - docs/GPU_OPTIMIZATION.md
autonomous: false

must_haves:
  truths:
    - "Full pipeline processes test data successfully"
    - "Multi-GPU output matches single-GPU baseline"
    - "Performance metrics show 3-4x speedup"
  artifacts:
    - path: "tests/test_integration_multi_gpu.py"
      provides: "End-to-end integration test"
      min_lines: 60
    - path: "docs/GPU_OPTIMIZATION.md"
      provides: "User documentation"
      min_lines: 80
  key_links:
    - from: "test_integration_multi_gpu.py"
      to: "virnucpro predict"
      via: "subprocess call to CLI"
      pattern: "subprocess.run.*virnucpro.*predict"
---

<objective>
Create end-to-end integration test and documentation, then verify the complete multi-GPU implementation.

Purpose: Ensure the full pipeline works correctly with multiple GPUs, produces identical results to single-GPU, and achieves performance targets.
Output: Integration test comparing multi vs single GPU outputs, documentation for users, and human verification checkpoint.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end integration test</name>
  <files>tests/test_integration_multi_gpu.py</files>
  <action>
    Create integration test that verifies multi-GPU output matches single-GPU baseline:

    1. Import unittest, subprocess, pathlib, tempfile, torch, time, shutil

    2. Create TestMultiGPUIntegration class:
       - setUpClass():
         - Check if multiple GPUs available, skip if not
         - Create temp directory for outputs
         - Copy small test dataset (or create synthetic one)

       - test_single_vs_multi_gpu_equivalence():
         ```python
         # Run with single GPU
         start_single = time.time()
         result_single = subprocess.run([
             'python', '-m', 'virnucpro', 'predict',
             '--input', str(self.test_input),
             '--output-dir', str(self.output_single),
             '--gpus', '0'
         ], capture_output=True, text=True, check=False)
         time_single = time.time() - start_single

         # Run with multiple GPUs
         start_multi = time.time()
         result_multi = subprocess.run([
             'python', '-m', 'virnucpro', 'predict',
             '--input', str(self.test_input),
             '--output-dir', str(self.output_multi),
             '--gpus', '0,1,2,3'  # Or detect available
         ], capture_output=True, text=True, check=False)
         time_multi = time.time() - start_multi

         # Compare outputs
         single_features = torch.load(self.output_single / 'merged_features.pt')
         multi_features = torch.load(self.output_multi / 'merged_features.pt')

         # Check equivalence (within floating point tolerance)
         torch.testing.assert_close(
             single_features['data'],
             multi_features['data'],
             rtol=1e-5,
             atol=1e-5
         )

         # Log speedup
         speedup = time_single / time_multi
         print(f"Speedup: {speedup:.2f}x (single: {time_single:.1f}s, multi: {time_multi:.1f}s)")
         self.assertGreater(speedup, 2.0, "Should see at least 2x speedup with 4 GPUs")
         ```

       - test_failed_file_recovery():
         - Create dataset with problematic sequence (very long)
         - Verify failed_files.txt created
         - Verify partial success exit code (2)

       - test_checkpoint_compatibility():
         - Run partial pipeline with old checkpoint format
         - Resume with new multi-GPU code
         - Verify successful completion

       - tearDownClass(): Clean up temp directories

    3. Add helper to create synthetic test data if needed

    Use subprocess with check=False to handle non-zero exit codes gracefully.
    Compare numerical outputs within tolerance for BF16 differences.
  </action>
  <verify>python -m pytest tests/test_integration_multi_gpu.py::TestMultiGPUIntegration::test_single_vs_multi_gpu_equivalence -v -s</verify>
  <done>Integration test verifies multi-GPU matches single-GPU output</done>
</task>

<task type="auto">
  <name>Task 2: Create user documentation</name>
  <files>docs/GPU_OPTIMIZATION.md</files>
  <action>
    Create comprehensive documentation for the GPU optimization features:

    ```markdown
    # GPU Optimization Guide

    ## Overview
    VirNucPro now supports automatic multi-GPU parallelization for ESM-2 protein feature extraction, reducing processing time from 45+ hours to under 10 hours.

    ## Features
    - **Automatic GPU detection**: Uses all available GPUs by default
    - **BF16 mixed precision**: 2x memory reduction on compatible GPUs (RTX 30xx, A100, etc.)
    - **Load balancing**: Files distributed round-robin across GPUs
    - **Progress monitoring**: Live dashboard shows per-GPU status
    - **Failure recovery**: Failed files logged, partial results preserved

    ## Usage

    ### Basic Usage (Auto-detect GPUs)
    ```bash
    virnucpro predict --input sequences.fa --output-dir results/
    ```

    ### Specify GPUs
    ```bash
    # Use specific GPUs
    virnucpro predict --input sequences.fa --output-dir results/ --gpus 0,1,2

    # Use single GPU (fallback mode)
    virnucpro predict --input sequences.fa --output-dir results/ --gpus 0
    ```

    ### Adjust Batch Size (for OOM issues)
    ```bash
    # Reduce batch size if encountering out-of-memory errors
    virnucpro predict --input sequences.fa --output-dir results/ --batch-size 1024
    ```

    ### Quiet Mode
    ```bash
    # Disable progress dashboard
    virnucpro predict --input sequences.fa --output-dir results/ --quiet
    ```

    ## Performance

    Expected speedup with multiple GPUs:
    - 2 GPUs: ~1.8-2x faster
    - 4 GPUs: ~3.5-4x faster
    - 8 GPUs: ~7-8x faster

    Actual speedup depends on:
    - File sizes and sequence lengths
    - GPU memory and compute capability
    - I/O bandwidth

    ## Hardware Requirements

    ### Minimum
    - Single NVIDIA GPU with 8GB+ VRAM
    - CUDA 11.0+
    - PyTorch 2.0+

    ### Recommended
    - 4+ NVIDIA GPUs with 16GB+ VRAM each
    - Ampere architecture or newer (RTX 30xx, A100) for BF16
    - NVMe SSD for faster I/O

    ## Troubleshooting

    ### Out of Memory Errors
    1. Reduce batch size: `--batch-size 512`
    2. Use fewer GPUs to increase memory per GPU
    3. Check for other processes using GPU memory

    ### CUDA Initialization Errors
    - Error: "Cannot re-initialize CUDA in forked subprocess"
    - Solution: Ensure you're using Python 3.9+ (uses spawn by default)

    ### Uneven GPU Usage
    - Files are distributed round-robin across GPUs
    - Very large files may cause imbalance
    - Consider splitting extremely large files

    ### Failed Files
    - Check `results/failed_files.txt` for list of failed files
    - Common causes: Corrupted sequences, extreme length sequences
    - Rerun with just failed files if needed

    ## Backward Compatibility

    - Existing checkpoints from single-GPU runs can be resumed
    - Output format unchanged - downstream analysis unaffected
    - CLI interface maintains backward compatibility

    ## Monitoring

    The progress dashboard shows:
    ```
    GPU 0: ████████████████████ 100% [250/250 files]
    GPU 1: ████████████████░░░░  85% [212/250 files]
    GPU 2: ██████████████████░░  90% [225/250 files]
    GPU 3: ████████████████████ 100% [250/250 files]
    ```

    GPU memory usage logged every 10 seconds in verbose mode.
    ```

    Include examples, performance expectations, and troubleshooting.
    Follow existing documentation style in the project.
  </action>
  <verify>wc -l docs/GPU_OPTIMIZATION.md</verify>
  <done>Documentation created with usage examples and troubleshooting</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete ESM-2 multi-GPU parallelization with BF16 optimization</what-built>
  <how-to-verify>
    1. Run the integration test to verify functionality:
       ```bash
       python -m pytest tests/test_integration_multi_gpu.py -v -s
       ```

    2. Test with actual data (if available):
       ```bash
       # Single GPU baseline
       time virnucpro predict --input test_data.fa --output-dir single_gpu_output/ --gpus 0

       # Multi-GPU (adjust based on available GPUs)
       time virnucpro predict --input test_data.fa --output-dir multi_gpu_output/ --gpus 0,1,2,3

       # Compare outputs
       python -c "import torch; s=torch.load('single_gpu_output/merged_features.pt'); m=torch.load('multi_gpu_output/merged_features.pt'); print('Match:', torch.allclose(s['data'], m['data'], atol=1e-5))"
       ```

    3. Check progress dashboard displays correctly (if multi-GPU available)

    4. Verify documentation is clear and complete:
       ```bash
       cat docs/GPU_OPTIMIZATION.md
       ```

    Expected results:
    - Multi-GPU should be 3-4x faster with 4 GPUs
    - Outputs should match within floating point tolerance
    - Dashboard should show per-GPU progress
    - No CUDA initialization errors
  </how-to-verify>
  <resume-signal>Type "approved" if tests pass and speedup achieved, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
- Integration test confirms multi-GPU matches single-GPU output
- Performance shows 3-4x speedup with 4 GPUs
- Documentation provides clear usage instructions
- Human verifies complete system works as expected
</verification>

<success_criteria>
- End-to-end test passes with output equivalence
- Measured speedup meets targets (3-4x with 4 GPUs)
- Documentation complete with examples and troubleshooting
- Human verification confirms functionality
- Subprocess calls use check=False for proper error handling
</success_criteria>

<output>
After completion, create `.planning/phases/01-esm-2-multi-gpu-foundation/01-04-SUMMARY.md`
</output>