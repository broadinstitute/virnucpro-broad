---
phase: 01-esm-2-multi-gpu-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - virnucpro/pipeline/gpu_monitor.py
  - virnucpro/pipeline/dashboard.py
autonomous: true

must_haves:
  truths:
    - "GPU memory usage is visible to users during processing"
    - "Progress tracking shows completion status for each GPU"
    - "Memory pressure triggers adaptive batch size adjustment"
  artifacts:
    - path: "virnucpro/pipeline/gpu_monitor.py"
      provides: "GPU memory and utilization tracking"
      exports: ["GPUMonitor", "get_gpu_memory_info", "check_bf16_support"]
    - path: "virnucpro/pipeline/dashboard.py"
      provides: "Rich-based progress dashboard"
      exports: ["MultiGPUDashboard", "create_progress_display"]
  key_links:
    - from: "gpu_monitor.py"
      to: "torch.cuda.mem_get_info"
      via: "Direct CUDA API for memory queries"
      pattern: "torch.cuda.mem_get_info"
    - from: "dashboard.py"
      to: "rich.progress"
      via: "Progress bars for each GPU"
      pattern: "from rich.progress import"
---

<objective>
Create GPU monitoring utilities and live progress dashboard for multi-GPU visibility.

Purpose: Provide real-time monitoring of GPU memory usage and processing progress to help users understand system behavior and diagnose issues.
Output: GPU monitoring utilities and rich dashboard for real-time progress tracking.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-CONTEXT.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPU monitoring utilities</name>
  <files>virnucpro/pipeline/gpu_monitor.py</files>
  <action>
    Create GPU memory monitoring module for adaptive batching and pre-flight validation:

    1. Import torch, psutil, logging, typing

    2. Implement `check_bf16_support()`:
       - Check if CUDA available
       - Get device capability using torch.cuda.get_device_capability()
       - Return True if capability >= (8, 0) (Ampere or newer), else False
       - Log warning if BF16 not supported

    3. Implement `get_gpu_memory_info(device_id=0)`:
       - Use torch.cuda.mem_get_info(device_id) for direct API access
       - Return dict with:
         - 'free': free memory in bytes
         - 'total': total memory in bytes
         - 'used': total - free
         - 'percent': (used/total) * 100

    4. Implement GPUMonitor class:
       - __init__(self, device_ids, log_interval=10.0)
       - start_monitoring(self): Begin periodic memory logging
       - stop_monitoring(self): Stop monitoring thread
       - get_current_stats(self): Return dict of current stats for all GPUs
       - detect_memory_pressure(self, threshold=0.9): Return GPUs over threshold
       - log_stats(self): Log current memory usage for all GPUs

    5. Implement `find_optimal_batch_size(model, sample_input, device, initial_batch=1, max_batch=512)`:
       - Binary search for optimal batch size
       - Try progressively larger batches until OOM
       - Clear cache between attempts
       - Return last working batch size

    Use torch.cuda APIs directly instead of parsing nvidia-smi (from research).
    Add proper exception handling for CUDA operations.
  </action>
  <verify>python -c "from virnucpro.pipeline.gpu_monitor import GPUMonitor, check_bf16_support; print(f'BF16 support: {check_bf16_support()}')"</verify>
  <done>GPU monitoring module imports and correctly detects BF16 support</done>
</task>

<task type="auto">
  <name>Task 2: Create multi-GPU progress dashboard</name>
  <files>virnucpro/pipeline/dashboard.py</files>
  <action>
    Create rich-based dashboard for live multi-GPU progress tracking:

    1. Import rich.progress, rich.table, rich.live, rich.console, pathlib, typing, datetime

    2. Implement MultiGPUDashboard class:
       - __init__(self, num_gpus, total_files_per_gpu)
       - Properties:
         - self.console = Console()
         - self.progress = Progress with BarColumn, TextColumn, TimeElapsedColumn
         - self.gpu_tasks = {} for tracking task IDs
         - self.start_time = None

       - start(self):
         - Initialize progress tasks for each GPU
         - Add task with description f"[cyan]GPU {gpu_id}"
         - Set total to files count for that GPU
         - Start Live display with self.progress

       - update(self, gpu_id, files_completed=1):
         - Advance progress for specific GPU
         - Update completion percentage

       - set_status(self, gpu_id, status_text):
         - Update task description with status
         - E.g., "[cyan]GPU 0 - Processing file_123.fa"

       - complete(self, gpu_id):
         - Mark GPU task as complete
         - Show elapsed time

       - complete_all(self):
         - Mark all GPU tasks as complete
         - Stop Live display gracefully

       - get_summary(self):
         - Return dict with total files, completed, elapsed time
         - Calculate overall throughput (files/sec)

    3. Implement `create_progress_display(file_assignments)`:
       - Helper to create dashboard from file assignments
       - Calculate total files per GPU
       - Return configured MultiGPUDashboard instance

    4. Add fallback handling for non-TTY environments:
       - Check if console is interactive
       - Fall back to simple logging if not TTY
       - Avoid rich display errors in non-interactive contexts

    Follow rich documentation patterns for concurrent progress bars (from research).
    Use Live display for smooth updates without flicker.
  </action>
  <verify>python -c "from virnucpro.pipeline.dashboard import MultiGPUDashboard; d = MultiGPUDashboard(4, {0:10, 1:10, 2:10, 3:10}); print('Dashboard created')"</verify>
  <done>Dashboard module imports and creates multi-GPU display instance</done>
</task>

</tasks>

<verification>
- GPU memory is monitored via direct CUDA APIs
- Rich dashboard displays concurrent progress for all GPUs
- Dashboard has fallback for non-TTY environments
- Memory monitoring supports adaptive batch sizing
</verification>

<success_criteria>
- gpu_monitor.py provides GPU memory tracking and BF16 detection
- dashboard.py creates live multi-GPU progress display with fallback handling
- Both modules follow existing codebase patterns
</success_criteria>

<output>
After completion, create `.planning/phases/01-esm-2-multi-gpu-foundation/01-02-SUMMARY.md`
</output>