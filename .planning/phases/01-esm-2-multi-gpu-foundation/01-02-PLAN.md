---
phase: 01-esm-2-multi-gpu-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/gpu_monitor.py
  - virnucpro/pipeline/dashboard.py
  - virnucpro/pipeline/features.py
autonomous: true

must_haves:
  truths:
    - "BF16 mixed precision reduces memory usage by 2x"
    - "Live dashboard shows per-GPU progress"
    - "GPU memory is monitored for adaptive batching"
  artifacts:
    - path: "virnucpro/pipeline/gpu_monitor.py"
      provides: "GPU memory and utilization tracking"
      exports: ["GPUMonitor", "get_gpu_memory_info", "check_bf16_support"]
    - path: "virnucpro/pipeline/dashboard.py"
      provides: "Rich-based progress dashboard"
      exports: ["MultiGPUDashboard", "create_progress_display"]
    - path: "virnucpro/pipeline/features.py"
      provides: "BF16 optimized ESM-2 extraction"
      contains: "autocast.*bfloat16"
  key_links:
    - from: "gpu_monitor.py"
      to: "torch.cuda.mem_get_info"
      via: "Direct CUDA API for memory queries"
      pattern: "torch.cuda.mem_get_info"
    - from: "features.py"
      to: "torch.cuda.amp.autocast"
      via: "BF16 automatic mixed precision"
      pattern: "autocast.*dtype=torch.bfloat16"
---

<objective>
Add BF16 mixed precision optimization for memory efficiency and create live progress dashboard for GPU visibility.

Purpose: Reduce ESM-2 memory footprint by 2x using BF16 and provide real-time monitoring of multi-GPU processing.
Output: GPU monitoring utilities, rich dashboard, and BF16-optimized feature extraction.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-CONTEXT.md
@.planning/phases/01-esm-2-multi-gpu-foundation/01-RESEARCH.md
@virnucpro/pipeline/features.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPU monitoring utilities</name>
  <files>virnucpro/pipeline/gpu_monitor.py</files>
  <action>
    Create GPU memory monitoring module for adaptive batching and pre-flight validation:

    1. Import torch, psutil, logging, typing

    2. Implement `check_bf16_support()`:
       - Check if CUDA available
       - Get device capability using torch.cuda.get_device_capability()
       - Return True if capability >= (8, 0) (Ampere or newer), else False
       - Log warning if BF16 not supported

    3. Implement `get_gpu_memory_info(device_id=0)`:
       - Use torch.cuda.mem_get_info(device_id) for direct API access
       - Return dict with:
         - 'free': free memory in bytes
         - 'total': total memory in bytes
         - 'used': total - free
         - 'percent': (used/total) * 100

    4. Implement GPUMonitor class:
       - __init__(self, device_ids, log_interval=10.0)
       - start_monitoring(self): Begin periodic memory logging
       - stop_monitoring(self): Stop monitoring thread
       - get_current_stats(self): Return dict of current stats for all GPUs
       - detect_memory_pressure(self, threshold=0.9): Return GPUs over threshold
       - log_stats(self): Log current memory usage for all GPUs

    5. Implement `find_optimal_batch_size(model, sample_input, device, initial_batch=1, max_batch=512)`:
       - Binary search for optimal batch size
       - Try progressively larger batches until OOM
       - Clear cache between attempts
       - Return last working batch size

    Use torch.cuda APIs directly instead of parsing nvidia-smi (from research).
    Add proper exception handling for CUDA operations.
  </action>
  <verify>python -c "from virnucpro.pipeline.gpu_monitor import GPUMonitor, check_bf16_support; print(f'BF16 support: {check_bf16_support()}')"</verify>
  <done>GPU monitoring module imports and correctly detects BF16 support</done>
</task>

<task type="auto">
  <name>Task 2: Create multi-GPU progress dashboard</name>
  <files>virnucpro/pipeline/dashboard.py</files>
  <action>
    Create rich-based dashboard for live multi-GPU progress tracking:

    1. Import rich.progress, rich.table, rich.live, rich.console, pathlib, typing, datetime

    2. Implement MultiGPUDashboard class:
       - __init__(self, num_gpus, total_files_per_gpu)
       - Properties:
         - self.console = Console()
         - self.progress = Progress with BarColumn, TextColumn, TimeElapsedColumn
         - self.gpu_tasks = {} for tracking task IDs
         - self.start_time = None

       - start(self):
         - Initialize progress tasks for each GPU
         - Add task with description f"[cyan]GPU {gpu_id}"
         - Set total to files count for that GPU
         - Start Live display with self.progress

       - update(self, gpu_id, files_completed=1):
         - Advance progress for specific GPU
         - Update completion percentage

       - set_status(self, gpu_id, status_text):
         - Update task description with status
         - E.g., "[cyan]GPU 0 - Processing file_123.fa"

       - complete(self, gpu_id):
         - Mark GPU task as complete
         - Show elapsed time

       - get_summary(self):
         - Return dict with total files, completed, elapsed time
         - Calculate overall throughput (files/sec)

    3. Implement `create_progress_display(file_assignments)`:
       - Helper to create dashboard from file assignments
       - Calculate total files per GPU
       - Return configured MultiGPUDashboard instance

    Follow rich documentation patterns for concurrent progress bars (from research).
    Use Live display for smooth updates without flicker.
  </action>
  <verify>python -c "from virnucpro.pipeline.dashboard import MultiGPUDashboard; d = MultiGPUDashboard(4, {0:10, 1:10, 2:10, 3:10}); print('Dashboard created')"</verify>
  <done>Dashboard module imports and creates multi-GPU display instance</done>
</task>

<task type="auto">
  <name>Task 3: Add BF16 optimization to ESM-2 extraction</name>
  <files>virnucpro/pipeline/features.py</files>
  <action>
    Modify extract_esm_features function to use BF16 mixed precision:

    1. Add import at top: from torch.cuda.amp import autocast

    2. In extract_esm_features function, after model loading:
       - Add BF16 support check:
         ```python
         from virnucpro.pipeline.gpu_monitor import check_bf16_support
         use_bf16 = check_bf16_support() and str(device).startswith('cuda')
         if use_bf16:
             logger.info("Using BF16 mixed precision for memory efficiency")
         ```

    3. Wrap the main processing loop with autocast:
       - Replace existing "with torch.no_grad():" with:
         ```python
         with torch.no_grad():
             with autocast(device_type='cuda', dtype=torch.bfloat16, enabled=use_bf16):
                 for batch_idx, batch in enumerate(batches):
                     # existing batch processing code
         ```

    4. After autocast block, ensure embeddings are converted back to FP32:
       - Change line storing embedding to:
         ```python
         embedding = representations[i, 1:truncate_len + 1].mean(dim=0).clone().float().to('cpu')
         ```

    5. Optionally increase default toks_per_batch to 3072 (from 2048) when BF16 enabled:
       - Add after BF16 check:
         ```python
         if use_bf16 and toks_per_batch == 2048:
             toks_per_batch = 3072  # Increase batch size with BF16
             logger.info(f"Increased toks_per_batch to {toks_per_batch} with BF16")
         ```

    Ensure BF16 only used on compatible GPUs (Ampere+).
    Convert back to FP32 for storage to maintain compatibility.
  </action>
  <verify>grep -n "autocast.*bfloat16" virnucpro/pipeline/features.py</verify>
  <done>ESM-2 feature extraction uses BF16 autocast when available</done>
</task>

</tasks>

<verification>
- BF16 support is detected and used only on compatible GPUs
- GPU memory is monitored via direct CUDA APIs
- Rich dashboard displays concurrent progress for all GPUs
- Memory usage reduced by ~2x with BF16 enabled
</verification>

<success_criteria>
- gpu_monitor.py provides GPU memory tracking and BF16 detection
- dashboard.py creates live multi-GPU progress display
- features.py modified to use BF16 autocast for ESM-2
- All modules follow existing codebase patterns
</success_criteria>

<output>
After completion, create `.planning/phases/01-esm-2-multi-gpu-foundation/01-02-SUMMARY.md`
</output>