---
phase: 05-model-training-validation
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - data/test_set/test_metadata.json
  - model_fastesm650.pth
  - training_log_fastesm650.txt
  - reports/validation_report.md
  - reports/speed_benchmark.md
autonomous: false

must_haves:
  truths:
    - "Test set exists in data/test_set/ with 10% of data stratified by class"
    - "Trained model checkpoint exists at model_fastesm650.pth with version 2.0.0 metadata"
    - "Training completed with early stopping and all metrics logged per epoch"
    - "Validation report exists at reports/validation_report.md with metrics comparison"
    - "Speed benchmark results exist at reports/speed_benchmark.md"
    - "User can run predictions end-to-end with model_fastesm650.pth via prediction.py"
    - "User has reviewed results and confirmed phase completion"
  artifacts:
    - path: "data/test_set/test_metadata.json"
      provides: "Train/test split metadata with file lists and distribution stats"
      contains: "train_files"
    - path: "model_fastesm650.pth"
      provides: "Trained FastESM2 MLP checkpoint with metadata"
    - path: "training_log_fastesm650.txt"
      provides: "Per-epoch training metrics log"
    - path: "reports/validation_report.md"
      provides: "Comparison report with metrics tables and threshold validation"
      contains: "FastESM2_650"
    - path: "reports/speed_benchmark.md"
      provides: "Speed benchmark results with timing data"
      contains: "speedup"
  key_links:
    - from: "scripts/create_test_set.py"
      to: "data/test_set/"
      via: "execution creates test set directory and metadata"
      pattern: "test_metadata.json"
    - from: "scripts/train_fastesm.py"
      to: "model_fastesm650.pth"
      via: "execution produces trained checkpoint"
      pattern: "model_fastesm650"
    - from: "scripts/evaluate_compare.py"
      to: "reports/validation_report.md"
      via: "execution produces markdown report"
      pattern: "validation_report"
    - from: "prediction.py"
      to: "model_fastesm650.pth"
      via: "load_checkpoint_with_validation loads trained model for end-to-end prediction"
      pattern: "load_checkpoint_with_validation"
---

<objective>
Execute the full Phase 5 pipeline in Docker: create test set, train model, evaluate performance, run speed benchmark, validate end-to-end prediction workflow, and verify results.

Purpose: This is the execution plan that runs all scripts created in plans 01 and 02 inside the Docker container with GPU access. Training on 2M sequences may take significant time. The final checkpoint verifies the user is satisfied with results and that the prediction workflow works end-to-end.

Output: Trained model (model_fastesm650.pth), test set (data/test_set/), validation report (reports/validation_report.md), speed benchmark (reports/speed_benchmark.md).
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-01-SUMMARY.md
@.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-02-SUMMARY.md
@docker-compose.yml
@scripts/create_test_set.py
@scripts/train_fastesm.py
@scripts/evaluate_compare.py
@scripts/benchmark_speed.py
@prediction.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Execute full pipeline in Docker</name>
  <files>data/test_set/test_metadata.json, model_fastesm650.pth, training_log_fastesm650.txt, reports/validation_report.md, reports/speed_benchmark.md</files>
  <action>
Run the complete Phase 5 pipeline inside the Docker container. Execute steps sequentially since each depends on the previous.

**Step 1: Create test set**
```
docker compose run --rm virnucpro python scripts/create_test_set.py
```
- Verify: data/test_set/test_metadata.json exists
- Verify: data/test_set/viral/ and data/test_set/non_viral/ directories exist with symlinks
- Check: Distribution report shows approximately 52% viral / 48% non-viral in both train and test sets (matching full dataset ratio)

**Step 2: Train FastESM2 MLP**
```
docker compose run --rm virnucpro python scripts/train_fastesm.py
```
- This will take significant time (200 epochs max, 2M sequences, early stopping may trigger sooner)
- Monitor: training_log_fastesm650.txt for per-epoch progress
- Verify: model_fastesm650.pth created after training completes
- Check: Training log shows decreasing loss and reasonable accuracy

NOTE: If train.py module-level code causes import issues for train_fastesm.py, fix by adjusting imports (the MLPClassifier and utilities should be duplicated in train_fastesm.py per plan 01 instructions, not imported from train.py).

NOTE: The Triton patch from extract_training_data.py may be needed if DNABERT-S is loaded during any evaluation step. If Triton errors occur, apply the same runtime patching pattern from scripts/extract_training_data.py.

**Step 3: Evaluate and generate report**
```
docker compose run --rm virnucpro python scripts/evaluate_compare.py
```
- Verify: reports/validation_report.md exists
- Check: Report contains metrics table, confusion matrix, threshold validation
- Check: All 6 metrics reported (F1, Accuracy, Precision, Recall, ROC-AUC, confusion matrix)

**Step 4: Run speed benchmark**
```
docker compose run --rm virnucpro python scripts/benchmark_speed.py
```
- Verify: reports/speed_benchmark.md exists
- Check: Report shows FastESM2 timing, ESM2 3B timing (or note if unavailable), speedup ratio
- Note: ESM2 3B benchmark may fail due to model size (3B params needs ~12GB VRAM). If so, script should report FastESM2 absolute timing and note the limitation.

**Step 5: Validate end-to-end prediction workflow (TEST-05)**

Run a sample prediction using prediction.py with the newly trained model_fastesm650.pth to validate the complete user-facing workflow works end-to-end. Use a small sample FASTA file (a few sequences from the test data or any available FASTA in data/).

```
docker compose run --rm virnucpro python prediction.py <sample_fasta> 300 model_fastesm650.pth
```

- Verify: prediction.py loads model_fastesm650.pth via load_checkpoint_with_validation() without errors
- Verify: The pipeline completes: sequence chunking -> ORF identification -> DNABERT-S extraction -> FastESM2 extraction -> merge -> classification -> output
- Verify: prediction_results.txt and prediction_results_highestscore.csv are produced with valid predictions (virus/others labels, scores)
- If the sample FASTA has known viral sequences, spot-check that viral sequences are classified as virus

If prediction.py fails due to the Triton patch issue with DNABERT-S, apply the same runtime Triton patch from scripts/extract_training_data.py to prediction.py.

If no suitable sample FASTA file exists, create a minimal one with 2-3 known viral nucleotide sequences (e.g., from the original training data FASTA files in data/) for this validation.

**Error handling:**
- If any step fails, check error output and fix the script issue
- Common issues: Docker container not built, CUDA not available, file paths wrong
- Apply deviation rules: fix bugs inline, don't expand scope

**Create reports directory if needed:**
```
mkdir -p reports
```
  </action>
  <verify>
1. data/test_set/test_metadata.json exists and contains train_files/test_files lists
2. model_fastesm650.pth exists (check file size > 1MB indicating real model weights)
3. training_log_fastesm650.txt exists with multiple epoch entries
4. reports/validation_report.md exists with metrics tables
5. reports/speed_benchmark.md exists with timing data
6. prediction.py ran successfully with model_fastesm650.pth and produced prediction output files
  </verify>
  <done>
All pipeline outputs exist: test set created, model trained, evaluation report generated, speed benchmark completed. End-to-end prediction workflow validated with model_fastesm650.pth -- prediction.py successfully loads the new checkpoint, runs the full extraction-merge-classify pipeline, and produces prediction results. All files contain valid content (not empty or error states).
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Review results and validate phase success</name>
  <what-built>Complete Phase 5 pipeline: test set creation, FastESM2 MLP training, performance evaluation against ESM2 3B baseline, speed benchmarking, and end-to-end prediction workflow validation. All results in reports/ directory.</what-built>
  <how-to-verify>
1. Review training log: `cat training_log_fastesm650.txt | tail -20`
   - Expected: Training converged (loss decreasing), early stopping may have triggered
   - Check: Final validation accuracy is reasonable (>80% for viral classification)

2. Review validation report: `cat reports/validation_report.md`
   - Expected: F1, Accuracy, Precision, Recall, ROC-AUC all reported
   - Check: Accuracy drop from ESM2 3B baseline is <5% (or baseline not available with explanation)
   - Check: Confusion matrix shows reasonable true positive/negative rates
   - Check: Threshold validation result (PASSED or FAILED with next steps)

3. Review speed benchmark: `cat reports/speed_benchmark.md`
   - Expected: FastESM2_650 timing data with 3-run average
   - Check: Speedup ratio >= 2x (or explanation if ESM2 3B benchmark unavailable)

4. Verify model checkpoint: `ls -la model_fastesm650.pth`
   - Expected: File exists, size > 1MB

5. Verify test set: `cat data/test_set/test_metadata.json | python -m json.tool | head -20`
   - Expected: Contains train_files, test_files lists with proper counts

6. Test prediction workflow end-to-end (TEST-05):
   - Run: `docker compose run --rm virnucpro python prediction.py <sample_fasta> 300 model_fastesm650.pth`
   - Or review the prediction output files already produced by Task 1 Step 5
   - Expected: prediction_results.txt has sequence IDs, predictions (virus/others), and scores
   - Expected: prediction_results_highestscore.csv aggregates results per sequence
   - Check: Predictions are sensible (viral sequences classified as virus, non-viral as others)

**Decision points:**
- If accuracy drop > 5%: Phase reports FAILED, user decides next steps (hyperparameter tuning in future milestone)
- If speed benchmark < 2x: User decides if the speed improvement is still worth it
- If prediction workflow fails: User decides whether to fix or defer
- If all metrics pass and predictions work: Phase 5 complete, FastESM2 migration validated
  </how-to-verify>
  <resume-signal>Type "approved" to complete Phase 5, or describe any issues to address</resume-signal>
</task>

</tasks>

<verification>
1. Test set created with 10% stratified split (data/test_set/ directory exists)
2. Model trained and checkpoint saved (model_fastesm650.pth with version 2.0.0)
3. Training log shows convergence (training_log_fastesm650.txt)
4. Validation report generated (reports/validation_report.md)
5. Speed benchmark completed (reports/speed_benchmark.md)
6. End-to-end prediction workflow validated (prediction.py runs with model_fastesm650.pth)
7. User has reviewed and approved results
</verification>

<success_criteria>
All Phase 5 success criteria validated:
1. New MLP classifier trained from scratch on 2048-dim features with training metrics logged
2. Training pipeline validates input dimensions before starting
3. New model checkpoint saved as model_fastesm650.pth with metadata
4. Test dataset created from 10% random sample
5. Automated metrics calculated for FastESM2 (and ESM2 3B if baseline available)
6. Accuracy within acceptable range of baseline (<5% drop)
7. Speed benchmark shows improvement on sample sequences
8. User has run predictions end-to-end and validated workflow (TEST-05)
9. Both pipelines available for comparison (prediction.py supports FastESM2, old models still loadable)
</success_criteria>

<output>
After completion, create `.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-03-SUMMARY.md`
</output>
