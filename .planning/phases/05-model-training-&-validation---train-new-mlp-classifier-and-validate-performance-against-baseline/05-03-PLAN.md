---
phase: 05-model-training-validation
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - data/test_set/test_metadata.json
  - model_fastesm650.pth
  - training_log_fastesm650.txt
  - reports/validation_report.md
  - reports/speed_benchmark.md
autonomous: false

must_haves:
  truths:
    - "Test set exists in data/test_set/ with 10% of data stratified by class"
    - "Trained model checkpoint exists at model_fastesm650.pth with version 2.0.0 metadata"
    - "Training completed with early stopping and all metrics logged per epoch"
    - "Validation report exists at reports/validation_report.md with metrics comparison"
    - "Speed benchmark results exist at reports/speed_benchmark.md"
    - "User has reviewed results and confirmed phase completion"
  artifacts:
    - path: "data/test_set/test_metadata.json"
      provides: "Train/test split metadata with file lists and distribution stats"
      contains: "train_files"
    - path: "model_fastesm650.pth"
      provides: "Trained FastESM2 MLP checkpoint with metadata"
    - path: "training_log_fastesm650.txt"
      provides: "Per-epoch training metrics log"
    - path: "reports/validation_report.md"
      provides: "Comparison report with metrics tables and threshold validation"
      contains: "FastESM2_650"
    - path: "reports/speed_benchmark.md"
      provides: "Speed benchmark results with timing data"
      contains: "speedup"
  key_links:
    - from: "scripts/create_test_set.py"
      to: "data/test_set/"
      via: "execution creates test set directory and metadata"
      pattern: "test_metadata.json"
    - from: "scripts/train_fastesm.py"
      to: "model_fastesm650.pth"
      via: "execution produces trained checkpoint"
      pattern: "model_fastesm650"
    - from: "scripts/evaluate_compare.py"
      to: "reports/validation_report.md"
      via: "execution produces markdown report"
      pattern: "validation_report"
---

<objective>
Execute the full Phase 5 pipeline in Docker: create test set, train model, evaluate performance, run speed benchmark, and verify results.

Purpose: This is the execution plan that runs all scripts created in plans 01 and 02 inside the Docker container with GPU access. Training on 2M sequences may take significant time. The final checkpoint verifies the user is satisfied with results.

Output: Trained model (model_fastesm650.pth), test set (data/test_set/), validation report (reports/validation_report.md), speed benchmark (reports/speed_benchmark.md).
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-01-SUMMARY.md
@.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-02-SUMMARY.md
@docker-compose.yml
@scripts/create_test_set.py
@scripts/train_fastesm.py
@scripts/evaluate_compare.py
@scripts/benchmark_speed.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Execute full pipeline in Docker</name>
  <files>data/test_set/test_metadata.json, model_fastesm650.pth, training_log_fastesm650.txt, reports/validation_report.md, reports/speed_benchmark.md</files>
  <action>
Run the complete Phase 5 pipeline inside the Docker container. Execute steps sequentially since each depends on the previous.

**Step 1: Create test set**
```
docker compose run --rm virnucpro python scripts/create_test_set.py
```
- Verify: data/test_set/test_metadata.json exists
- Verify: data/test_set/viral/ and data/test_set/non_viral/ directories exist with symlinks
- Check: Distribution report shows approximately 52% viral / 48% non-viral in both train and test sets (matching full dataset ratio)

**Step 2: Train FastESM2 MLP**
```
docker compose run --rm virnucpro python scripts/train_fastesm.py
```
- This will take significant time (200 epochs max, 2M sequences, early stopping may trigger sooner)
- Monitor: training_log_fastesm650.txt for per-epoch progress
- Verify: model_fastesm650.pth created after training completes
- Check: Training log shows decreasing loss and reasonable accuracy

NOTE: If train.py module-level code causes import issues for train_fastesm.py, fix by adjusting imports (the MLPClassifier and utilities should be duplicated in train_fastesm.py per plan 01 instructions, not imported from train.py).

NOTE: The Triton patch from extract_training_data.py may be needed if DNABERT-S is loaded during any evaluation step. If Triton errors occur, apply the same runtime patching pattern from scripts/extract_training_data.py.

**Step 3: Evaluate and generate report**
```
docker compose run --rm virnucpro python scripts/evaluate_compare.py
```
- Verify: reports/validation_report.md exists
- Check: Report contains metrics table, confusion matrix, threshold validation
- Check: All 6 metrics reported (F1, Accuracy, Precision, Recall, ROC-AUC, confusion matrix)

**Step 4: Run speed benchmark**
```
docker compose run --rm virnucpro python scripts/benchmark_speed.py
```
- Verify: reports/speed_benchmark.md exists
- Check: Report shows FastESM2 timing, ESM2 3B timing (or note if unavailable), speedup ratio
- Note: ESM2 3B benchmark may fail due to model size (3B params needs ~12GB VRAM). If so, script should report FastESM2 absolute timing and note the limitation.

**Error handling:**
- If any step fails, check error output and fix the script issue
- Common issues: Docker container not built, CUDA not available, file paths wrong
- Apply deviation rules: fix bugs inline, don't expand scope

**Create reports directory if needed:**
```
mkdir -p reports
```
  </action>
  <verify>
1. data/test_set/test_metadata.json exists and contains train_files/test_files lists
2. model_fastesm650.pth exists (check file size > 1MB indicating real model weights)
3. training_log_fastesm650.txt exists with multiple epoch entries
4. reports/validation_report.md exists with metrics tables
5. reports/speed_benchmark.md exists with timing data
  </verify>
  <done>
All pipeline outputs exist: test set created, model trained, evaluation report generated, speed benchmark completed. All files contain valid content (not empty or error states).
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Review results and validate phase success</name>
  <what-built>Complete Phase 5 pipeline: test set creation, FastESM2 MLP training, performance evaluation against ESM2 3B baseline, and speed benchmarking. All results in reports/ directory.</what-built>
  <how-to-verify>
1. Review training log: `cat training_log_fastesm650.txt | tail -20`
   - Expected: Training converged (loss decreasing), early stopping may have triggered
   - Check: Final validation accuracy is reasonable (>80% for viral classification)

2. Review validation report: `cat reports/validation_report.md`
   - Expected: F1, Accuracy, Precision, Recall, ROC-AUC all reported
   - Check: Accuracy drop from ESM2 3B baseline is <5% (or baseline not available with explanation)
   - Check: Confusion matrix shows reasonable true positive/negative rates
   - Check: Threshold validation result (PASSED or FAILED with next steps)

3. Review speed benchmark: `cat reports/speed_benchmark.md`
   - Expected: FastESM2_650 timing data with 3-run average
   - Check: Speedup ratio >= 2x (or explanation if ESM2 3B benchmark unavailable)

4. Verify model checkpoint: `ls -la model_fastesm650.pth`
   - Expected: File exists, size > 1MB

5. Verify test set: `cat data/test_set/test_metadata.json | python -m json.tool | head -20`
   - Expected: Contains train_files, test_files lists with proper counts

**Decision points:**
- If accuracy drop > 5%: Phase reports FAILED, user decides next steps (hyperparameter tuning in future milestone)
- If speed benchmark < 2x: User decides if the speed improvement is still worth it
- If all metrics pass: Phase 5 complete, FastESM2 migration validated
  </how-to-verify>
  <resume-signal>Type "approved" to complete Phase 5, or describe any issues to address</resume-signal>
</task>

</tasks>

<verification>
1. Test set created with 10% stratified split (data/test_set/ directory exists)
2. Model trained and checkpoint saved (model_fastesm650.pth with version 2.0.0)
3. Training log shows convergence (training_log_fastesm650.txt)
4. Validation report generated (reports/validation_report.md)
5. Speed benchmark completed (reports/speed_benchmark.md)
6. User has reviewed and approved results
</verification>

<success_criteria>
All Phase 5 success criteria validated:
1. New MLP classifier trained from scratch on 2048-dim features with training metrics logged
2. Training pipeline validates input dimensions before starting
3. New model checkpoint saved as model_fastesm650.pth with metadata
4. Test dataset created from 10% random sample
5. Automated metrics calculated for FastESM2 (and ESM2 3B if baseline available)
6. Accuracy within acceptable range of baseline (<5% drop)
7. Speed benchmark shows improvement on sample sequences
8. User has run predictions end-to-end and validated workflow
9. Both pipelines available for comparison (prediction.py supports FastESM2, old models still loadable)
</success_criteria>

<output>
After completion, create `.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-03-SUMMARY.md`
</output>
