---
phase: 05-model-training-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/create_test_set.py
  - scripts/train_fastesm.py
autonomous: true

must_haves:
  truths:
    - "Test set contains exactly 10% of data, stratified by class (viral/non-viral)"
    - "Test files stored in separate directory (data/test_set/), not mixed with training data"
    - "Training script uses only 90% train split, not full dataset"
    - "Model trained with early stopping and detailed per-epoch logging"
    - "Checkpoint saved as model_fastesm650.pth with version 2.0.0 metadata"
  artifacts:
    - path: "scripts/create_test_set.py"
      provides: "Stratified test set creation with fixed seed and distribution report"
      min_lines: 80
    - path: "scripts/train_fastesm.py"
      provides: "Training script with early stopping, logging, checkpoint saving"
      min_lines: 150
  key_links:
    - from: "scripts/create_test_set.py"
      to: "data/data_merge/"
      via: "reads merged .pt files and splits into train/test lists"
      pattern: "data_merge"
    - from: "scripts/train_fastesm.py"
      to: "scripts/create_test_set.py"
      via: "reads test set metadata to exclude test files from training"
      pattern: "test_set|test_metadata"
    - from: "scripts/train_fastesm.py"
      to: "train.py"
      via: "reuses MLPClassifier, EarlyStopping, save_checkpoint_with_metadata patterns"
      pattern: "MLPClassifier|EarlyStopping|save_checkpoint"
---

<objective>
Create test set splitting script and enhanced training script for FastESM2 MLP classifier.

Purpose: Enable reproducible model training with proper train/test separation matching paper methodology (10% test, 90% train with stratification). The test set must be created FIRST and persisted so both FastESM2 and any baseline evaluation use identical test data.

Output: Two scripts ready to run in Docker -- create_test_set.py produces the split, train_fastesm.py trains the model using only training data.
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@train.py
@units.py
@.planning/phases/04-training-data-preparation/04-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create stratified test set splitting script</name>
  <files>scripts/create_test_set.py</files>
  <action>
Create scripts/create_test_set.py that performs a reproducible 10% stratified split of the training data.

The script must:

1. **Discover all merged .pt files** from data/data_merge/ -- there are 20 subdirectories (1 viral: viral.1.1_merged with 105 files, 19 non-viral directories with 96 files total). Each directory has output_N_merged.pt files.

2. **Classify files as viral (label=1) or non-viral (label=0)** based on directory name containing "viral" prefix. Load one file per directory to confirm labels match (the 'labels' key in the .pt dict is [1] for viral, [0] for non-viral).

3. **Perform stratified split using sklearn.model_selection.train_test_split** with:
   - test_size=0.1 (10% test per user decision matching paper methodology)
   - stratify=labels (preserve viral/non-viral ratio)
   - random_state=42 (fixed seed for reproducibility per user decision)

4. **Copy test files to data/test_set/ directory** organized as:
   - data/test_set/viral/ -- test viral .pt files (symlinks to originals to save disk)
   - data/test_set/non_viral/ -- test non-viral .pt files (symlinks to originals)

5. **Save metadata** to data/test_set/test_metadata.json containing:
   - seed used (42)
   - split ratio (0.1)
   - train_files list (absolute paths)
   - test_files list (absolute paths)
   - distribution stats (viral count, non-viral count, ratios)
   - timestamp

6. **Print distribution report** showing:
   - Total files, test files, train files
   - Viral ratio in test vs train (should be nearly identical)
   - Number of viral families represented in test set
   - Sequence count in test set (sum of tensor sizes from loaded files)

Use argparse with defaults: --data-dir ./data/data_merge/ --output-dir ./data/test_set/ --split 0.1 --seed 42

Set comprehensive random seeds at script start: random.seed(), np.random.seed(), torch.manual_seed() per research recommendations.

Do NOT use per-class performance breakdown by viral family (deferred idea -- out of scope).
  </action>
  <verify>
Run: python scripts/create_test_set.py --help
Verify: Script accepts --data-dir, --output-dir, --split, --seed arguments.

Check: Script imports run without errors (sklearn.model_selection.train_test_split available).
  </verify>
  <done>
Script exists, parses arguments, imports resolve. When run with data, it will create data/test_set/ with symlinked .pt files and test_metadata.json containing train/test file lists and distribution stats.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create enhanced training script for FastESM2 MLP</name>
  <files>scripts/train_fastesm.py</files>
  <action>
Create scripts/train_fastesm.py that trains the MLP classifier using ONLY the training split (excluding test files).

Base this on the existing train.py patterns but with these enhancements per user decisions:

1. **Load train/test split from metadata:**
   - Read data/test_set/test_metadata.json to get train_files list
   - Use ONLY train_files for training (not full data_merge directory)
   - Create a separate validation split from train_files (e.g., 10% of train for validation during training)
   - Load test_files separately for final evaluation after training

2. **Reuse existing patterns from train.py:**
   - FileBatchDataset class (import or duplicate -- since train.py runs at module level, duplicate the class)
   - MLPClassifier from units or train.py (import from train.py will trigger module-level execution, so duplicate it or import from units -- actually MLPClassifier is defined in train.py not units.py, so duplicate it)
   - EarlyStopping class (duplicate from train.py)
   - save_checkpoint_with_metadata function (duplicate from train.py)
   - Import DimensionError, MERGED_DIM, DNA_DIM, PROTEIN_DIM, CHECKPOINT_VERSION from units

3. **Use existing hyperparameters** (per user locked decision -- apples-to-apples comparison):
   - input_dim = MERGED_DIM (2048)
   - hidden_dim = 512
   - num_class = 2
   - batch_size = 32
   - num_workers = 12
   - optimizer: SGD lr=0.0002, momentum=0.9
   - scheduler: StepLR step_size=10, gamma=0.85
   - num_epochs = 200 (max)
   - criterion: CrossEntropyLoss

4. **Early stopping** with configurable patience (Claude's discretion: default patience=7, slightly higher than existing 5 to allow more exploration with new embeddings):
   - Track validation loss
   - Restore best model weights when triggered
   - Log when early stopping activates

5. **Detailed logging** per user decision:
   - Per-epoch: epoch number, train loss, val loss, val accuracy, val F1, learning rate, time elapsed
   - Per-batch progress via tqdm (already in existing code)
   - Save training log to training_log_fastesm650.txt
   - Print summary at end: total epochs, best epoch, best val loss, final metrics

6. **Checkpoint saving:**
   - Default: save final trained model as model_fastesm650.pth (per user decision)
   - Optional --save-best flag: save best validation model instead
   - Use save_checkpoint_with_metadata() with version 2.0.0 metadata

7. **Reproducibility:**
   - Set all seeds at start: random, numpy, torch, torch.cuda, cudnn.deterministic=True, cudnn.benchmark=False
   - DataLoader worker_init_fn for worker seeding
   - Generator for DataLoader shuffle

8. **Dimension validation:**
   - Validate input dimensions BEFORE starting training (load one batch, check shape matches MERGED_DIM)
   - MLPClassifier.forward() already has runtime dimension validation

9. **argparse interface:**
   - --metadata path to test_metadata.json (default: ./data/test_set/test_metadata.json)
   - --epochs max epochs (default: 200)
   - --patience early stopping patience (default: 7)
   - --batch-size (default: 32)
   - --lr learning rate (default: 0.0002)
   - --output checkpoint output path (default: model_fastesm650.pth)
   - --save-best flag to save best validation model instead of final
   - --log-file training log path (default: training_log_fastesm650.txt)

Do NOT implement hyperparameter optimization (deferred idea -- out of scope).
Do NOT implement per-class breakdown by viral family (deferred idea -- out of scope).
  </action>
  <verify>
Run: python scripts/train_fastesm.py --help
Verify: Script shows all arguments with defaults matching user decisions.

Check: Script imports resolve (torch, sklearn, tqdm, units constants).
Verify: MLPClassifier input_dim defaults to MERGED_DIM (2048).
  </verify>
  <done>
Script exists with full argparse interface. When run in Docker with GPU, it loads train split from metadata, trains MLPClassifier with early stopping and detailed per-epoch logging, saves model_fastesm650.pth with version 2.0.0 metadata. Uses existing hyperparameters for apples-to-apples comparison.
  </done>
</task>

</tasks>

<verification>
1. scripts/create_test_set.py exists and runs --help without error
2. scripts/train_fastesm.py exists and runs --help without error
3. Both scripts import MERGED_DIM from units and use 2048-dim
4. Training script reads test_metadata.json to exclude test files
5. Training script uses existing hyperparameters (SGD lr=0.0002, momentum=0.9, StepLR step_size=10 gamma=0.85)
6. Early stopping patience defaults to 7 (Claude's discretion)
7. Checkpoint saves with save_checkpoint_with_metadata() and version 2.0.0
</verification>

<success_criteria>
- TEST-01: Test set creation script ready (10% stratified split, fixed seed 42, separate directory)
- TRAIN-02: Training script uses 2048-dim features with MLPClassifier
- TRAIN-03: Dimension validation before training start
- TRAIN-04: Checkpoint saved as model_fastesm650.pth with metadata
- TRAIN-05: Detailed per-epoch logging (loss, accuracy, F1, LR, time)
</success_criteria>

<output>
After completion, create `.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-01-SUMMARY.md`
</output>
