---
phase: 05-model-training-validation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/evaluate_compare.py
  - scripts/benchmark_speed.py
autonomous: true

must_haves:
  truths:
    - "Evaluation script calculates F1, Accuracy, Precision, Recall, ROC-AUC and confusion matrix"
    - "Evaluation script compares FastESM2 metrics against ESM2 3B baseline"
    - "Markdown validation report generated with metrics tables and threshold check"
    - "If accuracy drops >5%, report clearly states FAILED with suggested next steps"
    - "Speed benchmark measures embedding extraction time averaged over 3 runs with GPU sync"
  artifacts:
    - path: "scripts/evaluate_compare.py"
      provides: "Model evaluation, baseline comparison, and markdown report generation"
      min_lines: 150
    - path: "scripts/benchmark_speed.py"
      provides: "GPU-synchronized speed benchmark for FastESM2 vs ESM2 3B extraction"
      min_lines: 100
  key_links:
    - from: "scripts/evaluate_compare.py"
      to: "data/test_set/test_metadata.json"
      via: "loads test file list for evaluation"
      pattern: "test_metadata"
    - from: "scripts/evaluate_compare.py"
      to: "model_fastesm650.pth"
      via: "loads trained FastESM2 checkpoint with validation"
      pattern: "load_checkpoint|model_fastesm650"
    - from: "scripts/benchmark_speed.py"
      to: "units.py"
      via: "imports extract_fast_esm for FastESM2 timing"
      pattern: "extract_fast_esm"
---

<objective>
Create evaluation/comparison and speed benchmarking scripts for FastESM2 vs ESM2 3B.

Purpose: Provide automated, reproducible evaluation of the trained FastESM2 model against the ESM2 3B baseline, with a comprehensive markdown report and strict threshold validation (<5% accuracy drop). Speed benchmark proves the 2x speedup claim.

Output: Two scripts ready to run in Docker -- evaluate_compare.py produces metrics and report, benchmark_speed.py measures extraction speed.
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@train.py
@prediction.py
@units.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create evaluation and comparison script with markdown report</name>
  <files>scripts/evaluate_compare.py</files>
  <action>
Create scripts/evaluate_compare.py that evaluates the trained FastESM2 model on the test set, compares against ESM2 3B baseline, and generates a markdown validation report.

**Evaluation workflow:**

1. **Load test set** from test_metadata.json (test_files list)
2. **Load FastESM2 model** from checkpoint using load_checkpoint_with_validation() pattern from prediction.py (validate metadata, version 2.0.0, merged_dim=2048)
3. **Evaluate FastESM2 model on test set:**
   - Use FileBatchDataset to load test .pt files
   - Run inference with model.eval() and torch.no_grad()
   - Collect all predictions, true labels, and probability scores
   - Calculate: accuracy, precision, recall, F1, ROC-AUC using sklearn.metrics
   - Generate confusion matrix using sklearn.metrics.confusion_matrix

4. **ESM2 3B baseline handling:**
   - The old model (300_model.pth) was saved as full model object with 3328-dim input
   - Old ESM2 features no longer exist (overwritten in Phase 4)
   - extract_esm() was removed (raises NotImplementedError)
   - Therefore: ESM2 3B baseline metrics must come from the old model's training logs
   - Accept --baseline-metrics flag with JSON string or file path containing baseline metrics
   - If no baseline provided, attempt to load 300_model.pth and extract training metrics from logs, or skip comparison and only report FastESM2 metrics with a note
   - Print clear message: "ESM2 3B baseline metrics not available for direct comparison. Old model's original training validation metrics can be used as approximate baseline."

5. **Threshold validation** per user locked decision:
   - If baseline metrics available: check if accuracy drop > 5% (strict threshold)
   - If FAILED: print "DEPLOYMENT HALTED" with suggested next steps per user decision
   - If PASSED: print confirmation

6. **Generate markdown report** to reports/validation_report.md:
   - Header with timestamp, model info
   - Performance comparison table (pandas .to_markdown()):
     | Metric | FastESM2_650 | ESM2_3B_Baseline | Difference |
   - Confusion matrix (text format, not image -- Docker may not have display)
   - Threshold validation result (PASSED/FAILED)
   - Test set info (total samples, viral/non-viral counts)
   - If threshold failed: suggested next steps section

7. **Terminal summary** per user decision (both terminal AND report file):
   - Print all metrics to stdout
   - Print threshold result
   - Print path to full report

**argparse interface:**
- --model path to FastESM2 checkpoint (default: model_fastesm650.pth)
- --test-metadata path to test metadata (default: ./data/test_set/test_metadata.json)
- --baseline-metrics path to JSON file or inline JSON with ESM2 3B metrics (optional)
- --threshold maximum acceptable accuracy drop (default: 0.05)
- --report-dir output directory for report (default: ./reports/)
- --batch-size evaluation batch size (default: 256)

**Key implementation details:**
- Duplicate MLPClassifier and FileBatchDataset from train.py (since train.py has module-level execution)
- Import DimensionError, MERGED_DIM, CHECKPOINT_VERSION from units
- Use load_checkpoint_with_validation pattern from prediction.py
- ROC-AUC uses softmax probabilities for class 1 (viral)
- Confusion matrix labels: ['Non-Viral', 'Viral']

Do NOT implement per-class performance breakdown by viral family (deferred idea -- out of scope).
  </action>
  <verify>
Run: python scripts/evaluate_compare.py --help
Verify: Script shows all arguments with proper defaults.

Check: Script imports resolve (sklearn.metrics, pandas, torch, units constants).
Verify: Script includes accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix imports.
  </verify>
  <done>
Script exists with full evaluation pipeline. When run in Docker with a trained model and test set, it calculates all required metrics (F1, Accuracy, Precision, Recall, ROC-AUC, confusion matrix), compares against baseline if available, validates <5% accuracy drop threshold, and generates both terminal summary and reports/validation_report.md.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create GPU-synchronized speed benchmark script</name>
  <files>scripts/benchmark_speed.py</files>
  <action>
Create scripts/benchmark_speed.py that benchmarks FastESM2_650 vs ESM2 3B protein embedding extraction speed.

**Benchmark scope:** Per user locked decision, benchmark is "just embedding extraction time (FastESM2 vs ESM2 3B protein embeddings)."

**Benchmark protocol** per user locked decision:
- 3 runs averaged to account for timing variability
- FastESM2 must take <=50% of ESM2 3B time (strict 2x speedup)

**Implementation:**

1. **Sample selection** (Claude's discretion):
   - Select 100 protein sequences from the test set or training data
   - Use sequences of representative lengths (mix of short 100aa, medium 300aa, long 500aa)
   - Read protein sequences from FASTA files in data/ directory (e.g., viral.1.1.identified_protein.fa)
   - If FASTA files not available, generate synthetic protein sequences of realistic lengths

2. **FastESM2_650 benchmark:**
   - Load model: AutoModel.from_pretrained("Synthyra/FastESM2_650", trust_remote_code=True, torch_dtype=torch.float16).eval().cuda()
   - Warmup: 10 forward passes (not counted)
   - Timed runs: Process all sample sequences 3 times
   - Use torch.cuda.synchronize() before start and after end of each run
   - Use time.perf_counter() for timing (NOT time.time())
   - Record per-run times and average

3. **ESM2 3B benchmark:**
   - Load model via HuggingFace: AutoModel.from_pretrained("facebook/esm2_t36_3B_UR50D")
   - If model fails to load (too large for GPU memory, not available), report "ESM2 3B benchmark skipped: model failed to load" and proceed with FastESM2-only results
   - Same warmup and timing protocol as FastESM2
   - torch.cuda.synchronize() before/after each run

4. **Report results:**
   - Print table: model, total time, per-sequence time, sequences/second
   - Print speedup ratio: ESM2_3B_time / FastESM2_time
   - Print PASSED/FAILED based on 2x speedup threshold
   - Save results to reports/speed_benchmark.md

5. **Fallback if ESM2 3B unavailable:**
   - If ESM2 3B can't be loaded (GPU memory, missing model), print warning
   - Report FastESM2 absolute timing only
   - Note: "ESM2 3B comparison requires ~12GB GPU memory. Run on appropriate hardware for full comparison."

**argparse interface:**
- --num-sequences number of sequences to benchmark (default: 100)
- --num-runs number of timed runs (default: 3)
- --warmup number of warmup iterations (default: 10)
- --report-dir output directory (default: ./reports/)
- --fastesm-only flag to skip ESM2 3B (useful if GPU too small)

**Anti-patterns to avoid per research:**
- Do NOT use time.time() (CPU-only, inaccurate for GPU ops)
- Do NOT skip warmup (first runs include kernel compilation overhead)
- Do NOT do single-run measurement (need 3+ for statistical significance)
- Do NOT forget torch.cuda.synchronize() (GPU ops are async)
  </action>
  <verify>
Run: python scripts/benchmark_speed.py --help
Verify: Script shows all arguments including --num-runs, --warmup, --fastesm-only.

Check: Script imports torch.cuda.synchronize and time.perf_counter.
Check: Script uses warmup loop before timed runs.
  </verify>
  <done>
Script exists with GPU-synchronized benchmarking. When run in Docker with GPU, it loads FastESM2_650 (and optionally ESM2 3B), runs warmup iterations, performs 3 timed embedding extraction runs, reports average times and speedup ratio, validates 2x speedup threshold, saves results to reports/speed_benchmark.md.
  </done>
</task>

</tasks>

<verification>
1. scripts/evaluate_compare.py exists and runs --help without error
2. scripts/benchmark_speed.py exists and runs --help without error
3. Evaluation script calculates all 6 required metrics (F1, Accuracy, Precision, Recall, ROC-AUC, confusion matrix)
4. Evaluation script validates <5% accuracy drop threshold
5. Evaluation script generates markdown report
6. Benchmark script uses torch.cuda.synchronize() and time.perf_counter()
7. Benchmark script includes warmup runs before timed measurements
8. Benchmark script averages 3 runs per user decision
</verification>

<success_criteria>
- TEST-02: Evaluation script calculates F1, accuracy, precision, recall, ROC-AUC, confusion matrix
- TEST-03: Comparison against ESM2 3B baseline with threshold validation
- TEST-04: Speed benchmark with GPU-synchronized timing, 3-run average
- TEST-05: Scripts form complete evaluation pipeline runnable in Docker
- TEST-06: Both ESM2 3B and FastESM2_650 benchmarking available (with fallback if ESM2 3B unavailable)
</success_criteria>

<output>
After completion, create `.planning/phases/05-model-training-&-validation---train-new-mlp-classifier-and-validate-performance-against-baseline/05-02-SUMMARY.md`
</output>
