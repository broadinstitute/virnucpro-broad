---
phase: 04-training-data-preparation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/extract_training_data.py
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/extract_training_data.py` auto-discovers all training FASTA files in data/ and extracts FastESM2_650 protein embeddings"
    - "Extraction resumes from where it left off when interrupted and re-run"
    - "Progress bar shows per-file completion with ETA"
    - "Missing DNABERT-S embeddings cause immediate error before any extraction starts"
    - "On completion, validation suite confirms all embeddings are 1280-dim and merged features are 2048-dim"
  artifacts:
    - path: "scripts/extract_training_data.py"
      provides: "Single-command training data extraction with resume, progress, and validation"
      min_lines: 150
  key_links:
    - from: "scripts/extract_training_data.py"
      to: "units.py"
      via: "imports extract_fast_esm, merge_data, validate_merge_inputs, PROTEIN_DIM, MERGED_DIM"
      pattern: "from units import"
    - from: "scripts/extract_training_data.py"
      to: "features_extract.py"
      via: "mirrors data discovery logic from features_extract.py (viral vs host categories, random sampling)"
      pattern: "os\\.walk.*data"
---

<objective>
Create `scripts/extract_training_data.py` - a single-command script that re-extracts all training data with FastESM2_650 embeddings.

Purpose: This script replaces the manual multi-step extraction workflow in `features_extract.py` with a clean, resumable, single-command approach that produces FastESM2_650 protein embeddings for all training data.

Output: `scripts/extract_training_data.py` ready to run inside Docker container.
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@features_extract.py
@units.py
@docker-compose.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extract_training_data.py with auto-discovery, resume, progress, and validation</name>
  <files>scripts/extract_training_data.py</files>
  <action>
Create `scripts/extract_training_data.py` that processes the entire training dataset through FastESM2_650.

**Data discovery logic** (mirror from `features_extract.py`):
- Walk `./data/` to find all `*identified_nucleotide.fa` files
- Categorize into viral vs host (vertebrate, protozoa, plant, invertebrate, fungi, bacteria, archaea) using path prefixes
- For each category, discover split FASTA sub-files in `identified_nucleotide/` and `identified_protein/` subdirectories
- These split files are created by `split_fasta_file()` in the existing pipeline - they already exist on disk
- The corresponding DNABERT-S `.pt` files (`*_DNABERT_S.pt`) must also already exist

**Pre-flight checks** (run before any extraction):
- Verify DNABERT-S embeddings exist for all discovered nucleotide files (error if any missing, listing which files are missing)
- Verify CUDA is available (error if not)
- Print discovery summary: number of viral files, host files per category, total files to process

**Extraction workflow:**
- Load FastESM2_650 model once at startup (same pattern as `features_extract.py`: `AutoModel.from_pretrained("Synthyra/FastESM2_650", trust_remote_code=True, torch_dtype=torch.float16).eval().cuda()`)
- Load tokenizer from model: `model.tokenizer`
- For each protein FASTA file, call `extract_fast_esm(fasta_file=..., out_file=..., model=..., tokenizer=...)` where out_file follows the `*_ESM.pt` naming convention (same as `process_file_pro()` in features_extract.py: `f'{file.split(".fa")[0]}_ESM.pt'`)
- `extract_fast_esm()` already has resume capability (skips if out_file exists), so this provides file-level resume for free
- After protein extraction for each category, call `merge_data()` to produce merged files in `./data/data_merge/` following the same path conventions as `features_extract.py`
- `merge_data()` already has dimension validation built in (from Phase 3)

**Checkpoint file** (for tracking progress across restarts):
- Use JSON checkpoint file at `./data/.extraction_checkpoint.json`
- Track: `{"completed_files": ["path1.pt", "path2.pt", ...], "started_at": "ISO8601", "last_update": "ISO8601"}`
- On startup, load checkpoint and skip any files already in completed list
- Update checkpoint after each successful file extraction
- This provides an additional layer of resume beyond `extract_fast_esm()`'s built-in file-existence check

**Progress tracking:**
- Use tqdm for overall progress bar (total = number of protein files to process)
- Log per-file completion: `logger.info(f"[{completed}/{total}] Extracted {file} ({n_sequences} sequences, {elapsed:.1f}s)")`
- On completion, print full statistics:
  - Total sequences processed
  - Total files processed
  - Total time taken
  - Average time per file
  - Average time per sequence

**Post-extraction validation suite:**
- After all extraction and merging complete, run validation:
  1. Check every `*_ESM.pt` file: load and verify all embeddings are exactly 1280-dim
  2. Check every merged `*_merged.pt` file: verify data tensor has second dimension = 2048
  3. Count total sequences across all merged files
  4. Report any dimension mismatches as errors with file path and actual shape
- Print validation summary: "Validation passed: N files, M sequences, all dimensions correct"

**Error handling** (per user decisions - fail fast):
- Any protein extraction error: log detailed context (file path, tensor shapes, stack trace) and `sys.exit(1)`
- Dimension validation failure: halt immediately with details
- Missing DNABERT-S files: list all missing files and exit before starting extraction
- Wrap the main extraction loop in try/except to catch unexpected errors with full traceback

**Script structure:**
```python
#!/usr/bin/env python3
"""Re-extract all training data with FastESM2_650 embeddings."""

def discover_training_data(data_dir='./data/'):
    """Auto-discover all training FASTA files and return categorized lists."""
    ...

def load_checkpoint(checkpoint_path):
    """Load extraction checkpoint for resume capability."""
    ...

def save_checkpoint(checkpoint_path, completed_files, started_at):
    """Save extraction checkpoint."""
    ...

def validate_prerequisites(viral_files, host_files):
    """Verify DNABERT-S embeddings exist and CUDA is available."""
    ...

def extract_all(viral_protein_files, host_protein_files, model, tokenizer, checkpoint_path):
    """Extract FastESM2_650 embeddings for all protein files."""
    ...

def merge_all(viral_files, host_files):
    """Merge DNABERT-S and protein embeddings for all files."""
    ...

def validate_all(data_dir='./data/'):
    """Post-extraction validation suite."""
    ...

def main():
    ...

if __name__ == '__main__':
    main()
```

Important implementation notes:
- Use `random.seed(42)` for host file sampling consistency (same as features_extract.py)
- Host file sampling: select `ceil(len(viral_files)/7)` from each category, then downsample to `len(viral_files)` total (matching features_extract.py logic exactly)
- Process protein files sequentially (CUDA contexts not fork-safe, per 02-01 decision)
- DNABERT-S files use multiprocessing.Pool for CPU-bound extraction, but we are NOT re-extracting DNABERT-S - just reusing existing files
- The merge output path construction must match features_extract.py exactly for train.py compatibility
  </action>
  <verify>
  Run: `python -c "import ast; ast.parse(open('scripts/extract_training_data.py').read()); print('Syntax OK')"` to verify valid Python.

  Run: `grep -c 'def ' scripts/extract_training_data.py` should show at least 6 functions.

  Run: `grep 'extract_fast_esm' scripts/extract_training_data.py` to verify it uses the Phase 2 extraction function.

  Run: `grep 'merge_data' scripts/extract_training_data.py` to verify it uses the merge function.

  Run: `grep 'PROTEIN_DIM\|1280' scripts/extract_training_data.py` to verify dimension validation references.

  Run: `grep 'tqdm' scripts/extract_training_data.py` to verify progress bar usage.

  Run: `grep 'checkpoint' scripts/extract_training_data.py` to verify checkpoint/resume logic.

  Run: `grep 'DNABERT_S' scripts/extract_training_data.py` to verify DNABERT-S prerequisite checking.
  </verify>
  <done>
  `scripts/extract_training_data.py` exists with:
  - Auto-discovery of training data from data/ directory
  - Pre-flight checks for DNABERT-S prerequisites and CUDA
  - FastESM2_650 model loading and sequential protein extraction using extract_fast_esm()
  - JSON checkpoint file for resume across restarts
  - tqdm progress bar with per-file logging
  - Post-extraction validation suite checking 1280-dim proteins and 2048-dim merged features
  - Fail-fast error handling with detailed debugging context
  - Matching data discovery and path conventions from features_extract.py
  </done>
</task>

</tasks>

<verification>
1. Script parses without syntax errors
2. All key functions present: discover, checkpoint, validate_prerequisites, extract, merge, validate
3. Uses extract_fast_esm() from units.py (not reimplementing extraction)
4. Uses merge_data() from units.py (not reimplementing merging)
5. References PROTEIN_DIM (1280) and MERGED_DIM (2048) for validation
6. Has tqdm progress bar
7. Has checkpoint/resume logic
8. Checks for missing DNABERT-S files before starting
</verification>

<success_criteria>
- `scripts/extract_training_data.py` is a complete, runnable script
- Running `python scripts/extract_training_data.py` would auto-discover data, extract, merge, and validate
- Resume capability works via checkpoint file + extract_fast_esm's built-in file check
- Pre-flight validation catches missing DNABERT-S files before any GPU work starts
- Post-extraction validation confirms all dimension requirements
</success_criteria>

<output>
After completion, create `.planning/phases/04-training-data-preparation/04-01-SUMMARY.md`
</output>
