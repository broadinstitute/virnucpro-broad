---
phase: 04-training-data-preparation
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "All training protein FASTA files have been processed through FastESM2_650"
    - "Every *_ESM.pt file contains 1280-dim protein embeddings"
    - "Every *_merged.pt file contains 2048-dim merged features"
    - "Extraction completed without errors or dimension mismatches"
    - "Data is ready for MLP training in Phase 5"
  artifacts:
    - path: "data/data_merge/"
      provides: "Merged training data files with 2048-dim features"
  key_links:
    - from: "scripts/extract_training_data.py"
      to: "data/"
      via: "reads protein FASTA files, writes *_ESM.pt and *_merged.pt"
      pattern: "\\*_ESM\\.pt|\\*_merged\\.pt"
---

<objective>
Run the training data extraction script inside Docker container and verify results.

Purpose: Execute the actual data re-extraction with FastESM2_650 on GPU, producing all the training data needed for Phase 5 model retraining. This is the real-world run that fulfills TRAIN-01.

Output: All training data re-extracted and validated with FastESM2_650 embeddings.
</objective>

<execution_context>
@/home/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-training-data-preparation/04-01-SUMMARY.md
@scripts/extract_training_data.py
@docker-compose.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run extraction in Docker container</name>
  <files></files>
  <action>
Run the extraction script inside the Docker container:

```bash
docker-compose run --rm virnucpro python scripts/extract_training_data.py
```

Monitor output for:
- Discovery summary (number of files found per category)
- Progress bar advancement
- Any errors or warnings
- Final statistics and validation results

If the extraction is interrupted (OOM, timeout, etc.):
- Re-run the same command - the checkpoint/resume logic will pick up where it left off
- Check extraction_failures.log for any logged failures

If extraction completes successfully, capture the final output including:
- Total sequences processed
- Total files processed
- Time taken
- Validation pass/fail
  </action>
  <verify>
  The extraction script exits with code 0 and prints "Validation passed" in its output.

  Verify merged files exist: `find data/data_merge/ -name '*_merged.pt' | wc -l` should be > 0.

  Verify protein embedding dimensions: `python -c "import torch; import os; files=[os.path.join(r,f) for r,d,fs in os.walk('data') for f in fs if f.endswith('_ESM.pt')]; d=torch.load(files[0]); print(f'Dim: {d[\"data\"][0].shape}')"` should show `(1280,)`.
  </verify>
  <done>Extraction completes without errors and validation passes.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
  All training data has been re-extracted with FastESM2_650 embeddings. The extraction script discovered all training FASTA files, processed them through the new protein embedding pipeline, merged them with existing DNABERT-S embeddings, and validated all dimensions.
  </what-built>
  <how-to-verify>
  1. Check the extraction output log for any warnings or errors
  2. Verify the reported statistics look reasonable (total files, sequences, time)
  3. Confirm validation passed with all dimensions correct (1280-dim proteins, 2048-dim merged)
  4. Optionally spot-check a merged file: `python -c "import torch; d=torch.load('data/data_merge/viral.1.1_merged/output_1_1_merged.pt'); print(f'Shape: {d[\"data\"].shape}, Label: {d[\"labels\"]}')"` (adjust path to an actual file)
  5. Verify data_merge directory structure matches what train.py expects
  </how-to-verify>
  <resume-signal>Type "approved" to continue to Phase 5, or describe any issues.</resume-signal>
</task>

</tasks>

<verification>
1. Extraction script ran to completion inside Docker container
2. No errors or dimension mismatches reported
3. All protein embeddings are 1280-dim
4. All merged features are 2048-dim
5. data_merge/ directory contains viral and host merged files
6. Data structure is compatible with train.py's FileBatchDataset
</verification>

<success_criteria>
- TRAIN-01 requirement fulfilled: All training data re-extracted with FastESM2_650 embeddings
- Validation suite passed: all dimensions correct
- Data ready for Phase 5 model training
</success_criteria>

<output>
After completion, create `.planning/phases/04-training-data-preparation/04-02-SUMMARY.md`
</output>
