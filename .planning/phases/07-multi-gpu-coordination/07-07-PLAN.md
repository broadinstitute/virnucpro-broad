---
phase: 07-multi-gpu-coordination
plan: 07
type: execute
wave: 4
depends_on: ["07-06"]
files_modified:
  - virnucpro/pipeline/multi_gpu_inference.py
  - tests/unit/test_multi_gpu_inference.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "run_multi_gpu_inference orchestrates full workflow from FASTA to merged output"
    - "Creates/validates sequence index before spawning workers"
    - "Spawns workers via GPUProcessCoordinator"
    - "Aggregates shards after worker completion"
    - "Returns merged output path and list of failed ranks"
  artifacts:
    - path: "virnucpro/pipeline/multi_gpu_inference.py"
      provides: "run_multi_gpu_inference entry point for multi-GPU processing"
      exports: ["run_multi_gpu_inference"]
    - path: "tests/unit/test_multi_gpu_inference.py"
      provides: "Unit tests for full orchestration flow"
      min_lines: 80
  key_links:
    - from: "run_multi_gpu_inference"
      to: "create_sequence_index"
      via: "Index creation/validation"
      pattern: "create_sequence_index"
    - from: "run_multi_gpu_inference"
      to: "GPUProcessCoordinator"
      via: "Worker spawning"
      pattern: "coordinator\\.spawn_workers"
    - from: "run_multi_gpu_inference"
      to: "aggregate_shards"
      via: "Shard merging"
      pattern: "aggregate_shards"
---

<objective>
Create run_multi_gpu_inference entry point that orchestrates the full workflow.

Purpose: High-level function that takes FASTA files and returns merged embeddings. Handles: index creation, worker spawning, waiting, shard aggregation, and validation. Supports partial failure - returns results from successful workers with warnings about failures.

Output: run_multi_gpu_inference function as the main entry point for Phase 7.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-multi-gpu-coordination/07-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create multi-GPU inference entry point</name>
  <files>virnucpro/pipeline/multi_gpu_inference.py</files>
  <action>
Create new module virnucpro/pipeline/multi_gpu_inference.py:

1. run_multi_gpu_inference(
     fasta_files: List[Path],
     output_dir: Path,
     model_config: Dict,
     world_size: Optional[int] = None,
     timeout: Optional[float] = None
   ) -> Tuple[Path, List[int]]:
   """
   Run multi-GPU inference with fault tolerance.

   Orchestrates:
   1. Create/validate sequence index
   2. Spawn GPU workers via GPUProcessCoordinator
   3. Wait for worker completion
   4. Aggregate shards from successful workers
   5. Validate output completeness

   Args:
       fasta_files: List of FASTA file paths to process
       output_dir: Directory for outputs (index, shards, logs, merged result)
       model_config: Dict with 'model_type', 'model_name', 'token_budget' etc.
       world_size: Number of GPUs (default: torch.cuda.device_count())
       timeout: Max seconds to wait per worker (default: None = infinite)

   Returns:
       Tuple of (merged_output_path, failed_ranks)
       - merged_output_path: Path to merged embeddings.h5
       - failed_ranks: List of worker ranks that failed (empty if all succeeded)

   Raises:
       RuntimeError: If no workers completed successfully
       ValueError: If output validation fails (missing/duplicate sequences)
   """

   - output_dir = Path(output_dir)
   - output_dir.mkdir(parents=True, exist_ok=True)

   - # Auto-detect world_size if not specified
   - if world_size is None:
     - world_size = torch.cuda.device_count()
   - logger.info(f"Starting multi-GPU inference: {world_size} GPUs")

   - # Step 1: Create/validate sequence index
   - index_path = output_dir / "sequence_index.json"
   - create_sequence_index(fasta_files, index_path)

   - # Load expected sequence IDs for validation
   - index_data = load_sequence_index(index_path)
   - expected_ids = {s['sequence_id'] for s in index_data['sequences']}
   - logger.info(f"Index: {len(expected_ids)} sequences, {index_data['total_tokens']} tokens")

   - # Step 2: Spawn workers
   - coordinator = GPUProcessCoordinator(world_size, output_dir)
   - coordinator.spawn_workers(
       gpu_worker,
       (index_path, output_dir, model_config)
     )

   - # Step 3: Wait for completion
   - completion_status = coordinator.wait_for_completion(timeout=timeout)

   - # Identify successful and failed workers
   - successful_ranks = [r for r, ok in completion_status.items() if ok]
   - failed_ranks = [r for r, ok in completion_status.items() if not ok]

   - if failed_ranks:
     - logger.warning(
         f"Partial failure: {len(failed_ranks)}/{world_size} workers failed: {failed_ranks}\n"
         f"Successful workers: {successful_ranks}"
       )

   - if not successful_ranks:
     - raise RuntimeError("No workers completed successfully. Check logs/worker_*.log")

   - # Step 4: Collect successful shards
   - shard_files = [
       output_dir / f"shard_{rank}.h5"
       for rank in successful_ranks
       if (output_dir / f"shard_{rank}.h5").exists()
     ]

   - if not shard_files:
     - raise RuntimeError("No shard files found from successful workers")

   - # Step 5: Calculate expected IDs for successful workers only
   - if failed_ranks:
     - # Partial validation - only check IDs from successful workers
     - successful_expected = set()
     - for rank in successful_ranks:
       - indices = get_worker_indices(index_path, rank, world_size)
       - for i in indices:
         - successful_expected.add(index_data['sequences'][i]['sequence_id'])
     - expected_for_validation = successful_expected
     - logger.warning(
         f"Validating partial results: {len(successful_expected)} sequences "
         f"(missing {len(expected_ids) - len(successful_expected)} due to worker failures)"
       )
   - else:
     - expected_for_validation = expected_ids

   - # Step 6: Aggregate shards
   - output_path = output_dir / "embeddings.h5"
   - aggregate_shards(shard_files, output_path, expected_for_validation)

   - # Step 7: Report results
   - if failed_ranks:
     - missing_count = len(expected_ids) - len(successful_expected)
     - logger.warning(
         f"Completed with partial results: {len(successful_expected)} sequences embedded\n"
         f"Missing {missing_count} sequences from failed workers: {failed_ranks}\n"
         f"Check logs: {output_dir / 'logs' / 'worker_*.log'}"
       )
   - else:
     - logger.info(f"Multi-GPU inference complete: {output_path}")

   - return output_path, failed_ranks

2. Add convenience function for common case:
   run_esm2_multi_gpu(fasta_files, output_dir, world_size=None) -> Path:
   - Wrapper with ESM-2 defaults
   - Returns only output_path (raises if any failures)

3. Add imports and logger setup
  </action>
  <verify>
```
python -c "from virnucpro.pipeline.multi_gpu_inference import run_multi_gpu_inference; print('OK')"
```
  </verify>
  <done>
- run_multi_gpu_inference orchestrates full workflow
- Creates index, spawns workers, waits, aggregates
- Handles partial failure with warnings
- Returns merged output path and failed ranks
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for orchestration</name>
  <files>tests/unit/test_multi_gpu_inference.py</files>
  <action>
Create tests/unit/test_multi_gpu_inference.py:

Note: Heavy mocking to test orchestration flow without actual GPU workers.

1. Test fixtures:
   - mock_fasta_files: Temp FASTA files
   - temp_output_dir: Temp directory
   - mock_coordinator: Mock GPUProcessCoordinator
   - mock_aggregator: Mock aggregate_shards

2. TestOrchestrationFlow class:
   - test_creates_index_first: Verify create_sequence_index called before spawning
   - test_spawns_correct_workers: Verify spawn_workers called with correct args
   - test_waits_for_completion: Verify wait_for_completion called
   - test_aggregates_successful_shards: Verify aggregate_shards called with successful shards only
   - test_returns_output_path: Returns correct path

3. TestPartialFailureHandling class:
   - test_partial_failure_still_aggregates: Some workers fail, still get results
   - test_partial_failure_returns_failed_ranks: failed_ranks list populated
   - test_partial_failure_validates_partial: Only successful worker IDs validated
   - test_all_workers_fail_raises: RuntimeError when all fail

4. TestWorldSizeDetection class:
   - test_auto_detects_gpu_count: Uses torch.cuda.device_count() when not specified
   - test_respects_explicit_world_size: Uses provided value

5. TestConvenienceFunction class:
   - test_run_esm2_multi_gpu_defaults: Correct model config passed

Use pytest, unittest.mock extensively for isolation.
  </action>
  <verify>
```
pytest tests/unit/test_multi_gpu_inference.py -v
```
All tests pass.
  </verify>
  <done>
- Orchestration flow tests verify correct sequence of operations
- Partial failure handling tested
- Auto world_size detection tested
- All tests pass
  </done>
</task>

</tasks>

<verification>
1. Run unit tests: `pytest tests/unit/test_multi_gpu_inference.py -v`
2. Code review: Verify orchestration sequence is correct
3. Verify partial failure handling preserves successful results
</verification>

<success_criteria>
- run_multi_gpu_inference orchestrates full workflow
- Partial failures return results from successful workers
- All failures raise RuntimeError
- Validation uses appropriate expected IDs (full or partial)
- All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-multi-gpu-coordination/07-07-SUMMARY.md`
</output>
