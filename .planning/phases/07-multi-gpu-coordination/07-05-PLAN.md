---
phase: 07-multi-gpu-coordination
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - virnucpro/pipeline/shard_aggregator.py
  - tests/unit/test_shard_aggregator.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Shards aggregated into single HDF5 output"
    - "Chunk-wise streaming controls memory usage"
    - "Validation detects duplicates and missing sequences"
    - "Partial aggregation works when some workers failed"
  artifacts:
    - path: "virnucpro/pipeline/shard_aggregator.py"
      provides: "aggregate_shards function for HDF5 chunk-wise merge"
      exports: ["aggregate_shards", "validate_shard_completeness"]
    - path: "tests/unit/test_shard_aggregator.py"
      provides: "Unit tests for shard aggregation and validation"
      min_lines: 80
  key_links:
    - from: "aggregate_shards"
      to: "h5py.File"
      via: "Chunk-wise read/write for memory efficiency"
      pattern: "embeddings\\[chunk_start:chunk_end\\]"
    - from: "validate_shard_completeness"
      to: "expected_sequence_ids"
      via: "Set difference for missing/extra IDs"
      pattern: "expected_.*-.*seen"
---

<objective>
Create HDF5 shard aggregation with chunk-wise streaming and validation.

Purpose: After workers complete, aggregate shard_N.h5 files into single embeddings.h5 using chunk-wise streaming to control memory. Validate completeness: no duplicates, no missing sequences. Handle partial aggregation when some workers failed.

Output: aggregate_shards function and validation utilities.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-multi-gpu-coordination/07-CONTEXT.md
@.planning/phases/07-multi-gpu-coordination/07-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shard aggregator module</name>
  <files>virnucpro/pipeline/shard_aggregator.py</files>
  <action>
Create new module virnucpro/pipeline/shard_aggregator.py:

1. Constants:
   - CHUNK_SIZE = 10000  # Sequences per chunk for memory control

2. aggregate_shards(shard_files: List[Path], output_path: Path, expected_sequence_ids: Optional[Set[str]] = None) -> Path:
   """
   Aggregate HDF5 shards into single output with validation.

   Uses chunk-wise streaming to control memory usage. Validates:
   - No duplicate sequence IDs across shards
   - All expected sequence IDs present (if provided)

   Args:
       shard_files: List of shard HDF5 file paths
       output_path: Path for merged output HDF5
       expected_sequence_ids: Optional set of expected IDs for validation

   Returns:
       Path to merged output file

   Raises:
       ValueError: If duplicates found or sequences missing
   """
   - Count total sequences and get embedding dimension from first shard
   - Create output HDF5 with pre-allocated datasets:
     - 'embeddings': (total_sequences, embedding_dim), float32, chunked
     - 'sequence_ids': (total_sequences,), variable-length string
   - Process each shard sequentially, reading in chunks:
     - Read embeddings[chunk_start:chunk_end]
     - Read sequence_ids[chunk_start:chunk_end]
     - Check for duplicates (against seen_ids set)
     - Write to output at write_offset
     - Update write_offset
   - Validate completeness (if expected_sequence_ids provided):
     - missing = expected_sequence_ids - seen_ids
     - If missing: raise ValueError with first 10 missing IDs
     - extra = seen_ids - expected_sequence_ids
     - If extra: log warning (not error - allows extra debugging)
   - Log: "Aggregation complete: {output_path}, {N} sequences"
   - Return output_path

3. validate_shard_completeness(shard_files: List[Path], expected_ids: Set[str]) -> Tuple[Set[str], Set[str]]:
   """
   Check shard files for missing and extra sequence IDs.

   Quick validation without full aggregation - useful for pre-check.

   Returns:
       Tuple of (missing_ids, extra_ids)
   """
   - Collect all sequence_ids from shards (load only IDs, not embeddings)
   - Compute missing = expected - seen
   - Compute extra = seen - expected
   - Return (missing, extra)

4. get_shard_info(shard_path: Path) -> Dict:
   """Get metadata from shard file."""
   - Return: {'num_sequences': N, 'embedding_dim': D, 'sequence_ids_sample': first 5 IDs}

5. Add h5py import and handle context managers properly for exception safety:
   - Always use `with h5py.File(...) as f:` pattern
   - Delete partial output file on exception before re-raising

6. Logger: logger = logging.getLogger('virnucpro.pipeline.shard_aggregator')
  </action>
  <verify>
```
python -c "from virnucpro.pipeline.shard_aggregator import aggregate_shards, validate_shard_completeness; print('OK')"
```
  </verify>
  <done>
- aggregate_shards merges HDF5 shards chunk-wise
- Duplicate detection raises ValueError
- Missing sequence detection with clear error message
- Partial output cleaned up on failure
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for shard aggregation</name>
  <files>tests/unit/test_shard_aggregator.py</files>
  <action>
Create tests/unit/test_shard_aggregator.py:

1. Test fixtures:
   - create_test_shard: Create HDF5 shard with embeddings and sequence_ids
   - temp_output_dir: Temp directory for output files

2. TestAggregateShards class:
   - test_aggregate_two_shards: Merge 2 shards, verify combined output
   - test_chunk_wise_memory: Large shard processed in chunks (mock to verify chunking)
   - test_output_contains_all_embeddings: All embeddings present in output
   - test_output_contains_all_ids: All sequence IDs present in output
   - test_preserves_embedding_values: Embedding values identical after merge

3. TestDuplicateDetection class:
   - test_duplicate_within_shard: Shard with duplicate IDs raises ValueError
   - test_duplicate_across_shards: Same ID in two shards raises ValueError
   - test_duplicate_error_message: Error message includes duplicate ID

4. TestMissingValidation class:
   - test_missing_sequences_raises: Missing expected IDs raises ValueError
   - test_missing_error_lists_ids: Error includes first 10 missing IDs
   - test_extra_sequences_warns: Extra IDs log warning (not error)
   - test_no_expected_ids_skips_validation: None expected_ids skips check

5. TestValidateShardCompleteness class:
   - test_returns_missing_and_extra: Quick validation returns correct sets
   - test_empty_shards: Handles empty shards gracefully

6. TestExceptionSafety class:
   - test_cleanup_on_failure: Partial output deleted on exception

Use pytest, h5py, numpy, tempfile. Create realistic test shards.
  </action>
  <verify>
```
pytest tests/unit/test_shard_aggregator.py -v
```
All tests pass.
  </verify>
  <done>
- Aggregation tests verify merging, chunking, embedding preservation
- Duplicate and missing detection tested
- Exception safety verified
- All tests pass
  </done>
</task>

</tasks>

<verification>
1. Run unit tests: `pytest tests/unit/test_shard_aggregator.py -v`
2. Manual test: Create test shards, aggregate, inspect output HDF5
3. Verify chunk-wise reading controls memory (no full-shard load)
</verification>

<success_criteria>
- aggregate_shards merges HDF5 files chunk-wise
- Duplicate detection with clear error messages
- Missing sequence detection with first 10 IDs listed
- Partial output cleaned up on failure
- All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-multi-gpu-coordination/07-05-SUMMARY.md`
</output>
