---
phase: 07-multi-gpu-coordination
plan: 08
type: execute
wave: 5
depends_on: ["07-07"]
files_modified:
  - tests/integration/test_multi_gpu_integration.py
autonomous: false
user_setup: []

must_haves:
  truths:
    - "Multi-GPU inference produces correct embeddings on real hardware"
    - "Embeddings match single-GPU baseline (cosine similarity >0.999)"
    - "Work distribution is balanced across GPUs (within 10%)"
    - "Throughput scales linearly (4 GPUs = ~3.8x single GPU)"
  artifacts:
    - path: "tests/integration/test_multi_gpu_integration.py"
      provides: "GPU integration tests for multi-GPU pipeline"
      min_lines: 100
  key_links:
    - from: "test_multi_gpu_vs_single_gpu"
      to: "run_multi_gpu_inference"
      via: "End-to-end execution"
      pattern: "run_multi_gpu_inference"
    - from: "test_embedding_equivalence"
      to: "cosine_similarity"
      via: "Numerical validation"
      pattern: "cosine.*0\\.999"
---

<objective>
Create integration tests and validate multi-GPU pipeline on real hardware.

Purpose: Verify the full multi-GPU pipeline produces correct results on actual GPU hardware. Test embedding equivalence with single-GPU baseline, work distribution balance, and throughput scaling.

Output: Integration tests that run on GPU server + human verification of scaling.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-multi-gpu-coordination/07-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create multi-GPU integration tests</name>
  <files>tests/integration/test_multi_gpu_integration.py</files>
  <action>
Create tests/integration/test_multi_gpu_integration.py:

1. Pytest markers and skip conditions:
   ```python
   import pytest
   import torch

   pytestmark = pytest.mark.skipif(
       not torch.cuda.is_available() or torch.cuda.device_count() < 2,
       reason="Requires at least 2 GPUs"
   )
   ```

2. Test fixtures:
   - test_fasta_files: Create temp FASTA with 100-1000 sequences of varying lengths
   - temp_output_dir: Temp directory for outputs
   - single_gpu_embeddings: Run single-GPU inference for baseline comparison

3. TestMultiGPUEmbeddingEquivalence class:
   - test_multi_gpu_matches_single_gpu:
     - Run run_multi_gpu_inference with world_size=2 (or device_count)
     - Load resulting embeddings.h5
     - Compare with single-GPU baseline
     - Assert cosine similarity >0.999 for each sequence
     - Log any sequences below threshold for debugging

   - test_all_sequences_present:
     - Verify all input sequence IDs present in output
     - No duplicates

   - test_embedding_shapes_correct:
     - Verify embedding dimension matches model (e.g., 2560 for ESM-2 3B)

4. TestWorkDistribution class:
   - test_stride_distribution_balanced:
     - Check index file distribution metrics
     - Token count per worker within 10% of mean
     - Sequence count evenly distributed

   - test_shard_sizes_similar:
     - After workers complete, check shard file sizes
     - Sizes should be within 15% of mean

5. TestThroughputScaling class:
   - test_multi_gpu_faster_than_single:
     - Time single-GPU on subset (e.g., 100 sequences)
     - Time multi-GPU on same subset
     - Multi-GPU should be significantly faster (>1.5x for 2 GPUs)

   - test_gpu_utilization_high (informational):
     - Run inference, log GPU utilization from monitor
     - Target: >70% utilization

6. TestFaultTolerance class:
   - test_partial_failure_produces_partial_results:
     - Mock one worker to fail (inject via env var or similar)
     - Verify remaining workers produce valid output
     - Verify failed_ranks returned correctly

7. TestEdgeCases class:
   - test_single_gpu_mode: Run with world_size=1, verify works
   - test_small_dataset: Fewer sequences than GPUs handled gracefully
   - test_uneven_distribution: Odd sequence count distributed correctly

Use pytest, h5py, torch, numpy. Create realistic test sequences.
  </action>
  <verify>
Run on GPU server:
```
pytest tests/integration/test_multi_gpu_integration.py -v --tb=short
```
All tests pass (or skip gracefully on insufficient GPUs).
  </verify>
  <done>
- Integration tests verify multi-GPU correctness
- Embedding equivalence with single-GPU baseline
- Work distribution balance verified
- Edge cases covered
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete multi-GPU coordination pipeline:
- Sequence index with stride distribution
- GPUProcessCoordinator for worker lifecycle
- GPU worker function with async inference
- HDF5 shard aggregation
- run_multi_gpu_inference orchestration
- Integration tests
  </what-built>
  <how-to-verify>
1. SSH to GPU server with 4 GPUs

2. Run unit tests (should pass on CPU):
   ```bash
   pytest tests/unit/test_shard_index.py tests/unit/test_gpu_coordinator.py tests/unit/test_shard_aggregator.py tests/unit/test_gpu_worker.py tests/unit/test_multi_gpu_inference.py -v
   ```

3. Run integration tests on GPU:
   ```bash
   pytest tests/integration/test_multi_gpu_integration.py -v
   ```

4. Verify throughput scaling (manual test):
   - Process 1000 sequences on 1 GPU, note time
   - Process same 1000 sequences on 4 GPUs, note time
   - Expect ~3.5-4x speedup

5. Check worker logs:
   ```bash
   ls output_dir/logs/
   cat output_dir/logs/worker_0.log | tail -20
   ```
   Verify per-worker logs are separate and readable.

6. Inspect merged output:
   ```bash
   python -c "import h5py; f = h5py.File('output_dir/embeddings.h5'); print(f['embeddings'].shape, len(f['sequence_ids']))"
   ```
   Verify embedding count matches input sequence count.
  </how-to-verify>
  <resume-signal>Type "verified" with observations, or describe issues for next phase</resume-signal>
</task>

</tasks>

<verification>
1. Unit tests pass on CPU
2. Integration tests pass on GPU server
3. Embedding equivalence confirmed (cosine similarity >0.999)
4. Throughput scaling verified (~4x with 4 GPUs)
5. Per-worker logs separate and readable
</verification>

<success_criteria>
- All unit and integration tests pass
- Multi-GPU embeddings match single-GPU baseline
- Throughput scales linearly with GPU count
- Work distribution balanced across GPUs
- Partial failure handling works correctly
</success_criteria>

<output>
After completion, create `.planning/phases/07-multi-gpu-coordination/07-08-SUMMARY.md`
</output>
