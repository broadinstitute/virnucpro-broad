---
phase: 07-multi-gpu-coordination
plan: 06
type: execute
wave: 3
depends_on: ["07-02", "07-03", "07-04", "07-05"]
files_modified:
  - virnucpro/pipeline/gpu_worker.py
  - tests/unit/test_gpu_worker.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Worker sets up per-worker logging before any other operations"
    - "Worker loads model and creates dataloader with assigned indices"
    - "Worker runs AsyncInferenceRunner and saves HDF5 shard"
    - "Worker reports status (complete/failed) to results queue"
  artifacts:
    - path: "virnucpro/pipeline/gpu_worker.py"
      provides: "gpu_worker function for single-GPU shard processing"
      exports: ["gpu_worker"]
    - path: "tests/unit/test_gpu_worker.py"
      provides: "Unit tests for worker function (mocked GPU)"
      min_lines: 60
  key_links:
    - from: "gpu_worker"
      to: "setup_worker_logging"
      via: "First action in worker"
      pattern: "setup_worker_logging.*rank"
    - from: "gpu_worker"
      to: "AsyncInferenceRunner.run"
      via: "Inference loop with DataLoader"
      pattern: "runner\\.run.*dataloader"
    - from: "gpu_worker"
      to: "h5py.File"
      via: "Shard output"
      pattern: "shard_.*\\.h5"
---

<objective>
Create GPU worker function that runs async inference and saves HDF5 shard.

Purpose: The worker function is spawned by GPUProcessCoordinator. It sets up logging, loads the model, creates an IndexBasedDataset with assigned indices, runs AsyncInferenceRunner, and saves results to shard_{rank}.h5. Reports completion status to parent.

Output: gpu_worker function that integrates all Phase 5/6/7 components.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-multi-gpu-coordination/07-CONTEXT.md
@virnucpro/pipeline/async_inference.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPU worker function</name>
  <files>virnucpro/pipeline/gpu_worker.py</files>
  <action>
Create new module virnucpro/pipeline/gpu_worker.py:

1. gpu_worker(rank: int, world_size: int, results_queue: Queue, index_path: Path, output_dir: Path, model_config: Dict):
   """
   Independent GPU worker for shard processing.

   Spawned by GPUProcessCoordinator. Each worker:
   1. Sets up per-worker logging
   2. Loads sequence index and gets assigned indices
   3. Loads model (ESM-2 or DNABERT-S)
   4. Creates IndexBasedDataset and DataLoader
   5. Runs AsyncInferenceRunner
   6. Saves shard HDF5 file
   7. Reports completion to parent

   Args:
       rank: Worker rank (0 to world_size-1)
       world_size: Total number of workers
       results_queue: Queue for reporting status to parent
       index_path: Path to sequence index JSON
       output_dir: Directory for shard output and logs
       model_config: Dict with 'model_type', 'model_name', etc.

   Note:
       CUDA_VISIBLE_DEVICES already set by parent.
       Worker sees device 0 which maps to actual GPU {rank}.
   """

   - # Step 1: Setup logging FIRST (before any other operations)
   - log_dir = output_dir / "logs"
   - log_file = setup_worker_logging(rank, log_dir)
   - logger = logging.getLogger(f"gpu_worker_{rank}")
   - logger.info(f"Worker {rank}/{world_size} starting")

   - try:
     - # Step 2: Initialize CUDA
     - device = torch.device('cuda:0')  # Always 0 due to CUDA_VISIBLE_DEVICES
     - torch.cuda.set_device(device)
     - logger.info(f"CUDA device: {torch.cuda.get_device_name(0)}")

     - # Step 3: Load sequence index and get assigned indices
     - indices = get_worker_indices(index_path, rank, world_size)
     - logger.info(f"Assigned {len(indices)} sequences")

     - # Step 4: Load model
     - if model_config['model_type'] == 'esm2':
       - from virnucpro.models.esm2_flash import load_esm2_model
       - model, batch_converter = load_esm2_model(
           model_name=model_config.get('model_name', 'esm2_t36_3B_UR50D'),
           device=str(device)
         )
     - else:
       - raise ValueError(f"Unsupported model type: {model_config['model_type']}")

     - # Step 5: Create dataset and dataloader
     - dataset = IndexBasedDataset(index_path, indices)
     - collator = VarlenCollator(batch_converter)
     - dataloader = create_async_dataloader(
         dataset, collator,
         device_id=0,  # Always 0 due to CUDA_VISIBLE_DEVICES remapping
         token_budget=model_config.get('token_budget', None)
       )

     - # Step 6: Run async inference
     - runner = AsyncInferenceRunner(model, device)
     - all_embeddings = []
     - all_ids = []

     - for result in runner.run(dataloader):
       - all_embeddings.append(result.embeddings)
       - all_ids.extend(result.sequence_ids)

     - logger.info(f"Inference complete: {len(all_ids)} sequences")

     - # Step 7: Save shard HDF5
     - shard_path = output_dir / f"shard_{rank}.h5"
     - with h5py.File(shard_path, 'w') as f:
       - embeddings = torch.cat(all_embeddings, dim=0).numpy()
       - f.create_dataset('embeddings', data=embeddings)
       - dt = h5py.special_dtype(vlen=str)
       - f.create_dataset('sequence_ids', data=all_ids, dtype=dt)

     - logger.info(f"Shard saved: {shard_path} ({len(all_ids)} sequences)")

     - # Step 8: Report success
     - results_queue.put({
         'rank': rank,
         'status': 'complete',
         'shard_path': str(shard_path),
         'num_sequences': len(all_ids)
       })

   - except Exception as e:
     - logger.exception(f"Worker {rank} failed: {e}")
     - results_queue.put({
         'rank': rank,
         'status': 'failed',
         'error': str(e)
       })
     - sys.exit(1)

2. Add imports:
   - import sys, logging, torch, h5py
   - from pathlib import Path
   - from multiprocessing import Queue
   - from virnucpro.pipeline.worker_logging import setup_worker_logging
   - from virnucpro.data.shard_index import get_worker_indices
   - from virnucpro.data.sequence_dataset import IndexBasedDataset
   - from virnucpro.data.collators import VarlenCollator
   - from virnucpro.data.dataloader_utils import create_async_dataloader
   - from virnucpro.pipeline.async_inference import AsyncInferenceRunner
  </action>
  <verify>
```
python -c "from virnucpro.pipeline.gpu_worker import gpu_worker; print('OK')"
```
  </verify>
  <done>
- gpu_worker sets up logging first
- Loads model, creates dataset/dataloader with assigned indices
- Runs AsyncInferenceRunner, saves HDF5 shard
- Reports status to results queue
- Handles exceptions with proper logging and exit code
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for GPU worker</name>
  <files>tests/unit/test_gpu_worker.py</files>
  <action>
Create tests/unit/test_gpu_worker.py:

Note: These tests mock CUDA and model loading for CPU-only testing.

1. Test fixtures:
   - mock_model: Mock model with forward_packed that returns fake embeddings
   - mock_batch_converter: Mock batch converter
   - temp_index: Create temp index with test sequences
   - temp_output_dir: Temp directory for outputs
   - mock_results_queue: multiprocessing.Queue for testing

2. TestGPUWorkerFlow class (with extensive mocking):
   - test_worker_sets_up_logging: Verify setup_worker_logging called first
   - test_worker_gets_assigned_indices: Verify get_worker_indices called with rank
   - test_worker_creates_dataset_with_indices: IndexBasedDataset created with correct indices
   - test_worker_saves_hdf5_shard: Output shard file created with correct name
   - test_worker_reports_success: results_queue receives complete status
   - test_worker_reports_failure: Exception leads to failed status in queue

3. TestWorkerErrorHandling class:
   - test_model_load_failure: Model load error reported properly
   - test_inference_failure: Inference error reported properly
   - test_shard_save_failure: Disk error reported properly

4. Mock strategy:
   - Mock torch.cuda.* to avoid GPU requirement
   - Mock load_esm2_model to return mock model
   - Mock AsyncInferenceRunner to yield fake results
   - Keep shard_index and logging as real implementations

Use pytest, unittest.mock, multiprocessing.Queue, tempfile.
  </action>
  <verify>
```
pytest tests/unit/test_gpu_worker.py -v
```
All tests pass.
  </verify>
  <done>
- Worker flow tests verify logging, index loading, shard saving
- Error handling tests verify proper failure reporting
- All tests pass with mocked GPU dependencies
  </done>
</task>

</tasks>

<verification>
1. Run unit tests: `pytest tests/unit/test_gpu_worker.py -v`
2. Verify worker function imports all required components
3. Code review: Verify logging is first operation in worker
</verification>

<success_criteria>
- gpu_worker integrates all Phase 5/6/7 components
- Logging set up before any other operations
- HDF5 shard saved with embeddings and sequence_ids
- Status reported to results queue (complete/failed)
- All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-multi-gpu-coordination/07-06-SUMMARY.md`
</output>
