---
phase: 07-multi-gpu-coordination
plan: 04
type: execute
wave: 2
depends_on: ["07-01", "07-03"]
files_modified:
  - virnucpro/pipeline/gpu_coordinator.py
  - tests/unit/test_gpu_coordinator.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "GPUProcessCoordinator spawns independent processes (1 per GPU)"
    - "Each worker sees only its assigned GPU via CUDA_VISIBLE_DEVICES"
    - "Partial failure allows other workers to complete"
    - "Results queue collects worker status (complete/failed)"
  artifacts:
    - path: "virnucpro/pipeline/gpu_coordinator.py"
      provides: "GPUProcessCoordinator class for multi-GPU orchestration"
      exports: ["GPUProcessCoordinator"]
    - path: "tests/unit/test_gpu_coordinator.py"
      provides: "Unit tests for coordinator and worker spawning"
      min_lines: 80
  key_links:
    - from: "GPUProcessCoordinator.spawn_workers"
      to: "multiprocessing.Process"
      via: "CUDA-safe spawn context"
      pattern: "ctx\\.Process"
    - from: "GPUProcessCoordinator"
      to: "CUDA_VISIBLE_DEVICES"
      via: "Environment variable per worker"
      pattern: "CUDA_VISIBLE_DEVICES.*str.*rank"
---

<objective>
Create GPUProcessCoordinator for multi-GPU worker lifecycle management.

Purpose: Parent orchestrator spawns GPU workers using multiprocessing.Process (not mp.spawn) for fault tolerance. Each worker gets CUDA_VISIBLE_DEVICES set before spawning so it sees only its assigned GPU. Allows partial failure - surviving workers complete and produce partial results.

Output: GPUProcessCoordinator with spawn_workers and wait_for_completion methods.
</objective>

<execution_context>
@/home/unix/carze/.claude/get-shit-done/workflows/execute-plan.md
@/home/unix/carze/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-multi-gpu-coordination/07-CONTEXT.md
@.planning/phases/07-multi-gpu-coordination/07-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPUProcessCoordinator class</name>
  <files>virnucpro/pipeline/gpu_coordinator.py</files>
  <action>
Create new module virnucpro/pipeline/gpu_coordinator.py:

1. GPUProcessCoordinator class:
   ```python
   class GPUProcessCoordinator:
       """
       SPMD coordinator for multi-GPU processing with fault tolerance.

       Unlike mp.spawn which kills all workers on any failure, this coordinator
       uses multiprocessing.Process directly for independent worker lifecycle.
       Allows partial completion - surviving workers finish even if others fail.

       Attributes:
           world_size: Number of GPU workers to spawn
           output_dir: Directory for worker outputs and logs
           ctx: Multiprocessing context (spawn for CUDA safety)
           workers: Dict[rank, Process] mapping
           results_queue: Queue for worker status reporting
       """
   ```

   - __init__(self, world_size: int, output_dir: Path):
     - self.world_size = world_size
     - self.output_dir = Path(output_dir)
     - self.ctx = multiprocessing.get_context('spawn')
     - self.workers: Dict[int, mp.Process] = {}
     - self.results_queue = self.ctx.Queue()

   - spawn_workers(self, worker_fn: Callable, worker_args: Tuple):
     """
     Spawn independent GPU worker processes.

     Args:
         worker_fn: Worker function with signature:
             (rank, world_size, results_queue, *worker_args)
         worker_args: Additional args passed to each worker

     CRITICAL: CUDA_VISIBLE_DEVICES must be set in env BEFORE spawning.
     Each worker sees device 0 which maps to actual GPU {rank}.
     """
     - for rank in range(self.world_size):
       - env = os.environ.copy()
       - env['CUDA_VISIBLE_DEVICES'] = str(rank)  # Worker sees only this GPU
       - p = self.ctx.Process(
           target=worker_fn,
           args=(rank, self.world_size, self.results_queue, *worker_args),
           name=f"gpu_worker_{rank}"
         )
       - p.start()
       - self.workers[rank] = p
       - logger.info(f"Spawned worker {rank} (CUDA_VISIBLE_DEVICES={rank})")

   - wait_for_completion(self, timeout: Optional[float] = None) -> Dict[int, bool]:
     """
     Wait for workers with timeout, returns completion status per rank.

     Args:
         timeout: Max seconds to wait per worker (None = infinite)

     Returns:
         Dict mapping rank -> success (True) or failure (False)
     """
     - results = {}
     - for rank, process in self.workers.items():
       - process.join(timeout=timeout)
       - if process.is_alive():
         - logger.warning(f"Worker {rank} timed out, still running")
         - results[rank] = False
       - elif process.exitcode != 0:
         - logger.error(f"Worker {rank} failed with exit code {process.exitcode}")
         - results[rank] = False
       - else:
         - logger.info(f"Worker {rank} completed successfully")
         - results[rank] = True
     - return results

   - collect_results(self) -> List[Dict]:
     """Drain results queue and return all worker reports."""
     - results = []
     - while not self.results_queue.empty():
       - try:
         - results.append(self.results_queue.get_nowait())
       - except queue.Empty:
         - break
     - return results

   - terminate_all(self):
     """Force terminate all workers (emergency cleanup)."""
     - for rank, process in self.workers.items():
       - if process.is_alive():
         - logger.warning(f"Terminating worker {rank}")
         - process.terminate()
         - process.join(timeout=5)

2. Add proper imports: multiprocessing, os, logging, pathlib, typing, queue

3. Add logger: logger = logging.getLogger('virnucpro.pipeline.gpu_coordinator')
  </action>
  <verify>
```
python -c "from virnucpro.pipeline.gpu_coordinator import GPUProcessCoordinator; print('OK')"
```
  </verify>
  <done>
- GPUProcessCoordinator spawns workers with spawn context
- CUDA_VISIBLE_DEVICES set per worker before spawn
- wait_for_completion returns per-rank status
- Partial failure allows other workers to complete
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for GPUProcessCoordinator</name>
  <files>tests/unit/test_gpu_coordinator.py</files>
  <action>
Create tests/unit/test_gpu_coordinator.py:

1. Test fixtures:
   - temp_output_dir: Temp directory for outputs
   - mock_worker_fn: Simple worker that writes to file and reports to queue

2. TestGPUProcessCoordinator class:
   - test_spawn_workers_creates_processes: Verify N processes created
   - test_cuda_visible_devices_set: Mock env check, verify each worker gets different CUDA_VISIBLE_DEVICES
   - test_wait_for_completion_all_success: All workers complete, returns all True
   - test_wait_for_completion_partial_failure: One worker fails, others True, failed one False
   - test_results_queue_receives_reports: Workers send status to queue, coordinator collects
   - test_terminate_all: Stuck workers can be force-terminated

3. TestWorkerIsolation class:
   - test_workers_independent: Failure of one doesn't affect others (simulated)
   - test_spawn_context_is_spawn: Verify ctx is 'spawn' not 'fork'

4. Mock worker functions for testing:
   - successful_worker: Sleep briefly, report success
   - failing_worker: Raise exception, report failure
   - slow_worker: Sleep longer than timeout

Use pytest, multiprocessing, tempfile. Use short timeouts for fast tests.
  </action>
  <verify>
```
pytest tests/unit/test_gpu_coordinator.py -v
```
All tests pass.
  </verify>
  <done>
- Coordinator tests verify spawning, CUDA_VISIBLE_DEVICES, completion tracking
- Partial failure and independence tested
- All tests pass
  </done>
</task>

</tasks>

<verification>
1. Run unit tests: `pytest tests/unit/test_gpu_coordinator.py -v`
2. Verify spawn context used (not fork) for CUDA safety
3. Manual test: Spawn mock workers, verify independent completion
</verification>

<success_criteria>
- GPUProcessCoordinator spawns independent workers
- CUDA_VISIBLE_DEVICES set correctly per worker
- Partial failure allows successful workers to complete
- Results queue collects worker status reports
- All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-multi-gpu-coordination/07-04-SUMMARY.md`
</output>
