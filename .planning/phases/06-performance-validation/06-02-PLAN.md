---
phase: 06-performance-validation
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - tests/benchmarks/test_scaling.py
  - tests/benchmarks/test_stage_throughput.py
autonomous: true

must_haves:
  truths:
    - "Scaling benchmarks measure speedup across 1, 2, 4, 8 GPU configurations"
    - "Per-stage benchmarks identify bottlenecks in translation, DNABERT, ESM-2, merge"
    - "Near-linear scaling verified (2 GPUs ≈ 1.8x, 4 GPUs ≈ 3.5x)"
  artifacts:
    - path: "tests/benchmarks/test_scaling.py"
      provides: "Multi-GPU scaling tests"
      min_lines: 200
    - path: "tests/benchmarks/test_stage_throughput.py"
      provides: "Per-stage performance tests"
      min_lines: 150
  key_links:
    - from: "tests/benchmarks/test_scaling.py"
      to: "virnucpro.cli.predict"
      via: "subprocess calls to test CLI"
      pattern: "subprocess\\.run.*virnucpro predict"
---

<objective>
Implement GPU scaling validation and per-stage throughput benchmarks to verify linear speedup and identify bottlenecks.

Purpose: Validate that multi-GPU optimizations deliver expected scaling (2x GPUs = ~1.8x speed)
Output: Scaling benchmarks proving near-linear GPU utilization
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-performance-validation/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPU scaling benchmarks</name>
  <files>tests/benchmarks/test_scaling.py</files>
  <action>
    Implement multi-GPU scaling validation tests:

    1. Create TestGPUScaling class with pytest-benchmark:
       - Use @pytest.mark.gpu decorator (skip if no GPU)
       - Parametrize tests with GPU counts: [1, 2, 4, 8]
       - Skip configurations if insufficient GPUs available

    2. Add test_linear_scaling_synthetic():
       - Generate medium-sized synthetic dataset (1000 sequences)
       - Run pipeline with --gpus flag for each configuration
       - Measure end-to-end time using subprocess with timing
       - Calculate speedup ratios relative to single GPU
       - Assert near-linear scaling (within thresholds):
         * 2 GPUs: 1.6-2.0x speedup
         * 4 GPUs: 3.0-4.0x speedup
         * 8 GPUs: 6.0-8.0x speedup

    3. Add test_scaling_with_persistent_models():
       - Test scaling with --persistent-models flag
       - Verify additional speedup from eliminating model reload
       - Compare speedup ratios with/without persistent models

    4. Create benchmark report generation:
       - Collect timing data for all GPU configurations
       - Generate scaling plot data (GPU count vs speedup)
       - Output markdown table with results
       - Save JSON for CI regression tracking

    Use torch.cuda.device_count() to detect available GPUs.
    Use subprocess.run() with time.time() for end-to-end timing.
  </action>
  <verify>python -m pytest tests/benchmarks/test_scaling.py::TestGPUScaling -v --collect-only</verify>
  <done>GPU scaling benchmarks measure speedup across multiple GPU configurations</done>
</task>

<task type="auto">
  <name>Task 2: Implement per-stage throughput benchmarks</name>
  <files>tests/benchmarks/test_stage_throughput.py</files>
  <action>
    Create per-stage performance benchmarks to identify bottlenecks:

    1. Create TestStageThroughput class:
       - Test each pipeline stage independently
       - Measure sequences/second throughput
       - Track GPU utilization during each stage

    2. Add stage-specific benchmarks:
       - test_translation_throughput(): CPU parallelization performance
       - test_dnabert_throughput(): Multi-GPU DNABERT-S extraction
       - test_esm2_throughput(): Multi-GPU ESM-2 extraction
       - test_merge_throughput(): Parallel embedding merge
       - test_prediction_throughput(): Final prediction stage

    3. For each stage benchmark:
       - Use synthetic data (small dataset for quick tests)
       - Run stage in isolation using module imports
       - Measure: total time, sequences/sec, GPU utilization %
       - Compare optimized vs vanilla performance (if available)

    4. Add bottleneck identification:
       - Calculate percentage of total time per stage
       - Identify stages consuming >30% of pipeline time
       - Flag stages with <80% GPU utilization (for GPU stages)
       - Generate bottleneck report with recommendations

    5. Create throughput comparison table:
       - Stage name, duration, sequences/sec, GPU util %
       - Highlight bottlenecks in red (using markdown/HTML)
       - Include memory usage per stage

    Import stage functions directly (not via CLI) for isolated testing.
    Use gpu_monitor fixture from conftest for utilization tracking.
  </action>
  <verify>python -m pytest tests/benchmarks/test_stage_throughput.py -v -k "translation" --tb=short</verify>
  <done>Per-stage benchmarks identify bottlenecks and measure throughput for each pipeline component</done>
</task>

</tasks>

<verification>
- Scaling benchmarks run on 1, 2, 4, 8 GPU configurations
- Speedup ratios calculated and validated against thresholds
- Per-stage throughput measured in sequences/second
- Bottlenecks identified based on time percentage and GPU utilization
- Reports generated in markdown and JSON formats
</verification>

<success_criteria>
- Multi-GPU scaling shows near-linear speedup (1.8x for 2 GPUs, 3.5x for 4 GPUs)
- Per-stage benchmarks identify which stages are bottlenecks
- GPU utilization tracked for each stage
- Throughput measured in sequences/second for performance tracking
</success_criteria>

<output>
After completion, create `.planning/phases/06-performance-validation/06-02-SUMMARY.md`
</output>