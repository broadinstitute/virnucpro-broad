---
phase: 06-performance-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/benchmarks/__init__.py
  - tests/benchmarks/conftest.py
  - tests/benchmarks/data_generator.py
  - tests/benchmarks/utils.py
  - virnucpro/utils/gpu_monitor.py
autonomous: true

must_haves:
  truths:
    - "Benchmark infrastructure can generate synthetic test data of varying sizes"
    - "GPU monitoring captures real-time utilization and memory usage"
    - "Test configurations support 1, 2, 4, 8 GPU setups"
  artifacts:
    - path: "tests/benchmarks/data_generator.py"
      provides: "Synthetic FASTA generation"
      min_lines: 100
    - path: "tests/benchmarks/conftest.py"
      provides: "Pytest benchmark fixtures"
      exports: ["benchmark_config", "gpu_monitor"]
    - path: "tests/benchmarks/utils.py"
      provides: "Benchmark utilities"
      exports: ["BenchmarkTimer", "format_results"]
  key_links:
    - from: "tests/benchmarks/conftest.py"
      to: "virnucpro.utils.gpu_monitor"
      via: "import for GPU monitoring"
      pattern: "from virnucpro\\.utils\\.gpu_monitor import"
    - from: "tests/benchmarks/conftest.py"
      to: "tests.benchmarks.data_generator"
      via: "fixture imports generate_synthetic_fasta"
      pattern: "from tests\\.benchmarks\\.data_generator import generate_synthetic_fasta"
---

<objective>
Create benchmark infrastructure with synthetic data generation, GPU monitoring, and multi-GPU test configurations.

Purpose: Establish foundation for performance validation and regression testing
Output: Benchmark framework ready for scaling and performance tests
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-performance-validation/06-CONTEXT.md
@.planning/phases/06-performance-validation/06-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark infrastructure and fixtures</name>
  <files>tests/benchmarks/__init__.py, tests/benchmarks/conftest.py, tests/benchmarks/utils.py</files>
  <action>
    Create benchmark infrastructure with pytest-benchmark integration:

    1. In tests/benchmarks/__init__.py:
       - Add package docstring explaining benchmark suite structure

    2. In tests/benchmarks/conftest.py:
       - Create pytest fixtures for benchmark configuration
       - Add fixture for GPU availability detection (skip tests if no GPU)
       - Create gpu_monitor fixture using enhanced GPU monitoring from virnucpro.utils.gpu_monitor.NvitopMonitor
       - Import generate_synthetic_fasta from tests.benchmarks.data_generator for data generation fixtures
       - Add benchmark_dir fixture for test data and reports
       - Configure pytest-benchmark settings (warmup, iterations)

    3. In tests/benchmarks/utils.py:
       - Create BenchmarkTimer class wrapping torch.utils.benchmark.Timer
       - Add GPU memory tracking utilities
       - Create format_results() for markdown/JSON output
       - Add scaling calculation utilities (speedup ratios)
       - Create test configuration helper for multi-GPU setups

    Use torch.utils.benchmark for accurate CUDA timing (handles synchronization automatically).
    Import NvitopMonitor from virnucpro.utils.gpu_monitor for enhanced monitoring.
  </action>
  <verify>python -m pytest tests/benchmarks/conftest.py --collect-only</verify>
  <done>Benchmark fixtures and utilities created, pytest can collect benchmark tests</done>
</task>

<task type="auto">
  <name>Task 2: Implement synthetic data generator</name>
  <files>tests/benchmarks/data_generator.py</files>
  <action>
    Create synthetic test data generator for controlled benchmarks:

    1. Create generate_synthetic_fasta() function:
       - Parameters: num_sequences, min_length, max_length, output_path
       - Generate viral-like DNA sequences with realistic length distributions
       - Use BioPython for FASTA file writing

    2. Add preset configurations for standard test sizes:
       - TINY: 10-50 sequences (quick tests)
       - SMALL: 100-500 sequences (CI tests)
       - MEDIUM: 1000-5000 sequences (standard benchmarks)
       - LARGE: 10000+ sequences (stress tests)

    3. Create generate_benchmark_dataset() wrapper:
       - Creates directory structure: tests/data/synthetic/{size}/
       - Generates multiple FASTA files for multi-file benchmarks
       - Tracks metadata (sequence counts, total size) in JSON

    4. Add real_sample_loader() function:
       - Loads real viral samples from tests/data/small_real/ if available
       - Returns None if no real data (allows benchmarks to run without)

    Generated data should be gitignored but reproducible via seed parameter.
  </action>
  <verify>python -c "from tests.benchmarks.data_generator import generate_synthetic_fasta; generate_synthetic_fasta(10, 100, 500, '/tmp/test.fasta'); import os; print(f'File created: {os.path.exists(\"/tmp/test.fasta\")}')"</verify>
  <done>Synthetic data generator creates FASTA files with configurable sequence counts and lengths</done>
</task>

<task type="auto">
  <name>Task 3: Enhance GPU monitoring for benchmarks</name>
  <files>virnucpro/utils/gpu_monitor.py</files>
  <action>
    Enhance existing GPU monitor with nvitop integration for benchmarks:

    1. Add NvitopMonitor class (keep existing GPUMonitor for backward compatibility):
       - Use nvitop Python API for programmatic GPU metrics access
       - Track: utilization %, memory used/total, temperature, power draw
       - Support multi-GPU monitoring with per-GPU metrics
       - Add start_monitoring() and stop_monitoring() with threading
       - Write metrics to log file: logs/gpu_metrics_{timestamp}.log for MON-01 requirement

    2. Add metric aggregation methods:
       - get_average_utilization() across time window
       - get_peak_memory_usage() per GPU
       - get_throughput_metrics() (sequences/sec calculation)
       - export_metrics() for JSON/markdown output
       - save_logs() to write utilization and memory logs to file

    3. Create fallback to nvidia-smi if nvitop not available:
       - Try import nvitop, catch ImportError
       - Fall back to existing nvidia-smi parsing approach
       - Log warning about reduced monitoring capabilities

    4. Add benchmark-specific tracking:
       - Stage transitions (translation, DNABERT, ESM-2, merge)
       - Per-stage timing and GPU utilization
       - Idle time detection between stages
       - Log all metrics with timestamps for MON-01 compliance

    Keep changes backward compatible - existing code using GPUMonitor should still work.
    Ensure logs directory is created if it doesn't exist.
  </action>
  <verify>python -c "from virnucpro.utils.gpu_monitor import GPUMonitor; monitor = GPUMonitor(); print('GPU Monitor loads successfully')"</verify>
  <done>GPU monitoring enhanced with nvitop support, benchmark-specific metrics, and logging capability</done>
</task>

</tasks>

<verification>
- Benchmark infrastructure created with pytest fixtures
- Synthetic data generator produces FASTA files of varying sizes
- GPU monitoring captures utilization and memory metrics with logging
- All components integrate for multi-GPU benchmark testing
</verification>

<success_criteria>
- pytest collects benchmark fixtures without errors
- Synthetic FASTA files generated with specified sequence counts
- GPU monitor tracks metrics and writes logs (falls back gracefully if no GPU)
- Infrastructure ready for scaling and performance tests
</success_criteria>

<output>
After completion, create `.planning/phases/06-performance-validation/06-01-SUMMARY.md`
</output>