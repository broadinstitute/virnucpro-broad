---
phase: 06-performance-validation
plan: 04
type: execute
wave: 3
depends_on: ["06-01", "06-02", "06-03"]
files_modified:
  - tests/benchmarks/test_vanilla_equivalence.py
  - tests/benchmarks/vanilla_baseline.py
autonomous: true

must_haves:
  truths:
    - "Optimized pipeline produces identical predictions to vanilla implementation"
    - "Numerical differences stay within BF16/FP32 tolerance (rtol=1e-3)"
    - "All intermediate outputs validated for correctness"
  artifacts:
    - path: "tests/benchmarks/test_vanilla_equivalence.py"
      provides: "Output equivalence validation"
      min_lines: 200
    - path: "tests/benchmarks/vanilla_baseline.py"
      provides: "Vanilla pipeline runner"
      min_lines: 100
  key_links:
    - from: "tests/benchmarks/test_vanilla_equivalence.py"
      to: "numpy.allclose"
      via: "numerical comparison"
      pattern: "np\\.allclose.*rtol=1e-3"
---

<objective>
Validate that optimized pipeline produces equivalent results to vanilla implementation within acceptable numerical tolerance.

Purpose: Ensure optimizations don't compromise prediction accuracy
Output: Correctness validation proving optimizations maintain output quality
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-performance-validation/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create vanilla baseline runner</name>
  <files>tests/benchmarks/vanilla_baseline.py</files>
  <action>
    Implement vanilla pipeline runner for comparison baseline:

    1. Create VanillaRunner class:
       - Run pipeline with all optimizations disabled
       - Use single GPU, no parallel processing
       - Disable FlashAttention, BF16, persistent models
       - Match original 45-hour baseline configuration

    2. Add run_vanilla_pipeline() function:
       - Parameters: input_dir, output_dir, gpu_id=0
       - Run with minimal flags: just --input and --output
       - Capture all intermediate outputs for comparison
       - Return paths to all generated files

    3. Add generate_reference_outputs():
       - Run vanilla pipeline on small test dataset
       - Save outputs to tests/data/reference/
       - Include: predictions, embeddings, consensus files
       - Generate checksums for validation

    4. Create baseline comparison utilities:
       - load_predictions() to parse CSV files
       - load_embeddings() to load .pt files
       - compare_files() for basic validation
       - get_baseline_timings() for performance comparison

    This provides the "ground truth" for correctness validation.
    Should work even with old/unoptimized code paths.
  </action>
  <verify>python -c "from tests.benchmarks.vanilla_baseline import VanillaRunner; runner = VanillaRunner(); print('Vanilla runner created')"</verify>
  <done>Vanilla baseline runner provides reference implementation for comparison</done>
</task>

<task type="auto">
  <name>Task 2: Implement equivalence validation tests</name>
  <files>tests/benchmarks/test_vanilla_equivalence.py</files>
  <action>
    Create comprehensive equivalence tests comparing optimized vs vanilla:

    1. Create TestVanillaEquivalence class:
       - Compare outputs at multiple pipeline stages
       - Use numpy.allclose with appropriate tolerances
       - Generate detailed difference reports

    2. Add test_prediction_equivalence():
       - Compare final prediction CSV files
       - Check: file paths, prediction labels, confidence scores
       - Use pandas for CSV comparison
       - Assert all predictions match (same file -> same result)
       - Allow confidence score differences within rtol=1e-3

    3. Add test_embedding_equivalence():
       - Compare DNABERT-S embeddings (.pt files)
       - Compare ESM-2 embeddings (.pt files)
       - Load tensors and use torch.allclose(rtol=1e-3, atol=1e-5)
       - Account for BF16 vs FP32 precision differences
       - Report max absolute and relative differences

    4. Add test_consensus_equivalence():
       - Compare consensus sequences (CSV files)
       - Verify identical consensus results
       - Check translation consistency
       - This should match exactly (no floating point)

    5. Add test_incremental_equivalence():
       - Test each optimization individually:
         * Vanilla vs multi-GPU only
         * Vanilla vs BF16 only
         * Vanilla vs FlashAttention only
         * Vanilla vs all optimizations
       - Identify if any specific optimization causes divergence

    6. Create difference analysis:
       - For any mismatches, analyze pattern:
         * Systematic bias (all values shifted)
         * Random noise (within expected tolerance)
         * Specific samples affected
       - Generate report with recommendations

    Use rtol=1e-3 for BF16/FP32 tolerance (research recommended).
    Save difference reports for debugging if tests fail.
  </action>
  <verify>python -m pytest tests/benchmarks/test_vanilla_equivalence.py::test_prediction_equivalence --collect-only</verify>
  <done>Equivalence tests validate optimized pipeline maintains prediction accuracy</done>
</task>

</tasks>

<verification>
- Vanilla baseline provides reference outputs
- Predictions compared and match within tolerance
- Embeddings validated with appropriate numerical tolerance
- Consensus sequences match exactly
- Each optimization's impact on accuracy tested
</verification>

<success_criteria>
- Optimized pipeline predictions match vanilla within rtol=1e-3
- No systematic bias introduced by optimizations
- All intermediate outputs validated
- Detailed equivalence reports generated
</success_criteria>

<output>
After completion, create `.planning/phases/06-performance-validation/06-04-SUMMARY.md`
</output>