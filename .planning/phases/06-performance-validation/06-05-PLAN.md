---
phase: 06-performance-validation
plan: 05
type: execute
wave: 4
depends_on: ["06-01", "06-02", "06-03", "06-04"]
files_modified:
  - tests/benchmarks/report_generator.py
  - virnucpro/cli/benchmark.py
  - tests/benchmarks/regression_tracker.py
autonomous: false

must_haves:
  truths:
    - "Performance reports generated in markdown and JSON formats"
    - "CLI command available for running benchmark suite"
    - "Regression tracking identifies performance degradations"
  artifacts:
    - path: "tests/benchmarks/report_generator.py"
      provides: "Report generation utilities"
      exports: ["generate_markdown_report", "generate_json_report"]
    - path: "virnucpro/cli/benchmark.py"
      provides: "Benchmark CLI command"
      min_lines: 150
    - path: "tests/benchmarks/regression_tracker.py"
      provides: "Performance regression detection"
      exports: ["check_regression", "update_baseline"]
  key_links:
    - from: "virnucpro/cli/benchmark.py"
      to: "tests.benchmarks"
      via: "benchmark suite invocation"
      pattern: "from tests\\.benchmarks import"
---

<objective>
Generate comprehensive performance reports and provide CLI access to benchmark suite with regression tracking.

Purpose: Enable continuous performance monitoring and regression detection
Output: Reporting infrastructure for performance validation and CI integration
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-performance-validation/06-01-SUMMARY.md
@.planning/phases/06-performance-validation/06-02-SUMMARY.md
@.planning/phases/06-performance-validation/06-03-SUMMARY.md
@.planning/phases/06-performance-validation/06-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create report generation system</name>
  <files>tests/benchmarks/report_generator.py</files>
  <action>
    Implement comprehensive report generation for benchmark results:

    1. Create ReportGenerator class:
       - Aggregate results from all benchmark tests
       - Support markdown and JSON output formats
       - Include charts/visualizations (as markdown tables)

    2. Add generate_markdown_report() function:
       - Create human-readable performance report
       - Include sections:
         * Executive Summary (pass/fail, key metrics)
         * Scaling Performance (GPU speedup table and analysis)
         * End-to-End Results (<10 hour validation)
         * Per-Stage Breakdown (bottleneck analysis)
         * Memory Usage Profile
         * Equivalence Validation (accuracy confirmation)
       - Use markdown tables, formatting, color coding
       - Save to tests/reports/benchmark_YYYYMMDD_HHMMSS.md

    3. Add generate_json_report():
       - Machine-readable format for CI integration
       - Include all metrics with structured data:
         * gpu_scaling: {1: time, 2: time, 4: time, speedups}
         * stage_throughput: {stage: {time, seq_per_sec, gpu_util}}
         * memory_usage: {peak, average, efficiency}
         * equivalence: {predictions_match, max_difference}
       - Save to tests/reports/benchmark_YYYYMMDD_HHMMSS.json

    4. Add comparison utilities:
       - compare_runs() to diff two benchmark runs
       - identify_improvements() and identify_regressions()
       - generate_comparison_report() for PR reviews

    5. Create summary dashboard generator:
       - Single-page overview of all metrics
       - Pass/fail status for each requirement
       - Performance vs baseline comparison
       - Actionable recommendations

    Use pandas for data manipulation and tabulate for markdown tables.
  </action>
  <verify>python -c "from tests.benchmarks.report_generator import ReportGenerator, generate_markdown_report; print('Report generator imports successfully')"</verify>
  <done>Report generator creates comprehensive markdown and JSON performance reports</done>
</task>

<task type="auto">
  <name>Task 2: Add benchmark CLI command</name>
  <files>virnucpro/cli/benchmark.py</files>
  <action>
    Create CLI command for running benchmark suite:

    1. Create benchmark subcommand for virnucpro CLI:
       - Add to CLI using existing pattern (like profile.py)
       - Support various benchmark modes and options

    2. Add command-line arguments:
       - --suite: Which benchmarks to run (scaling/throughput/memory/all)
       - --data-size: Test data size (tiny/small/medium/large)
       - --gpus: GPU configuration to test (auto-detect by default)
       - --output-dir: Where to save reports (default: tests/reports/)
       - --compare-to: Previous run to compare against
       - --vanilla-baseline: Run vanilla comparison (slower)
       - --quick: Fast subset for CI (small data, fewer configs)

    3. Implement benchmark execution:
       - Import and run appropriate test suites
       - Capture pytest output and benchmark results
       - Handle GPU availability gracefully
       - Show progress with Rich console output

    4. Add result processing:
       - Aggregate results from all tests
       - Generate reports using report_generator
       - Display summary in console
       - Return exit code based on pass/fail

    5. Add example usage to help text:
       - virnucpro benchmark --quick (CI validation)
       - virnucpro benchmark --suite scaling --gpus 0,1,2,3
       - virnucpro benchmark --vanilla-baseline --data-size large

    Register command in main CLI entry point.
    Follow existing CLI patterns for consistency.
  </action>
  <verify>virnucpro benchmark --help</verify>
  <done>Benchmark CLI command provides easy access to performance validation suite</done>
</task>

<task type="auto">
  <name>Task 3: Implement regression tracking</name>
  <files>tests/benchmarks/regression_tracker.py</files>
  <action>
    Create regression detection system for CI integration:

    1. Create RegressionTracker class:
       - Track performance over time
       - Detect significant regressions (>10% slowdown)
       - Maintain baseline performance data

    2. Add baseline management:
       - load_baseline() from JSON file
       - update_baseline() with new results
       - Store in tests/benchmarks/baseline.json
       - Version baselines for compatibility

    3. Add check_regression() function:
       - Compare current run to baseline
       - Flag regressions exceeding thresholds:
         * >10% slowdown in total time (fail)
         * >5% reduction in GPU utilization (warning)
         * >20% increase in memory usage (warning)
       - Return detailed regression report

    4. Add trend analysis:
       - Track metrics over multiple runs
       - Identify gradual degradation patterns
       - Generate trend visualizations
       - Predict when thresholds will be exceeded

    5. Create CI integration utilities:
       - github_comment_format() for PR comments
       - exit_code_for_regressions() (0=pass, 1=fail)
       - regression_badge_data() for README badges

    Store historical data for trend analysis.
    Design for easy GitHub Actions integration.
  </action>
  <verify>python -c "from tests.benchmarks.regression_tracker import RegressionTracker, check_regression; tracker = RegressionTracker(); print('Regression tracker initialized')"</verify>
  <done>Regression tracking system detects performance degradations for CI</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete benchmark suite with reports and CLI</what-built>
  <how-to-verify>
    1. Run quick benchmark: virnucpro benchmark --quick --suite scaling
    2. Check generated report in tests/reports/
    3. Verify markdown report shows:
       - GPU scaling results
       - Performance metrics
       - Pass/fail status
    4. Confirm JSON report is machine-readable
  </how-to-verify>
  <resume-signal>Type "approved" if reports look good, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
- Markdown and JSON reports generated with all metrics
- CLI command runs benchmark suite easily
- Regression detection catches >10% performance degradation
- Reports suitable for CI integration and PR reviews
</verification>

<success_criteria>
- virnucpro benchmark command executes successfully
- Reports show all performance metrics clearly
- Regression tracker identifies slowdowns
- CI-ready with exit codes and JSON output
</success_criteria>

<output>
After completion, create `.planning/phases/06-performance-validation/06-05-SUMMARY.md`
</output>