---
phase: 06-performance-validation
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - tests/benchmarks/test_end_to_end.py
  - tests/benchmarks/test_memory_usage.py
autonomous: true

must_haves:
  truths:
    - "End-to-end benchmark validates <10 hour processing for typical samples"
    - "Memory usage patterns tracked throughout pipeline execution"
    - "Performance meets requirements on 4-GPU system"
  artifacts:
    - path: "tests/benchmarks/test_end_to_end.py"
      provides: "Full pipeline performance validation"
      min_lines: 250
    - path: "tests/benchmarks/test_memory_usage.py"
      provides: "Memory profiling tests"
      min_lines: 150
  key_links:
    - from: "tests/benchmarks/test_end_to_end.py"
      to: "virnucpro predict"
      via: "CLI invocation"
      pattern: "virnucpro predict.*--parallel.*--gpus"
---

<objective>
Validate end-to-end pipeline performance meets <10 hour requirement and profile memory usage patterns.

Purpose: Prove the optimization project achieves its core goal of <10 hour processing
Output: Performance validation showing target metrics achieved
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-performance-validation/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create end-to-end performance validation</name>
  <files>tests/benchmarks/test_end_to_end.py</files>
  <action>
    Implement full pipeline benchmarks validating <10 hour processing:

    1. Create TestEndToEndPerformance class:
       - Test complete pipeline with realistic workloads
       - Validate <10 hour requirement on 4 GPUs
       - Generate performance reports with timing breakdowns

    2. Add test_typical_sample_under_10_hours():
       - Use "typical" dataset (1000-5000 sequences)
       - Run full pipeline with 4 GPUs and all optimizations
       - Measure total time and project to full sample
       - Assert projected time < 10 hours (with 10% tolerance)
       - Generate detailed timing report

    3. Add test_large_sample_performance():
       - Test with "large" dataset (10000+ sequences)
       - Measure throughput and memory stability
       - Project processing time for production workloads
       - Verify no memory leaks or OOM errors

    4. Add test_optimization_impact():
       - Compare performance with different optimization flags:
         * Baseline: no optimizations
         * With FlashAttention: --use-flash-attention
         * With BF16: automatic on Ampere+
         * With persistent models: --persistent-models
         * All optimizations combined
       - Generate comparison table showing impact of each

    5. Create detailed performance report:
       - Total time and per-stage breakdown
       - GPU utilization average and peaks
       - Memory usage patterns and peaks
       - Throughput in sequences/hour
       - Bottleneck analysis with recommendations
       - Project to full 45-hour baseline sample

    Use real sample data if available, otherwise largest synthetic dataset.
    Include checkpointing to allow resume for long-running tests.
  </action>
  <verify>python -m pytest tests/benchmarks/test_end_to_end.py::test_typical_sample_under_10_hours --collect-only</verify>
  <done>End-to-end benchmark validates <10 hour processing requirement for typical samples</done>
</task>

<task type="auto">
  <name>Task 2: Implement memory profiling benchmarks</name>
  <files>tests/benchmarks/test_memory_usage.py</files>
  <action>
    Create memory usage profiling tests to ensure stability:

    1. Create TestMemoryUsage class:
       - Profile memory patterns during pipeline execution
       - Detect memory leaks and fragmentation
       - Validate OOM prevention mechanisms

    2. Add test_memory_stability():
       - Run pipeline with medium dataset
       - Track GPU memory usage every second
       - Verify memory returns to baseline between stages
       - Assert no gradual memory increase (leak detection)
       - Use torch.cuda.memory_stats() for detailed tracking

    3. Add test_peak_memory_per_stage():
       - Measure peak memory for each pipeline stage
       - Identify memory-intensive operations
       - Verify peaks stay within GPU capacity
       - Generate memory usage timeline chart data

    4. Add test_memory_with_large_batches():
       - Test with maximum batch sizes
       - Verify OOM prevention (should reduce batch, not crash)
       - Measure memory efficiency (useful vs allocated)
       - Test expandable segments behavior

    5. Add test_persistent_models_memory():
       - Compare memory with/without --persistent-models
       - Measure memory overhead of keeping models loaded
       - Verify cache clearing prevents fragmentation
       - Calculate memory/speed tradeoff

    6. Create memory report:
       - Peak memory per GPU
       - Memory timeline visualization data
       - Fragmentation metrics
       - Memory efficiency (used/allocated ratio)
       - Recommendations for batch sizes based on GPU memory

    Use torch.cuda.memory_stats() and nvitop for comprehensive tracking.
    Generate both human-readable and machine-parseable reports.
  </action>
  <verify>python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); from tests.benchmarks.test_memory_usage import TestMemoryUsage"</verify>
  <done>Memory profiling benchmarks track usage patterns and validate stability throughout execution</done>
</task>

</tasks>

<verification>
- End-to-end benchmark confirms <10 hour processing on 4 GPUs
- Memory profiling detects leaks and validates OOM prevention
- Performance reports show optimization impact
- All requirements validated with actual measurements
</verification>

<success_criteria>
- Typical sample (thousands of sequences) processes in <10 hours on 4 GPUs
- Memory usage remains stable without leaks
- Each optimization's impact quantified
- Detailed performance reports generated
</success_criteria>

<output>
After completion, create `.planning/phases/06-performance-validation/06-03-SUMMARY.md`
</output>